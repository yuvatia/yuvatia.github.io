export const frontmatter = {
    title: 'Differential Calculus',
    summary: "An exhaustive overview of differential calculus in Banach spaces with examples from Euclidean space, concluding with the Taylor expansion & remainder theorems.",
    date: '2024-08-16',
    tags: ['calculus', 'analysis', 'derivative', 'taylor']
};

*Note: a PDF version of this entry can be found [here](pdfs/differential_calculus.pdf).*

# Differential Calculus

This entry is the first in a series of entries that are paired into pairs of a rigorous overview of the analytical foundations of a subject, followed by an examination of the same topics via numerical methods using tools and frameworks established in the field of numerical analysis. As such, each pair in the series is mostly self contained, although some cross-referencing will happen on occasion, especially in closely related field (such as differential and integral calculus).

Each pair serves a dual purpose:
1. Providing a rigorous analytical examination of a topic in the first entry.
2. Providing a more practical examination of a topic through the lens of numerical analysis which can then be applied to many real-life applications.

This entry provides a discussion of differential calculus via means of Banach spaces (which will be introduced shortly) and topology. Most definitions and theorems intentionally deal with abstract structures to provide for easier generalization (and also because I think that, while dealing with abstracts, one may come to a more fundamental understanding of the concrete), followed by reflections on those results in Euclidean spaces which are indeed the more common setting people who have studied Calculus I & II are familiar with.

I also make some references to definitions and results from set theory, which was previously presented briefly in [Graph Theory Part 1](/#/posts/Graph%20Theory,%20Part%201:%20Combinatorics), and topology, which was previously presented briefly in [Graph Theory Part 2](#/posts/Graph%20Theory,%20Part%202:%20Topology). Some basic knowledge of material usually covered in Calculus I, II & III is helpful, although we build almost all results from first principles so it is not necessary. Finally, basic knowledge of linear algebra is also needed,most of it and more is covered in a past entry which can be found [here](/#/posts/Affine%20Spaces).

The three most important concepts discussed here, which carry over to the next entry, are:
1. The derivative of a function
2. (Multi)linear maps
3. Taylor's theorem

This entry is based on many different sources, most prominent of which are listed here:
1. Differential Calculus by Henri Cartan
2. [The Bright Side of Mathematics](https://thebrightsideofmathematics.com/)' Functional Analysis videos on [YouTube](https://www.youtube.com/playlist?list=PLBh2i93oe2qsGKDOsuVVw-OCAfprrnGfr).
3. Prof. Hicham Gebran's Differential Calculus (M3302) Lebanese University lectures, available on [YouTube](https://www.youtube.com/playlist?list=PLxVzJMbBTmvc8Zg6s46ra5F7tH0gaW95c).
4. Dr. Aviv Censor's Calculus I (104031) & II (104032) Technion lectures, also available [on](https://www.youtube.com/playlist?list=PLW3u28VuDAHLBQrejV70zRa6sesxkPmgA) [YouTube](https://www.youtube.com/playlist?list=PLW3u28VuDAHJ6oveq15M0DmdbjCMO3H-7) (note: lectures are in Hebrew, but there exist recordings of similar curriculum in English by the same lecturer).

# Metric Space

Informally speaking, a metric space is a structure which consists of a set whose elements are called *points* and a distance function which maps distances to points, called a *metric*.

Formally speaking, we define a **metric space** $M$ by a pair $(P, d)$ such that $P$ is a set and a metric $d$ is a function $d: P \times P \to \mathbb{R}$ such that $d$ satisfies $\forall x, y, z \in P, x \neq y$:

1. $d(x, x) = 0$ (the distance of a point from itself is zero)
2. $d(x, y) \gt 0$ (positivity)
3. $d(x, y)=d(y, x)$ (symmetry)
4. $d(x, z) \leq d(x, y) + d(y, z)$ (triangle inequality)

 These axioms on $d$ encapsulate what one naturally expects when thinking of a notion of distance - the distance of a point from itself is always zero, it cannot be negative, the distance between two point is the same regardless of which way we travel and the shortest path between two points is always the direct path.

If the set of points $P$ is finite, we say that the metric space $M$ is finite and call it a **finite metric space**.

For the sake of our discussion, we will be interested in defining a **neighbourhood** of a point $p \in P$. Recall the definition of an open $\epsilon$-ball around $p$ in set notation presented in a previous discussion on topology (adjusted to the notation used in this section)

$$
B_{\epsilon}(x)=\{x \in P, d(x,p) \lt \epsilon  \}
$$

We call this an **open ball** around $p$ with radius $\epsilon$, or a **neighbourhood** of $p$ with radius $\epsilon$, we denote it by $B_{\epsilon}(x)$.

### Examples of a Metric

Take the Euclidean notion of distance in one-dimension $\mathbb{R}$:

$$
d(x, y) = |x - y|
$$
with $x, y \in \mathbb{R}$. It satisfies all the axioms, so $d$ as defined above is indeed a metric, so $(\mathbb{R}, d)$ is a metric space. Since when dealing with Euclidean spaces it is assumed that we are also using the Euclidean notion of distance, we often refer by abuse of notation to the metric space $\mathbb{R}$.

Similarly, in the plane we have the Euclidean distance as stated in Pythagoras' theorem:

$$
d(x, y) = \sqrt{(x_{0}-y_{0})^{2}+(x_{1}-y_{1})^{2}}
$$
Which is a natural extension of the distance function in $\mathbb{R}$ (observe that in that case the distance function is the square root of the first term), and can also be shown to be a metric. Similarly we have another geometric result on the distance between two points in 3-dimensional Euclidean space which, again, is a metric.

In fact, this can be generalised to $\mathbb{R}^{n}$. Let $d: \mathbb{R}^{n} \times \mathbb{R}^{n} \to \mathbb{R}$   be defined as

$$
d(x, y) = \sqrt{\sum_{i=0}^{n-1}(x_{i}-y_{i})^{2}}
$$
The first three axioms are trivially satisfied. For the fourth axiom, observe that since $d(x, y) \geq 0$, we have $d(x, y)^{2} \geq 0$ so by the properties of the inequality we have

$$
d(x, z) \leq d(x, y) + d(y, z) \iff d(x, z)^{2} \leq (d(x, y) + d(y, z))^{2}
$$
Expanding the rhs, we get

$$
d(x, y)^{2} + 2d(x, y)d(y, z) + d(y, z)^{2} = (*)
$$

$d(x, z)^{2}$ can also be expressed as

$$
\begin{gathered}
\sum_{}(x_{i}-z_{i})^{2} = \sum((x_{i}-y_{i})+(y_{i}-z_{i}))^{2} = \\
\sum((x_{i}-y_{i})^{2}+2(x_{i}-y_{i})(y_{i}-z_{i})+(y_{i}-z_{i})^2) = \\
\sum(x_{i}-y_{i})+2 \sum (x_{i}-y_{i})(y_{i}-z_{i})+\sum(y_{i}-z_{i})^2 = \\
d(x, y)^{2} + 2 \sum(x_{i}-y_{i})(y_{i}-z_{i})+d(y, z)^{2} = (**)
\end{gathered}
$$
Putting $(*)$ and $(**)$ together in the inequality we wish to prove, we have

$$
d(x, y)^{2} + 2 \sum(x_{i}-y_{i})(y_{i}-z_{i})+d(y, z)^{2} \leq d(x, y)^{2} + 2d(x, y)d(y, z) + d(y, z)^{2}
$$
Which simplifies to

$$
\sum(x_{i}-y_{i})(y_{i}-z_{i}) \leq d(x, y)d(y, z)
$$
Observe that the LHS is the dot product of $x-y$ and $y-z$ so this equation takes the form of the [Cauchy-Schwartz inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality), completing the proof, so $d_{n}$ is a metric in every real space.

## Product Metric

Let $(X_{1}, d_{X_{1}}), (X_{2}, d_{X_{2}}), \dots, (X_{n}, d_{X_{n}})$ be a set of finitely many ($n$) metric spaces, and consider their Cartesian product $X^{n}=X_{1} \times X_{2} \times \dots \times X_{n}$ and denote an element of $X^{n}$ via the $n$-tuple $(x_{1}, x_{2}, \dots, x_{n})$.

**Definition (Product metric)**: The map $d_{X^{n}}: X^{n} \times X^{n} \to \mathbb{R}$, defined as

$$
d_{X^{n}}(x, y)= \max \{d_{X_{1}}(x_{1}, y_{1}), d_{X_{2}}(x_{2}, y_{2}), \dots, d_{X_{n}}(x_{n}, y_{n})\}
$$
Is called the **product metric** on the product space $X^{n}$.

Note: When $n$ is not dinite we replace $\max$ with $\sup$.

It can be verified (left as an exercise) that this is indeed a metric, so $(X^{n}, d_{X^{n}})$ is a metric space.

The product metric is the usual metric of a product space, i.e. if the metric space is a product space and the metric isn't specified, one may assume that the metric used is the product metric.

## Isometry

**Definition (isometry)**: Let $(A, d_{A})$ and $(B, d_{B})$ be metric spaces and let $f: A \to B$  be a map. $f$ is called an **isometry** or **distance preserving** if $\forall a, b \in A, d_{A}(a, b) = d_B{}(f(a), f(b))$.

**Example**: Consider $f: \mathbb{R} \to \mathbb{R}^{2}$ with the usual metric, given by $f(x)=(x, 0)$, then $\forall x, y \in \mathbb{R}$:

$$
d(f(x), f(y)) = d((x, 0), (y, 0)) = \sqrt{(x-y)^{2}+(0-0)^{2}}=x-y=d(x, y)
$$
So $f$ is an isometry.

**Proposition**: An isometry is injective.

**Proof**: Suppose not, then $\exists a, b \in A, a \neq b$ such that $f(a)=f(b)$. Since $a \neq b$ then by the metric properties $d(a, b) \gt 0$, but since $f(a)=f(b)$ then again by metric properties $d(f(a), f(b))=0$, so $f$ is not an isometry, which is a contradiction.

**Definition (global isometry)**: A bijective isometry is a global isometry.

**Definition (isometric spaces)**: Let $A, B$ be metric spaces. $A$ and $B$ are said to be **isometric** if there exists a map $f: A \to B$ which is a bijective isometry (i.e. a global isometry).

**Corollary**: A global isometry on metric spaces is an isomorphism on the spaces.

**Proof**: Since $f$ is a bijective isometry we can take $f \circ f^{-1}$ and get an identity on $B$, similarly $f^{1}\circ f$ is the identity on $A$, so $f, f^{-1}$ form an isomorphism between $A$ and $B$.

**Definition (linear isometry)**: An isomtery which is a linear map is called a linear isometry.

**Example**: Rotations in $\mathbb{R}^{k}$ are linear isometries.

## Open and Closed Sets

Recall that a topology is a collection of open sets. To study the topological properties of a metric space, we need to define what *open* means in the context of a metric space.

Recall the definition presented for an open ball, which is the neighbourhood of a single point. Now, let $X$ be a set of points, then we say that X is **open** if it is the neighbourhood of all its points, i.e. a set $X$ is **open** if $\forall x \in X, \exists \epsilon \gt 0$ such that $\exists B_{\epsilon}(X) \subseteq X$.  

The definition of a **closed set** and of a **boundary of a set** are the usual definitions from topology - a set is closed if its complement is open, and a boundary of $X$ is a set $A\subseteq X$ such that there are no open balls for any point of $A$ that is only in $A$ or $A^{c}$ (its complement with respect to the set of points in the metric space) and it is denoted by $\partial X$, i.e.

$$
\partial X = \{x | \forall \epsilon \gt 0, B_{\epsilon}(x) \cap A \neq \emptyset \land B_{\epsilon}(x) \cap A^{c} \neq \emptyset \}
$$

The definition of a boundary of a set leads to equivalent definitions for an open set and a closed set in terms of its boundary: 

Let $X$ be a set. 
1. $X$ is **open** $\iff$ $X \cap \partial X = \emptyset$.
2. $X$ is **closed** $\iff$ $X \cup \partial X = X$. 
3. The **closure** of $X$ is the set $\overline{X}=X \cup \partial X$, and it "closes" the set $X$. 
4. The points in the closure of $X$ not in the boundary of $X$ are called *interior points*, and the points on the boundary are called *boundary points*.

Note that the definition of an open set and a closed set are not contradictory, i.e. a set can be both open and closed, and can also be neither open nor closed.

Let's verify that this definition satisfies the axioms for a topology. We denote the family of open sets in $X$ as $\tau$. For $\tau$ to be a topology on $X$, it needs to satisfy:

1. $\emptyset$ and $X$ are in $\tau$.
2. $\forall t \subseteq \tau, (\cup_{x \in t} x) \in \tau$ .
3. Let $t$ be a finite subset of $\tau$, then $(\cap_{x \in t}x) \in \tau$.

**Claim**:  $\tau$ is a topology on $X$, where $(X, d)$ is a metric space and $\tau$ is the set of open subsets of $X$ defined via the metric $d$.

**Proof**: We show that $\tau$ satisfies all 3 axioms.

1. By definition since $\tau$ is a set we have $\emptyset \in \tau$. As for $X$ -  if it is not open then  $X \cap \partial X \neq \emptyset$, i.e. there exist $x \in X$ that is also in $\partial X$,  but then by definition $B_{\epsilon}(x) \cap X^{c} \neq \emptyset$, but since $X$ is the set of all points in the space we have $X^{c}=\emptyset$ so $B_{\epsilon}(x) \cap X^{c}$ cannot contain any member that is not $\emptyset$ for all epsilon, so we have a contradiction which means that $X$ is an open set.
2. Note that each member of $t$ is an open set, and each element of the union belongs to at least one member of $t$ so it has an open ball in that member, but since the union consists of all elements in all members $t$ then it also contains this open ball for each point, so we have that each point in the union has a neighbourhood in the union so it is open.
3. Denote the intersection of all members of $t$ as $I$. Consider some $x \in I$, by definition it belongs to all members of $t$, and since they are all open sets $x$ has an open ball in each set. Pick an ordering on $t$ and let $\epsilon_{i}$ be the radius of an open ball around $x$ in $t_{i}$. Since the collection is finite, we can take the smallest such $\epsilon_{i}$, denote it $\epsilon_{\min}$. Since all balls share the same centre in $x$, then all balls contain $B_{\epsilon_{\min}}$, and since each ball is a subset of each element of $t$, we have that $B_{\epsilon_{\min}} \subseteq I$, which means that $x$ has an open ball in $I$, and since $x$ represents a generic element of $I$, we have that $I$ is an open set by definition. 

As a practical example, consider the metric space on the set of points $P=(0, 3] \times [5, \infty)$ with the usual metric, and consider the subset $A=(0, 3] \subset P$.

**Exercise**: Show that $A$ is open.

**Solution**: Take some $x \in A$. Either $x=3$ or not. If $x=3$, take $\epsilon=1$, then we have $B_{1}(3)$ consisting of all points $p$ in $P$ such that $d(p, 3) \lt 1$, but all such points are in $A$, so $B_{1}(3) \subset A$. If $x \neq 3$, then we take $\epsilon = \frac{1}{2}\min(d(x, 3), d(x, 0))$, and again we have an open ball in $A$, so in conclusion we have that $A$ is open.

**Exercise**: Show that $A$ is closed.

**Solution**: Consider $\partial A$. There exists no point in $P$ such that it satisfies the condition of a boundary point on $A$, so we have $\partial A = \emptyset$, but then $A \cup \partial A = A$, so by the equivalent definition to closeness we have that $A$ is closed.

In conclusion, we have that $A$ is both open and closed. We call such set a **Clopen set**.

**Theorem**: Let $A$ be a set in a topological space $X$, then $A$ is clopen $\iff$ $\partial A = \emptyset$.

**Proof**: The proof in $\impliedby$ direction is immediate so we only prove the other direction.$\implies$ Let $A$ be clopen and suppose its boundary is not the empty set, then $\exists x \in \partial A$. Since $A$ is open, $x \notin A$, so $A \subset A \cup \{ x \} \subseteq A \cup \partial A$, but since $A$ is closed $A \cup \partial A = A$ so $A \supseteq A \partial A$, which is a contradiction.

**Definition (cover)**: A **cover** of a set $A$ in a topological space $X$ (not necessarily a metric space) is a set $B$ of subsets of $X$ such that $A$ is contained in the union of all elements of the cover, i.e. $\cup_{i \in |B|} B_{i} \supseteq X$. 

For example, in $\mathbb{R}$, consider the interval $[0, 5]$. The set of internals $\{[0, 3], [2, 4], [1, 5] \}$ is a cover of $[0, 5]$ in $\mathbb{R}$.

If all elements of the cover $B$ are open sets, then we say that $B$ is an **open cover**. A **subcover** of a cover $B$ of $A$ is a subset $C \subseteq B$ such that $C$ is still a cover of $A$. A **refinement** of a cover $B$ is a new cover $D$ such that every element (set) of $D$ is contained in some element (set) of $C$. For example, if we take $B$ to be the cover for $[0, 5]$ provided above, then

$$
D = \{[0, 3], [2, 3], [3, 4], [1, 3], [3, 5]\}
$$
Is a refinement of $B$.

If a cover has finitely many elements, then it is a **finite** cover.

**Proposition**: Given a metric space $(M, d)$ and some $\epsilon \gt 0$, the set $C_{\epsilon}=\{B_{\epsilon}(x) | x \in M \}$ is an open cover of $M$, called an $\epsilon$-cover of $M$.

**Proof**: We know that epsilon open balls are open sets, so all that remains is to show that the set $C_{\epsilon}$ is indeed a cover of $M$, but this follows immediately from how $C_{\epsilon}$ is defined: for any member of $M$, we have an epsilon ball in $C_{\epsilon}$, so $M \subseteq \cup_{x \in M} B_{\epsilon}(x)$, so $C_{\epsilon}$ is an open cover of $M$.

# Sequences and Convergence

We define a **sequence** $x$ as a mapping from $x: \mathbb{N} \to X$ where $X$ is a set, and denote the image of $n \in \mathbb{N}$ under $a$ as $x_{n}$.

Consider some metric space $M=(X, d)$ and a sequence $x: \mathbb{N} \to X$ in $M$. We say that the sequence $x_{n}$ is **convergent** if $\exists \tilde{x} \in X$ such that $\forall \epsilon \gt 0$ $\exists N \in \mathbb{N}$ such that if $n \gt N$ then $d(x_{n}, \tilde{x}) \lt \epsilon$. Otherwise we say $x_{n}$ **diverges**.

Informally this property can be thought of as the accumulation of points in the sequence near $\tilde{x}$ such that as $n$ increases, the points get closer and closer.  This definition implies that for any open $\epsilon$-ball around $\tilde{x}$ there is a finite number of elements of $x_{n}$ outside the open ball (where their cardinality is the value of $N$ for that $\epsilon$).

If such $\tilde{x}$ exists in the metric space, we say that $x_{n}$ **converges to a limit** with $\tilde{x}$ called the limit of $x_{n}$, and we denote this relation by

$$
\lim_{n \to \infty} x_{n} = \tilde{x}
$$
**Fact**: If $x_{n}$ converges in $M$, then $\tilde{x}$ is unique.

**Proof**: Suppose not, then let $L_{1}$ and $L_{2}$ be two such limits. Take $\epsilon = \frac{1}{2}d(L_{1}, L_{2})$. Since $d$ is a metric then $\epsilon \gt 0$ so by definition of convergence of a sequence we have for both $L_{1}$ and $L_{2}$

$$
\exists N_{i}, \forall n \gt N_{i}: d(x_{n}, L_{i}) \lt \frac{1}{2}d(L_{1}, L_{2})
$$

Where $i$ is either $1$ or $2$.  Now, take $N=\max(N_{1}, N_{2}) + 1$.  We have $N \gt N_{1}$ and $N \gt N_{2}$ so $\forall n \gt N$ the above relation holds. By addition of the inequalities, we get

$$
d(x_{n}, L_{1}) + d(x_{n}, L_{2}) \lt d(L_{1}, L_{2})
$$
However by the triangle inequality we have

$$
d(x_{n}, L_{1}) + d(x_{n}, L_{2}) \geq d(L_{1}, L_{2})
$$
Which is a contradiction.

Observe that this definition is a generalization of the definition for convergence of a sequence in $\mathbb{R}$, where we used the Euclidean metric in 1-dimensional space explicitly and write $|\tilde{x} - x_{n}| \lt \epsilon$ to define the criterion for convergence (with the correct $N$ for $\epsilon$).

Now that we have defined sequences and convergence, we can prove an alternative definition for closed sets.

**Definition (accumulation/limit point)**: Let $\tilde{a} \in A$, if $\forall \epsilon \gt 0, \exists a \in A \cap B_{\epsilon}(\tilde{a}), a \neq \tilde{a}$ then we say that $\tilde{a}$ is an **accumulation point** or a **limit point**.

**Proposition**: An accumulation point is the limit of a convergent sequence in $A$.

**Proof**: Let $\tilde{a}$ be an accumulation point in $A$. Consider the sequence $\frac{1}{n} \in \mathbb{R}$.  For each $n$, we pick an epsilon ball $B_{\frac{1}{n}}(\tilde{a})$ centred at the accumulation point. By definition each such $\epsilon$-ball contains some $a_{n} \in A, a_{n} \neq \tilde{a}$. Consider the sequence $a_{n}$, it satisfies $d(a_{n}, \tilde{a}) \lt \frac{1}{n}$ and also that $\forall m, n \in \mathbb{N}, m \gt n \implies d(a_{m}, \tilde{a}) \lt a_{n, \tilde{a}}$. Let $\epsilon \gt 0$ and pick $N \in \mathbb{N}$ such that $N = \lfloor \frac{1}{\epsilon} \rfloor$, so $d(a_{N}, \tilde{a}) \lt \frac{1}{\lfloor \frac{1}{\epsilon} \rfloor} \lt \epsilon$, and $\forall n \gt N$ we have $d(a_{n}, \tilde{a}) \lt d(a_{N}, \tilde{a}) \lt \epsilon$, so $\tilde{a}$ is the limit of $a_{n}$.

**Proposition**: If $\exists \tilde{a} \in A$ such that $\forall n \in \mathbb{N}, a_{n}=\tilde{a}$ then $a_{n}$ is a constant sequence and its limit is $\tilde{a}$. Note that this also holds if $a_{n}$ is constant for all but a finite amount of indices.

**Proof**: We prove the stronger version of the proposition: since $a_{n}$ is constant almost everywhere, $\exists N \in \mathbb{N}$ such that $\forall n \gt N, a_{n}=\tilde{a}$. Observe that $\forall \epsilon \gt 0$ $\forall n \gt \mathbb{N},  d(a_{n}\tilde{a})=0 \lt \epsilon$.

**Theorem**: Let $M=(X, d)$ be a metric space and let $A \subseteq X$. $A$ is closed $\iff$ for every convergent sequence in $A$, its limit is in $A$.

**Proof**: We prove both directions by contraposition, we means that we wish to prove the following: $A$ is not closed $\iff$ there exists a convergent sequence $a_{n} \subseteq A$, such that $\lim_{n \to \infty}a_{n} \notin A$.

"if" direction: Let $\tilde{a} \in X$ be the limit of $a_{n}$, and note that $\tilde{a} \notin A$. Now, consider an epsilon ball around $\tilde{a}$, by definition of the limit $\forall \epsilon \gt 0, \exists N \in \mathbb{N}$ such that $\forall n \gt N$ all points $a_{n}$ in the sequence are in $B_{\epsilon}(\tilde{a})$, and since the sequence is in $A$ we have $\forall \epsilon \gt 0, B_{\epsilon}(\tilde{a}) \cap A \neq \emptyset$. This means that in $A^{c}=X \setminus A$ we have $\tilde{x}$ as a boundary point, which means that $A^{c}$ is by definition not open, but that also means by definition that $A$ is not closed, completing the proof.

"only if" direction: Again, observe that $A$ is not closed $\iff$ $A^{c}$ is not open, which implies that there is some $\tilde{a} \in A^{c}$ such that $\forall \epsilon \gt 0, B_{\epsilon}(\tilde{a}) \cap A \neq \emptyset$. In particular, consider the epsilon sequence defined by $\epsilon_{n} = \frac{1}{n}$. Clearly $\forall n, \epsilon_{n} \gt 0$, so we can construct a sequence of epsilon balls around $\tilde{a}$, with each point containing at least one point from $A$ since the intersection with $A$ is non-empty. For each $n$, denote such member of $A$ that is also a member of $B_{\epsilon_{n}}(\tilde{a})$ as $a_{n}$. We now have a sequence of points. Let us prove that it converges to $\tilde{a}$: For all $\epsilon$, pick $n = \lfloor \frac{1}{\epsilon} \rfloor$,  recall that we constructed $a_{n}$ such that they belong to the epsilon ball with radius $\frac{1}{n}$, so we have $d(a_{n}, \tilde{a}) \lt \frac{1}{n}$, and by arithmetics we have $d(a_{n}, \tilde{a}) \lt \epsilon$,  so $\tilde{a}$ is the limit of $a_{n}$ by definition, but since we have $\tilde{a} \in A^{c}$ we have that $a_{n}$ is a convergent sequence of elements from $A$ whose limit is not in $A$, which completes the proof.

Now consider the following: what if, instead of the metric space $M=(X, d)$, we were to look at the metric sub-space $M'=(A, d)$? Clearly by the second part of the proof we would have a sequence in $M'$ whose points get closer and closer (as measured by the metric $d$), but as we have seen the limit is outside $A$ so such series would never converge. This motivates generalizing our notion of a convergent sequence.

Let $M=(A, d)$ be a metric space. A sequence $a_{n} \in A$ is called a **Cauchy sequence** if $\forall \epsilon \gt 0, \exists N \in \mathbb{N}$ such that if $n, m \gt N$ then $d(a_{n}, a_{m}) \lt \epsilon$. Observe that this definition does require $a_{n}$ to converge in $M$ for $a_{n}$ to be a Cauchy sequence. If all Cauchy sequence converge in $M$, then we say that $M$ is a **complete** metric space. Intuitively, a complete metric space is one which has no "holes" in its boundary or interior.

An incomplete metric space can be completed by adding all limits of Cauchy sequences in $M$ to $A$.

As an example of a complete metric space, consider the metric space $M=(A, d)$ where $d$ is defined as

$$
d(x, y) = \begin{cases} 1 & x \neq y \\ 0 & \text{otherwise} \end{cases}
$$
Observe that $\forall \epsilon \in (0, 1)$ we have $d(x, y) \lt \epsilon$ if and only if $x = y$, in which case we have $d(x, y) = 0$, which means that for a large enough $n$ the sequence $a_{n}$ is fixed, which means its limit is also the fixed value and since the limit is a member of the sequence it is contained in the set of points of the metric space, so the metric space is complete.

### Convergence of a Sequence in Euclidean Space

Let us turn our attention to sequences in the metric space $(R^{n}, d)$ where $d$ is the usual Euclidean metric. In real analysis, we focus on Euclidean spaces with the Euclidean metric, and while it is true that the Euclidean space is a complete metric space, not all results which hold for Euclidean space generalize to other complete metric spaces. Also, our Euclidean metric changes based on the dimension of the Euclidean space. Since the real Euclidean space is the space where a lot of applied math is conducted, it is useful to forgo the generality of metric spaces and focus on Euclidean spaces. Indeed, our main goal in this series is to circle back to Taylor's Theorem for real multivariable functions.

Let's revisit the definition of a convergent sequence, this time with notations specific to $(\mathbb{R}^{1}, d)$:

**Definition (convergence of a sequence in $\mathbb{R}^{1}$)**: A sequence $a_{n}$ in $R^{1}$ is convergent if $\exists \tilde{a} \in \mathbb{R}^{1}$ such that $\forall \epsilon \gt 0 \in \mathbb{R}, \exists N \in \mathbb{N}$, such that $\forall n \gt N, |a_{n} - \tilde{a}| \lt \epsilon$. 

Now consider the metric space $(R^{n}, d)$. We claim that the following is true:

**Theorem**: Consider the metric spaces ($R^{n}, d_{n})$ and $(R^{1}, d_{1})$ where $d_{i}$ is the usual $n$-dimensional Euclidean metric. Let $x_{n}$ be a sequence in $R^{n}$, then: $x_{n} \underset{n \to \infty}{\to} \tilde{x}$ $\iff$ $x_{n_{i}} \underset{n \to \infty}{\to} \tilde{x}_{i}$, where $\tilde{x}_{i}$ is the $i$-th element of $\tilde{x}$ and $x_{n_{i}}$ is the $i$-th element $x_{n}$.

**Proof**: First identify that what we need to show is that

$$
\exists \epsilon \gt 0, \exists N \in \mathbb{N}, \forall n \gt N:d(x_{n}, \tilde{x}) \lt \epsilon \iff \exists \epsilon \gt 0, \exists N \in \mathbb{N}, \forall n \gt N \land \forall i \in [1, n]: d(x_{n_{i}},\tilde{x}_{i}) \lt \epsilon
$$
Where the last term on each side is the important term and the rest are there for rigor and correctness.

$x_{n} \underset{n \to \infty}{\to} \tilde{x} \impliedby x_{n_{i}} \underset{n \to \infty}{\to} \tilde{x}_{i}$ :  We use the following lemma, which can be proven easily by induction:

**Lemma**: In $\mathbb{R}$, the sum of squares is less than or equal to the square of sums, i.e.

$$
\sum_{i=1}^{n}a_{i}^{2} \leq \left( \sum_{i=1}^{n}a_{i}\right)^{2}
$$
Also, recall that by inequality arithmetics, if $a, b \gt 0$ then $a \lt b \iff a^{2} \lt b^{2}$.

Back to our proof: Let $\epsilon \gt 0$. By the convergence of each one dimensional sequence, we have that for $\frac{\epsilon}{n}$, $\exists N_{1}, N_{2}, \dots, N_{n}$, such that for each sequence $x_{n_{i}}$. Pick $N = \max\{N_{1}, N_{2}, \dots, N_{n}\}$. Then $\forall n \gt N$, we have $d_{1}(x_{n_{i}}, \tilde{x}_{i}) \lt \frac{\epsilon}{n}$, and by addition of inequalities we have

$$
(*) = \sum_{i = 1}^{n}d_{1}(x_{n},\tilde{x_{i}}) \lt \epsilon
$$
Now, consider  $d_{n}(x_{n}, \tilde{x})$ where $\tilde{x}$ is an $n$-tuple whose $i$-th element is $\tilde{x}_{i}$, i.e. the limit of $x_{n_{i}}$. We have:

$$
d_{n}(x_{n}, \tilde{x}) = \sqrt{\sum_{i=1}^{n} d_{1}(x_{n_{i}}, \tilde{x}_{i})^{2}} \underset{\text{by lemma}}{\leq} \sqrt{ \left( \sum_{i=1}^{n} d_{1}(x_{n_{i}, \tilde{x}_{i}}) \right)^{2}} = (*) \lt \epsilon
$$
So $x_{n}$ converges to $\tilde{x}$.

$x_{n} \underset{n \to \infty}{\to} \tilde{x} \implies x_{n_{i}} \underset{n \to \infty}{\to} \tilde{x}_{i}$ : Observe that

$$
\begin{gathered}
\forall i \in [1, n],\quad d_{1}(x_{n_{i}}, \tilde{x_{i}}) = \sqrt{d_{1}(x_{n_{i}}, \tilde{x}_{i})^2} \underset{(*)}{\leq} \sqrt{\sum_{i=1}^{n}d_{1}(x_{n_{i}}, \tilde{x}_{i})^{2}} = d_{n}(x_{n}, \tilde{x}) \\\\
(*) \quad \text{addition of non-negative terms}
\end{gathered}
$$
So since we have $d_{n}(x_{n}, \tilde{x}) \lt \epsilon$, for some $n \gt N$ since $\tilde{x}$ is the limit of $x_{n}$, we have that $d_{1}(x_{n},\tilde{x}_{i}) \lt \epsilon$ for all $i$ in $[1, n]$ and $\forall \epsilon \gt 0$, so $x_{n_{i}} \underset{n \to \infty}{\to} \tilde{x}_{i}$.

The significance of this result is that it lets us generalize results from the real line to $\mathbb{R}^{n}$ relatively easily, as we will see later.

**Note**: This proof generalizes without changes to any product space, and we will use this property later.

**Theorem**: Let $a_{n}$ be a sequence which converges to $\tilde{a}$, then $|a_{n}| \to |\tilde{a}|$.

**Proof**: By the second triangle inequality we know that $|\space |a_{n}| \space - |\tilde{a}| \space | \leq |a_{n} - \tilde{a}|$, and we know that $\forall \epsilon \gt 0, \exists N \in \mathbb{N}$ such that $\forall n \in \mathbb{N}, (n \gt N \implies |a_{n}-\tilde{a}| \lt \epsilon)$, so for the same $\epsilon-N$ we have that $d(|a_{n}|, ||\tilde{a}|) \lt \epsilon$, which completes the proof.

## Normed Vector Space

Consider a vector space $V = (A,  \mathbb{F}, +, \cdot)$ such that $\mathbb{F} \in \{\mathbb{R}, \mathbb{C}\}$ (i.e. the scalar field is either the field of real numbers or the field of complex numbers). Let $v \in V$ be a vector. Recall that we tend to represent vectors are arrows from the origin towards a point whose coordinates is the coordinate vector $[v]_{B}$ with respect to some basis $B$. Analytically, it is interesting to talk about the length of such vector. Recall from $\mathbb{R}^{n}$ that we usually define the length of a vector as $||v|| = \sqrt{\sum_{i=1}^{n}v_{i}^{2}}$, i.e. root of the sum of the squared component of $v$. This definition is also the intuitive definition for length in the physical world, but much like when defining a metric we sought a generalization of distance, we seek to generalize the notion of length as well.

**Definition (norm)**: Let $V$ be a vector space with scalars from $\mathbb{R}$ or $\mathbb{C}$. A **norm** is map $||\cdot||: V \to \mathbb{R}$ such that it satisfies the following $\forall v, u \in V$ and $\forall \lambda \in \mathbb{F}$:

1. Non-negative: $||v|| \geq 0$.
2. Positive-definite: $||v|| = 0 \iff 0_{V}$.
3. Absolute homogeneous: $||\lambda v|| = |\lambda| ||v||$, where $|\lambda|$ is the absolute value of $\lambda$ as defined in $\mathbb{F}$.
4. Triangle inequality: $||v + u|| \leq ||v|| + ||u||$.

Note that by $(4)$, $||\cdot||$ is not linear.

**Definition (Euclidean norm)**: Consider the map $||\cdot||_{2} : V \to \mathbb{R}$ defined as $||v||_{2} = \sqrt{\sum_{i=1}^{n} v_{i}^{2}}$. This map is called the **Euclidean norm**. One can quickly verify that it indeed satisfies all axioms of a norm.

**Definition (normed vector space)**: Consider the tuple $(V, ||\cdot||)$ which is a vector space equipped with a norm map. We call this structure a **normed vector space**. If the choice of the norm is implied by $V$ (i.e. in $\mathbb{R}^{n}$ the usual norm is $||\cdot||_{2}$) then we sometime refer to $V$ as a normed vector space via abuse of notation.

**Definition (induced metric)**: Let $(V, ||\cdot||)$ be a vector space. Then the map $d_{||\cdot||}: V \times V \to \mathbb{R}$ defined as

$$
d_{||\cdot||}(u, v) = ||u - v||
$$

Is the **induced metric** of the norm $||\cdot||$.

**Proposition**: The induced metric $d_{||\cdot||}$ is a metric.

**Proof**: We show that $d_{||\cdot||}$ satisfies all 4 axioms of a metric $\forall x, y, z \in V, x \neq y$. For simplicity, we shall refer to $d_{||\cdot||}$ as $d$.

1. $d(x, x) = ||x - x|| = ||0|| = 0_{V}$.
2. Denote $u = x - y$, since $x \neq y$ we know that $u \neq 0_{V}$, so by properties $(1)$ and $(2)$ of the norm we have $||u|| \gt 0$. so $d(x - y) = || x - y|| = ||u|| \gt 0$.
3. $d(x, y) = || x - y|| = ||-(y - x)|| = |-1| ||y - x|| = || y - x|| = d(y, x)$
4. $d(x, z) = ||x - z|| = ||x - y + y - z|| \leq ||x - y || + ||y - z|| = d(x, y) + d(y, z)$

**Corollary**: $(V, d_{||\cdot||})$ is a metric space.

**Example**: The Euclidean metric $d_{\text{euclid}}$ is the induced metric of the Euclidean norm $||\cdot||_{2}$:

$$
|| x - y||_{2} = \sqrt{\sum_{i=1}^{n}(x_{i}-y_{i})^{2}} = d_{\text{euclid}}(x, y)
$$

We now have a metric space which is also a vector space with scalars from $\mathbb{R}$ or $\mathbb{C}$, so we can talk about *convergence of sequences* in $(V, d_{||\cdot||})$ and about the *completeness* of that space (recall that a metric space is *complete* if all Cauchy sequences in the metric space converge to a limit in the metric space).

**Definition (Banach space)**: If $(V, d_{||\cdot||})$ is a *complete metric space*, then it is called a **Banach space**.

**Example**: The Euclidean space $\mathbb{R}^{n}$ with the usual norm is a Banach space: it is a vector space with scalars from $\mathbb{R}$ and the induced metric of the usual norm is the usual metric, and the metric space $(\mathbb{R}^{n}, d_{\text{euclid}})$ is complete.

**Example 2**: Consider the Babylonian method for calculating the square root of a number $K$:

$$
x_{n+1} = \frac{1}{2}(x_{n}+\frac{S}{x_{n}})
$$
Which converges to $\sqrt{S}$ in $\mathbb{R}$ for all $x_{1} \gt 0$ (where $x_{1}$ is the initial guess). In particular, for a rational $x_{1}$ and a rational $S$, it follows that $x_{n}$ is rational $\forall n \in \mathbb{N}$. Now, consider the metric space $\mathbb{Q}$ with the usual Euclidean metric. It is a metric space and the sequence we just presented is clearly a Cauchy sequence in $\mathbb{Q}$, however since it converges to $\sqrt{S}$  in $\mathbb{R}$ then for $2$ its limit is an irrational number by the classic result that $\sqrt{2}$ is irrational,  so $\exists a_{n} \in \mathbb{Q}$ such that $a_{n}$ is a Cauchy sequence which does not converge it $\mathbb{Q}$, so it is not a Cauchy sequence.

### Limit Arithemetics

Consider a Banach space $B$. Since it is a vector space, it is equipped with addition (and subtraction) and scalar multiplication. Since it is a complete metric space, it is equipped with a notion of convergence of a sequence. This is a good framework for discussing the behaviour of limits under arithemetics. Let's start with a warm-up:

**Theorem**: Let $a_{n}$ be a sequence and $\lambda$ be a scalar from $\mathbb{F}$. If $a_{n}$ converges to $\tilde{a}$, then $\lambda a_{n}$ converges to $\lambda \tilde{a}$.

**Proof**: If $\lambda = 0$ then the sequence is constantly $0$ so clearly it converges to $0$ which is equal to $\lambda \tilde{a}$. Now suppose $\lambda \neq 0$. Let $\epsilon \gt 0$. Since $a_{n}$ converges to $\tilde{a}$, we have for $\frac{\epsilon}{\lambda} \gt 0$ a number $N(\frac{\epsilon}{\lambda})$ such that $\forall n \gt N(\frac{\epsilon}{\lambda})$, $d(a_{n}, \tilde{a}) \lt \frac{\epsilon}{\lambda}$. Consider $d(\lambda a_{n}, \lambda \tilde{a})$:

$$
d(\lambda a_{n}, \lambda \tilde a) = ||\lambda a_{n} - \lambda \tilde{a}||  = |\lambda| ||a_{n}-\tilde{a}|| = \lambda d(a_{n}, \tilde{a}) = \lambda \frac{\epsilon}{\lambda} = \epsilon
$$
Which completes the proof.

Let's move on to a more interesting result.

**Theorem (Limit arithemetics)**: Let $a_{n}, b_{n}$ be sequences in $B$ converging to $\tilde{a}, \tilde{b}$ respectively, then:

$$
\begin{gathered}
1. \quad a_{n} + b_{n} \underset{n \to \infty}{\to} \tilde{a} + \tilde{b} \\
2. \quad a_{n} - b_{n} \underset{n \to \infty}{\to} \tilde{a} - \tilde{b} \\
\end{gathered}
$$

**Proof**:

$1.\quad$ Let $\epsilon \gt 0$. Since $a_{n}$ and $b_{n}$ converge to $\tilde{a}, \tilde{b}$ respectively, then $\exists N_{1}, N_{2} \in \mathbb{N}$ such that $d(a_{n}, \tilde{a}) \lt \frac{\epsilon}{2}$ $\forall n \gt N_{1}$ and $d(b_{n}, \tilde{b}) \lt \frac{\epsilon}{2}$ $\forall n \gt N_{2}$, so by picking $N=\max{N_{1}, N_{2}}$ we have that $\forall N \gt n$, the following holds:

$$
d(a_{n}, \tilde{a}) + d(b_{n}, \tilde{b}) \lt \epsilon
$$
Now, consider the metric $d(a_{n}+b_{n}, \tilde{a} + \tilde{b})$. Recall that this is the induced metric of the norm $||\cdot||$, so we have: 

$$
\begin{gathered}
d(a_{n}+b_{n}, \tilde{a}+\tilde{b}) = \\
||(a_{n}+b_{n})-(\tilde{a}+\tilde{b})|| = ||(a_{n}-\tilde{a}) + (b_{n} - \tilde{b})|| \\
&\leq ||a_{n}-\tilde{a}|| + ||b_{n}-\tilde{b}|| = \\
& d(a_{n}, \tilde{a}) + d(b_{n}, \tilde{b}) = \epsilon
\end{gathered}
$$

Thus $a_{n}+b_{n}$ converges to $\tilde{a}+\tilde{b}$.

$2. \quad$ We use the following lemma:

**Lemma**: $b_{n} \underset{n \to \infty}{\to} \tilde{b} \iff -b_{n} \underset{n \to \infty}{\to} -\tilde{b}$

**Proof**:
$$
d(-b_{n}, -\tilde{b}) = || -b_{n} - (-\tilde{b})|| = |-1|||b_{n} - \tilde{b}|| = ||b_{n} - \tilde{b}|| = d(b_{n}, \tilde{b})
$$

From the lemma and from $(1)$ it follows immediately that the limit of $a_{n}-b_{n}$ is $\tilde{a}-\tilde{b}$.

### Real Line Limit Arithmetics

In this section, we discuss some results which hold in $\mathbb{R}$, but do not necessarily hold, or can even be stated, for higher dimensions.

**Theorem**: Let $a_{n}$ be a sequence in $\mathbb{R}$ such that $a_{n} \gt 0$ except for a finite number of points, then $\lim a_{n} = 0 \iff \lim \frac{1}{a_{n}} = \infty$, i.e. only if $\frac{1}{a_{n}}$ diverges.

**Proof**: Observe the following chain of iff steps for some $N \in \mathbb{N}$ and $\forall n \gt N$:

$$
|a_{n}| \lt \epsilon \iff \frac{1}{|a_{n}|} \gt \frac{1}{\epsilon} \iff \left |\frac{1}{a_{n}}\right | \gt \frac{1}{\epsilon} 
$$
Let $\epsilon \gt 0$. Suppose $\frac{1}{a_{n}}$ diverges to infinity, so $\forall M > 0 \exists N \in \mathbb{N}$ such that if $n \gt N$ then $\frac{1}{a_{n}} \gt M$.  Pick $M$ such that $\epsilon \gt \frac{1}{M}$. Now we have $|a_{n}| \lt \frac{1}{M} \lt \epsilon$ so $a_{n}$ converges to $0$. For the other direction, again let $M \gt 0$ and pick $\epsilon \lt \frac{1}{m}$, now we have (for some $N$ and $n \gt N$) $\left | \frac{1}{a_{n}} \right | \gt \frac{1}{\epsilon} \gt M$ so the reciprocal diverges to infinity.

**Theorem (Limit Arithmetics in $\mathbb{R}$)**: Let $a_{n}$ and $b_{n}$ be convergent sequences in $\mathbb{R}$ that converge to $\tilde{a}, \tilde{b}$ respectively, then the following statements are true:

$$
\begin{gathered}
1. \quad \lim_{n \to \infty}a_{n}b_{n}  = \tilde{a} \tilde{b}\\
2. \quad \lim_{n \to \infty}\frac{a_{n}}{b_{n}} = \frac{\tilde{a}}{\tilde{b}}
\end{gathered}
$$
Where $(2)$ holds only if $\tilde{b} \neq 0$ and $b_{n}$ is non-zero.

**Proof**:

$1. \quad$ Let $\epsilon \gt 0$. By the triangle inequality

$$
d(a_{n}b_{n},\tilde{a}\tilde{b}) \leq d(a_{n}b_{n}, \tilde{b}a_{n}) + d(\tilde{b}a_{n}, \tilde{a}\tilde{b})
$$
Expand the RHS by recalling that the metric is induced by a norm

$$
||a_{n}b_{n} - \tilde{b} a_{n}|| + ||\tilde{b}a_{n} - \tilde{a}\tilde{b}||
$$

Since we are in $\mathbb{R}$, the following transitions are allowed:

$$
||a_{n}(b_{n}-\tilde{b})|| + ||\tilde{b}(a_{n}-\tilde{a})|| = |a_{n}|||b_{n}-\tilde{b}|| + |\tilde{a}|||a_{n} - \tilde{a}|| = |a_{n}|d(b_{n}, \tilde{b}) + |\tilde{a}|d(a_{n}, \tilde{a}) = (*)
$$
Since $a_{n}$ converges then we can find an $M \gt 0$ such that $a_{n} \lt M$ starting from $n \gt N(M)$ for some $N(M) \in \mathbb{N}$, and then we get:

$$
M|d(b_{n},\tilde{b}) + |\tilde{a}|d(a_{n},\tilde{a}) = (**)
$$
Since both sequences converge we can find $N_{a}(\epsilon'), N_{b}(\epsilon')$ such that each metric is less than $\epsilon'$ for all $n_{a} \gt N_{a}(\epsilon'), n_{b} \gt N_{b}(\epsilon')$. Pick $n$ such that $n \gt N=\max\{N(M), N_{a}(\epsilon'), N_{b}(\epsilon')\}$. Now we have

$$
(**) \lt M\epsilon ' + |\tilde{a}|\epsilon' = \epsilon'(M + |\tilde{a}|) = (***)
$$

Notice that $M + |\tilde{a}| \gt 0$ as the sum of two non-negative numbers with $M$ being strictly positive, so we can pick $\epsilon' = \frac{\epsilon}{M + |\tilde{a}|}$ and now we get

$$
(***) = \epsilon
$$
Which completes the proof.

$2. \quad$ We shall prove that if $b_{n}$ converges to $\tilde{b}$ such that $\tilde{b}$ is not zero and $b_{n}$ is non-zero, then its reciprocal converges to $\frac{1}{\tilde{b}}$ (i.e. the reciprocal of the limit), from that and $(1)$ we immediately get $(2)$.

Let $\epsilon \gt 0$. Since $b_{n} \underset{n \to \infty} \to \tilde{b}$,, then for all $\epsilon' \gt 0 \exists N(\epsilon') \in \mathbb{N}$ such that $\forall n \gt N(\epsilon'), d(b_{n}, \tilde{b}) \lt \epsilon'$ , so we have

$$
d(\frac{1}{b_{n}}, \frac{1}{\tilde{b}}) = \left|\left| \frac{1}{b_{n}}-\frac{1}{\tilde{b}} \right|\right| = \left|\left| \frac{\tilde{b}}{b_{n}\tilde{b}} - \frac{b_{n}}{b_{n}\tilde{b}} \right|\right| = \left|\left|\frac{\tilde{b}-b_{n}}{b_{n}\tilde{b}} \right|\right| = \frac{||\tilde{b}-b_{n}||}{||b_{n}\tilde{b}||}\lt\frac{\epsilon'}{||b_{n}\tilde{b}||}=(*)
$$
By a similar argument to the one from $(1)$ we can also find $N(M)$ such that $n \gt N(M) \implies b_{n} \gt M$, pick the larger of these $N$ and we get

$$
(*) \lt \frac{\epsilon'}{M||\tilde{b}||} = (**)
$$
Again we can choose $\epsilon' = M||\tilde{b}||\epsilon$ and then we get $(**)=\epsilon$ which completes the proof.

**Example (Limit of the Babylonian Method)**: Suppose the Babylonian method converges (which requires proof but suppose that it does), then from the uniqueness of the limit we have that $\lim_{n \to \infty}a_{n+1} = \lim_{n \to \infty}a_{n} = L$ for some $L \in \mathbb{R}$, now by limit arithmetics we have

$$
L = \frac{1}{2}(L + \frac{S}{L}) \Rightarrow L^{2} = S \underset{a_{n} \text{ is positive}}{\Rightarrow} L = \sqrt{S}
$$

**Theorem**: Let $a_{n}$ be a sequence in $\mathbb{R}$ which converges to $0$ (the zero vector in $\mathbb{R}$) as $n \to \infty$, and let $b_{n} \in \mathbb{R}$ be a bounded sequence for almost every $n$ (i.e. $\exists M \gt 0 \exists N \in \mathbb{N}$ such that $\forall n \in \mathbb{N}, n \gt N \implies ||b_{n}|| \leq M$), then $a_{n}b_{n} \to 0$.

**Proof**: Let $\epsilon \gt 0$, observe that since $\mathbb{R}^{n}$ is a Banach space the metric $d$ is an induced metric of the norm so

$$
d(a_{n}b_{n}, 0) = ||a_{n}b_{n}|| = ||a_{n}||||b_{n}|| = (*)
$$

Since $b_{n}$ is bounded then $\exists M \gt 0$ and some $N(M) \in \mathbb{N}$ such that $\forall n \gt N(M), ||b_{n}|| \lt M$, so $\forall n \gt N(M)$ we get

$$
(*) \leq M ||a_{n}|| = (**)
$$
Now since $a_{n} \to 0$ then $\forall \delta \gt 0\exists N(\delta) \gt 0$ such that $\forall n \in \mathbb{N}, n \gt N(\delta)$, we have $d(a_{n}, 0)= ||a_{n}|| \lt \delta$, taking $N = \max\{N(\delta), N(M)\}$ we have $\forall n \gt N$

$$
(**) \lt M \delta
$$
Now pick $\delta$ such that $\delta = \frac{\epsilon}{M}$, and now we have  $(**) \lt \epsilon$, which completes the proof.

**Example**: Consider the sequence $a_{n} = \sin(n^{2}+4)$ and $b_{n} = \frac{1}{n^{2}}$ and consider $a_{n}b_{n}$, since $\forall n \in \mathbb{N}, ||a_{n}|| \leq 1$ and $b_{n} \underset{n \to \infty}{\to}{0}$ (one way to prove this is to consider that $b_{n}=\frac{1}{n} \cdot \frac{1}{n}$ and it is trivial to show that $\frac{1}{n}$ converges to $0$ (either directly or  by showing that $c_{n}=n$ diverges to infinity and $b_{n}$ is non-negative so by a previous result $b_{n} \to 0$) so by limit arithemetics in $\mathbb{R}$ we have that $b_{n}$ converges to $0$), so $a_{n}b_{n} \underset{n \to \infty}{\to}0$.

**Theorem (Squeeze theorem)**: Let $a_{n}, b_{n}, c_{n}$ be sequences in $\mathbb{R}$ such that $a_{n}, c_{n}$ both converge to the same limit $L \in M$, and let $(\mathbb{R}, \leq)$ be the usual total order on $\mathbb{R}$, then if $\exists N \in \mathbb{N}$ such that $\forall n \gt N, a_{n} \leq b_{n} \leq c_{n}$, it follows that $b_{n}$ also converges to $L$.

**Proof**: Observe that $a_{n} \leq b_{n} \leq c_{n} \implies a_{n} - L \leq b_{n} - L \leq c_{n} - L$, and by limit arithemetics $a_{n}-L, c_{n}-L$ converge to $0$, so it suffices to show that $b_{n}-L$ converges to $0$. Consider the sequences $a'_{n}=a_{n}-L, b'_{n}=b_{n}-L, c'_{n}=c_{n}-L$, then it suffices to show that if $a'_{n} \leq b'_{n} \leq c'_{n}$ for almost all $n \in \mathbb{N}$ and if $(a'_{n}, c'_{n}) \underset{n \to \infty} \to (0, 0)$,  then $b'_{n} \underset{n\ to \infty}{\to} 0$.

Let $\epsilon \gt 0$, observe that $d(b'_{n}, 0)=|b'_{n}|$, so we need to show that $\exists N(\epsilon) \in \mathbb{N}$ such that $\forall n \gt N(\epsilon), |b'_{n}| \lt \epsilon$, which is equivalent to $-\epsilon \lt b'_{n} \lt \epsilon$. Now, since $a'_{n} \to 0, \exists N_{1}(\epsilon) \in \mathbb{N}$ such that $|a'_{n}| \lt \epsilon$, i.e. $-\epsilon \lt a'_{n} \lt \epsilon$, for all $n \gt N_{1}(\epsilon)$. A similar result holds for $c'_{n}$ with some $N_{2}(\epsilon) \in \mathbb{N}$, and by taking $N(\epsilon)=\max\{N, N_{1}(\epsilon), N_{2}(\epsilon)\}$ (recall that $N$ is the index from which the inequality holds), we get by the properties of a (total) order relation:

$$
-\epsilon \lt a'_{n} \lt b'_{n} \lt c'_{n} \lt \epsilon \Rightarrow -\epsilon \lt b'_{n} \lt \epsilon \Rightarrow |b'_{n}| \lt \epsilon 
$$
$\square$

## Compactness

**Definition (bounded set)**: A subset $A$ in a metric space $(M, d)$ is **bounded** if any of the following conditions is met:
1. $\exists r \gt 0 \in \mathbb{R}$ such that $\forall a, b \in A, d(a, b) \lt r$.
2. $\exists r \gt 0 \in \mathbb{R}, \exists a_{0} \in A$ such that $A \subseteq B_{r}(a_{0})$. 

$(1) \implies (2)$: Consider an ordering on $A$. Take $a_{0} \in A$. Now, $\forall a_{i} \in A$, we have by $(1)$ that $\exists r_{i} \gt 0 \in \mathbb{R}$ such that $d(a_{0}, a_{i}) \lt r_{i}$. Denote $r =\sup_{i \in |A|}r_{i}$ (it is easy to see that the supremum is attained). Now we have an open ball $B_{r}(a_{0})$ which by construction contains all points in $A$, so we are done.

$(2) \implies (1)$ : By the triangle inequality we have $\forall a, b, a_{0} \in A$:

$$
d(a, b) \leq d(a, a_{0}) + d(a_{0}, b)
$$
By $(2)$, we have $d(a, a_{0}) \lt r$ and $d(a_{0}, b) \lt r$, so we have

$$
d(a, b) \lt 2r
$$
Take $r_{a, b} = 2r$ and we have $(1)$.

**Definition (sub-sequence)**: Let $A$ be a set. A sub-sequence $(a_{n})_{k}: \mathbb{N} \to A$ of a sequence $a_{n}: \mathbb{N} \to A$ if the composition of a mapping $n_{k}: \mathbb{N} \to \mathbb{N}$ with the mapping $a_{n}$, i.e. it is a subset of the image of $a_{n}$.

**Example**: Consider some set $a_{n}=n$. Now, consider the sub-sequence $b_{n} = a_{2n}$, which is the sequence of all even indexed elements of $a_{n}$. We can also write $b_{n}=2n$.

Less formally, a sub-sequence of a sequence is any "infinite" number of points in the sequence, which can then be re-ordered to a new sequence via the map $n_{k}: k \mapsto n$.

**Proposition**: Let $(a_{n})_{k}$ be a sub-sequence of $a_{n}$ in a metric space $(M, d)$. If $a_{n}$ converges to a limit $\tilde{a}$ in $M$, then $(a_{n})_{k}$ also converges to the same limit $\tilde{a}$. 

**Proof**: Let $\tilde{a} = \lim_{n \to \infty} a_{n}$. Let $\epsilon \gt 0$, then $\exists N \in \mathbb{N}$ such that $\forall n \gt N, d(a_{n}, \tilde{a}) \lt \epsilon$. Consider the sub-sequence $(a_{n})_{k} \subseteq a_{n}$. Assume $(a_{n})_{k}$ does not converge $\tilde{a}$, then there is no $N'$ for which $\forall k \gt N', (a_{n})_{k} \in B_{\epsilon}(\tilde{a})$, but this means that there are infinitely many such $(a_{n})_{k}$ outside of $B_{\epsilon}(\tilde{a})$, which means that there are infinitely many $a_{n}$ elements outside of $B_{\epsilon}(\tilde{a})$, but there can be at most $N$ such elements, so we have a contradiction, so $(a_{n})_{k}$ must also converge to $a_{n}$. 

Note that the converse is not true (consider for example the following sequence: $a_{n} = \begin{cases} n & n \text{ is odd} \\ \frac{1}{n} & n \text{ is even} \end{cases}$, consider the sub-sequence $a_{2n}$ - clearly it converges to $0$, yet in general $a_{n}$ diverges.

**Definition (compact space)**: Let $\tau$ be a topology on a set $X$, then the topological space $(X, \tau)$ is **compact** if every open cover of $X$ has a finite sub-cover.

**Definition (sequentially compact metric space)**: Let $(M, d)$ be a metric space. We say it is **sequentially compact** if every sequence in $M$ has a convergent sub-sequence.

**Theorem**: In a metric space $M$, the following are equivalent:
1. $M$ is compact.
2. $M$ is sequentially compact.

**Proof**: $(1) \implies (2)$ : Assume $M$ is compact. Consider an $\epsilon$- cover of $M$, $C_{\epsilon}$. Since $M$ is compact and $C_{\epsilon}$ is open, then $C_{\epsilon}$ has a finite sub-cover of $M$, denote it $D_{\epsilon}$. Now consider a sequence $a_{n}$. There are infinitely many such $a_{n}$, and since $D_{\epsilon}$ is a cover of $M$ it is also a cover of the set of all $a_{n}$ so by the pigeonhole principle since $D_{\epsilon}$ is finite and $a_{n}$ is infinite there exists at least one member of $D_{\epsilon}$ such that it contains infinitely many elements of $a_{n}$, for an arbitrarily small $\epsilon$, so we have at least one $B_{\epsilon}(x)$ for some $x \in M$ such that $B_{\epsilon}(x)$ is an **accumulation point** of $a_{n}$ (i.e. it contains infinitely many members of $a_{n}$), denote the set of points $(a_{n})_{k}$, such that $\forall k \in \mathbb{N}, (a_{n})_{k} \in B_{\epsilon}(x)$, and now we have a sequence $(a_{n})_{k}$ which is a sub-sequence of $a_{n}$ and converges to $x$, so $M$ is also sequentially compact.

$(2) \implies (1)$:  Assume $M$ is sequentially compact. Let $C$ be an open cover of $M$. Consider a sequence $a_{n}$, since $M$ is sequentially compact by assumption it has a convergent sub-sequence $(a_{n})_{k}$, i.e. it has an accumulation point around some $x \in M$ such that $\forall \epsilon \gt 0, B_{\epsilon}(x)$ contains infinitely many members of $(a_{n})_{k}$ and thus of $a_{n}$. Denote the limit of the sub-sequence as $\tilde{a_{k}}$. Since $C$ is a cover of $M$ and $\tilde{a_{k}} \in M$, then $C$ contains at least one member $C_{\tilde{a_{k}}}$ such that for a small enough epsilon, $B_{\epsilon}(\tilde{a_{k}}) \subseteq C_{\tilde{a_{k}}} \subseteq C$ (since $C$ is an open cover and so each member and in particular $C_{\tilde{a_{k}}}$ is open, otherwise this would not hold).

Pick some $\epsilon \gt 0$. Now, pick some $a_{1} \in M$, then consider $M \setminus B_{\epsilon}(a_{1})$, and pick $a_{2}$ from that set, then consider $M \setminus (B_{\epsilon}(a_{1}) \cup B_{\epsilon}(a_{2}))$, and so on. Assume that all such $\epsilon$-balls do not form a finite cover (we know they form an open $\epsilon$-cover from a previous result), so we have a sequence $a_{n}$ such that $\forall n, k \in \mathbb{N}, d(a_{n}, a_{k}) \gt \epsilon$ by construction for the $\epsilon$ we picked, which means that the sequence is not Cauchy, but by assumption $M$ is sequentially compact, so $a_{n}$ must have a convergent sub-sequence, but that sub-sequence is also not Cauchy so cannot converge, which is a contradiction, so the $\epsilon$-cover as constructed must be finite, and now by the result from the first paragraph we know that each such $B_{\epsilon}(a_{n})$ is contained in (at least) one such $C_{\tilde{a_{n}}}$, and since the $\epsilon$-cover is contained in the union of all such $C_{\tilde{a_{n}}}$, then the union of that set is also a finite open cover of $M$, which is a sub-cover of $C$, so $M$ is compact.

**Theorem**: Let $(M, d)$ be a metric space and let $A \subseteq M$ be a compact subspace, then $A$ is closed and bounded.

**Proof**: We break the theorem to two lemmas, one claiming that such $A$ is closed and the other that such $A$ is bounded, so by combining the results of both lemmas we get the original theorem.

**Lemma 1**: $A$ is closed.

**Proof of Lemma 1**: Recall $A$ is closed $\iff$ every convergent sequence in $A$ has its limit in $A$. Suppose $A$ is compact but not closed, then $A$ has a convergent sequence $a_{n}$ such that it has an accumulation point at some point $\tilde{a} \in M \setminus A$. Consider some $\epsilon$-cover $C$ of $M$ (which is also a cover of $A$ since $A \subseteq M$) such that $\epsilon$ is small enough to not intersect some $\epsilon_{\tilde{a}}$ neighbourhood of $\tilde{a}$, now since $A$ is compact it has a finite sub-cover $C'$. Since we constructed $C$ such that it does not intersect some $\epsilon_{\tilde{a}}$ neighbourhood of $\tilde{a}$, then we know that $B_{\epsilon_{\tilde{a}}}(\tilde{a}) \cup (\cup^{c' \in C'} c') = \emptyset$,  but since $\tilde{a}$ is an accumulation point of $a_{n}$ there must exist infinitely many elements of $a_{n}$ in $B_{\epsilon{\tilde{a}}}(\tilde{a})$, but then we have some element $x$ not in the cover $C'$, which contradicts $C'$ being a cover of $A$, which means that if $A$ is compact then all convergent sequences in $A$ have their limit in $A$, which means that $A$ is closed.

**Lemma 2**: $A$ is bounded.

**Proof of Lemma 2**: Let $\epsilon=1$. Consider an $\epsilon$-cover of $M$, which has a finite sub-cover $C$ since $M$ (and of $A$) is compact. Each element of $C$ is contained in some element of $M$, so each element is contained in some $\epsilon$-ball with $\epsilon=1$. Since $C$ is finite, we also have a finite set of corresponding balls. Now consider the maximal distance between two ball centres. Since $C$ is finite it is attained. Denote this distance $K$, and the respective centres $C_{1}, C_{2}$. Let $p, q \in A$. Either they are not in the balls around $C_{1}, C_{2}$ so they must be closer than any two points in these balls, or they are inside these balls. In that case, by the triangle inequality we have

$$
d(p, q) \leq d(p, C_{1}) + d(C_{1}, q) \leq d(p, C_{1}) + d(C_{1}, C_{2}) + d(C_{1}, q) \leq 1 + K + 1 = K + 2
$$
So we have that $\forall p, q \in A$ the distance is finite, so $A$ is bounded.

**Proposition**: A closed subset of a compact set is compact.

**Proof**: Let $A$ be a compact set in a metric space $(M, d)$ and let $B \subseteq A$ be a closed subset in $A$, then $B^{c} = M \setminus B$ is an open set in $M$. Let $C_{B}$ be an open cover of $B$, and consider the set $C_{A} = C_{B} \cup \{ B^{c}\}$, which is an open cover of $M$ since $B \subseteq \cup C_{B}$ and $B^{C}=M \setminus B$ and in particular of $A$ since $A \subseteq M$.  Now since $A$  is compact, then $C_{A}$ also admits a finite sub-cover of $A$, $C'_{A}$, which is also a finite cover of $B$, but since $B^{C} \cap B = \emptyset$ by definition, then the part of $C'_{A}$ which covers $B$ must be a subset of $C_{B}$, and since $C'_{A}$ is finite then that subset is also finite so it is a finite sub-cover of $C_{B}$, which is a general open cover of $B$, which completes the proof.

### Compactness in Euclidean Space

The notion for compactness and sequential compactness comes from two famous results in real analysis - the Heine-Borel theorem and the Bolzano-Weierstrass theorem.

First, we shall present these theorems in $\mathbb{R}$, i.e. the real line.

**Theorem (Heine-Borel in $\mathbb{R}$)**: Let $S \subseteq \mathbb{R}$, the following are equivalent:

1. $S$ is closed and bounded.
2. $S$ is compact.

**Proof**: $\mathbb{R}$ is a metric space so by a previous result $(2) \implies (1)$, so we will only prove that $(1) \implies (2)$. Suppose $S$ is closed and bounded. By the proposition from the last section it suffices to show that $S$ is a subset of a compact set, so as a closed subset of a compact set it is also compact. Since $S$ is bounded in $\mathbb{R}$, then there exists an interval $I = [-a, a]$ for some $a \gt 0$ such that $S \subseteq I$. By proving $I$ is compact, we prove that $S$ is compact as a closed subset of $I$.

Suppose $I$ is not compact, then there exists an open cover $C$ of $I$ which admits no finite sub-cover. Denote $I_{0}=I$. Bisect $I$ to two parts of equal length $[-a, 0]$ and $[0, a]$. Since $C$ has no finite sub-cover then at least one of the two parts of $I$ must not have a finite sub-cover in $C$, otherwise $C$ admits a finite sub-cover of $I$ as the union of two finite sub-covers, denote it $I_{1}$. By the same argument, after bisecting $I_{1}$ it must have at least one half which has no finite sub-cover in $C$, denote it $I_{2}$, and repeat ad infinitum. Observe that the length of $I_{n+1}$ is half the length of $I_{n}$, which is initially $d(a, -a)=2a$, so the sequence of lengths of the intervals is given by $|I_{n}| = \frac{a}{2^{n-1}}$, which converges to $0$ as $n \to \infty$ in $\mathbb{R}$. Also observe that by this construction we have $\forall n \in \mathbb{N}, I_{n} \subset I_{n+1}$.

Now, consider a sequence $a_{n}$ such that $a_{n} \in I_{n}$. Observe that, we have that $d(a_{n}, a_{n+1}) \leq |I_{n}| = \frac{a}{2^{n-1}}$ which converges to $0$ so $a_{n}$ is a Cauchy sequence and since $I$ is closed then it converges to a limit $L$ and since $I_{n}$ is closed then $a_{n}$ is closed so it must converge to $L$ in $I_{n}$ and since $I_{n} \subseteq I_{n+1}$ then $\forall n \in \mathbb{N}, L \in I_{n}$, i.e. all intervals have a shared point. This result is also known as **Cantor's intersection theorem**.

Since $C$ is an open cover of $I$ then it has some member $U \in C$ such that $L \in U$, and since $U$ is open then $\exists \epsilon \gt 0$ such that $B_{\epsilon}(L) \subseteq U$. Now since $|I_{n}| \to 0$, then there exists a large enough $n$ such that $I_{n} \subseteq B_{\epsilon}(L) \subseteq U$, but then $I_{n}$ has a finite cover in $U$, which is a single element of $C$, but we constructed the sequence $I_{n}$ such that each interval does not have a finite cover in $C$, which is a contradiction, so every open cover of $I$ has a finite sub-cover, so $I$ is compact, so $S$ is compact and we are done.  

**Theorem (Heine-Bore in $\mathbb{R}^{n}$)**: Let $S \subseteq \mathbb{R}^{n}$, the following are equivalent:
1. $S$ is closed and bounded.
2. $S$ is compact.

**Proof Sketch**: The proof for the one-dimensional case generalizes without any changes except for changes in terminology (instead of intervals we consider $n$-boxes $[-a, a]^{n}$).

**Theorem (Bolzano-Weierstrass (BW) in $\mathbb{R^{n}}$)**: Let $A$ be a set in $\mathbb{R}^{n}$. If $A$ is closed and bounded, then every sequence $a_{n} \in A$ has a convergent sub-sequence also in $A$.

**Proof**: Observe that BW can be stated as follows - if a set in $\mathbb{R}^{n}$ is closed and bounded then it is sequentially compact. Now, from Heine-Borel we have that if a set in $\mathbb{R}^{n}$ is closed and bounded then it is compact, and by a previous result we know that in a metric space a set is compact $\iff$ it is sequentially compact, so BW and Heine-Borel are equivalent in a metric space, and since $\mathbb{R}^{n}$ is a metric space one immediately implies the other, which completes the proof.

Given BW in $\mathbb{R}$ (which can be proved from Heine-Borel in $\mathbb{R}$ or from Cantor's interesction theorem) we can generalize BW to $\mathbb{R}^{n}$ using our previous result that a sequence $a_{n}$ in $\mathbb{R}^{n}$ converges to a limit $\tilde{a}$ if and only if all sequences $a_{n_{i}}$ converge to $\tilde{a}_{i}$ for $i \in \mathbb{N}$ ranging from $1$ to $n$. 

**Alternative proof**: Let $A \subseteq \mathbb{R}^{n}$ be closed and bounded and let $a_{n}$ be a sequence in $A$. We prove the theorem by an iterative argument. If $n=1$ this is trivial, otherwise consider the sequence $a_{1_{n}} \in \mathbb{R}$ which is the sequence of the first element of $a_{n}$. Since $a_{n}$ is closed and bounded, then this sequence is also closed and bounded, and now by BW in $\mathbb{R}$ we know that it has a convergent sub-sequence $a_{1_{n_{k}}}$.  Now consider the sub-sequence $a_{2_{n_{k}}}$ in $\mathbb{R}$ which is also bounded as a sub-sequence of a bounded sequence $a_{2_{n}}$,  so again by BW in $\mathbb{R}$ we have that it has a convergent sub-sequence $a_{2_{n_{k_{j}}}}$. Notice that since $a_{1_{n_{k}}}$ is convergent then $a_{1_{n_{k_{j}}}}$ is also convergent as a sub-sequence of a convergent sequence and to the same limit as $a_{1_{n_{k}}}$, so by the previous result on convergence of elements in an $n$-tuple we have that $(a_{1_{n_{k_{j}}}}, a_{2_{n_{k_{j}}}})$ also converges. Observe that it is a sub-sequence of $(a_{1_{n}}, a_{2_{n}})$, so if $n=2$ we are done. Otherwise, repeat the same argument until you get to $n$.

## Functions and Limits

**Definition (function in metric spaces)**: Let $(A, d_{A})$ and $(B, d_{B})$ be metric spaces and let $M \subseteq A$ and $N \subseteq B$. A mapping $f: M \to N$ is called a **function**.

**Definition (deleted neighbourhood of a point)**: Let $x \in A$. Consider a neighbourhood of $x$, $B(x)$ and consider the set $B(x) \setminus x$, i.e. a neighbourhood of $x$ with $x$ removed. We say that $B(x) \setminus x$ is a **deleted neighbourhood** of $x$. 

**Example (deleted neighbourhood)**: Let $B_{\epsilon}(x)$ be an $\epsilon$-ball around $x$ is some metric space $A$. Then the deleted neighbourhood $B_{\epsilon}(x) \setminus x$ can also be expressed as $\{ a \in A | 0 \lt d(a, x) \lt \epsilon \}$.

**Definition (limit of a function)**: Let $A, B$ be two metric spaces and let $M \subseteq A, N \subseteq B$ with a function $f: M \to N$. Let $p \in A$ be an accumulation point of $M$ and let $x \in M$ and $L \in N$. If the following property holds:

$\forall \epsilon \gt 0, \exists \delta \gt 0,$ such that $\forall x \in M$,  if $x$ satisfies $0 \lt d_{A}(x, p) \lt \delta$ (i.e. all $x$ in a deleted $\delta$-neighbourhood of $p$), then $d_{B}(f(x), L) \lt \epsilon$. 

If the property holds, we say that $L$ is the limit of $f$ as $x \to p$ is $L$, and write

$$
\lim_{x \to p}f(x) = L
$$

**Fact**: The limit of a function at a point is unique.

**Proof**: Suppose not, then take $L_{1}, L_{2} \in N$ such that both are limits of $f$ at $p$. Take $\epsilon = \frac{1}{2}d_{B}(L_{1}, L_{2})$, then $\exists \delta \gt 0$ such that $\forall x \in M$ if $x$ is in a deleted $\delta$-neighbourhood of $p$ then for $i \in \{1, 2\}$, 

$$
d_{B}(f(x), L_{i}) \lt \frac{1}{2}d_{B}(L_{1}, L_{2})
$$
By summing over $i$ we get

$$
d_{B}(f(x), L_{1}) + d_{B}(f(x), L_{2}) \lt d_{B}(L_{1}, L_{2})
$$
but by the triangle inequality by the symmetry of the metric we have

$$
d_{B}(L_{1}, L_{2}) \leq d_{B}(f(x), L_{1}) + d_{B}(L_{2}, f(x)) = d_{B}(f(x), L_{1}) + d_{B}(f(x), L_{2}) \lt d_{B}(L_{1}, L_{2})
$$

Which is a contradiction.

**Theorem**: Let $X_{n}$ be the class of all sequences $x_{n} \in A$ that converge to $p \in A$ and are not $p$, and let $f: A \to B$ be a function from a metric subspace $A$ to a metric subspace $B$, then

$$
\exists L \in B, \lim_{x \to p}f(x) = L \iff \forall x_{n} \in X_{n}, \lim_{x_{n} \to p}f(x_{n}) = L
$$
**Proof**: $\implies$ Since $f$ has a limit at $p$ then $\forall \epsilon \gt 0 \exists \delta \gt 0$ such that $\forall x \in A$ s.t. $0 \lt d(x, p) \lt \delta$, implies $d(f(x), L) \lt \epsilon$ for some $L \in B$.. Let $x_{n}$ be a sequence in $A$ that converges to $p$, then for $\delta$ we can find $N(\delta)$ such that $\forall n \gt N(\delta), d(x_{n}, p) \lt \delta$, and since we assume $x_{n} \neq p$ then $d(x_{n}, p) \gt 0$ so we have that $x_{n}$ satisfies the condition on $x$ so $f(x_{n}) \underset{n \to \infty}{\to}L$.
$\impliedby$ Let $\epsilon \gt 0$ and let $\delta = \frac{1}{n}$ and consider a deleted $\delta$-ball centred at $p$ and consider the sequence of such $\delta$-balls. There exists a point $x \in A$ at each $\delta$-ball, denote it $x_{n}$, now we have a sequence of points $x_{n}$ such that $d(x_{n}, p) \lt \frac{1}{n}$ which tends to $0$ as $n \to \infty$, so we have a convergent sequence whose limit is $p$, and by assumption $f(x_{n}) \underset{x_{n}\to p}{\to}L$, but we can construct such $x_{n}$ sequence by considering any point at any such $\delta$-ball, so $f(x) \underset{x \to p} \to L$ by definition.

**Corollary**: This theorem provides an analytical way of proving that $f$ has no limit as $x \to p$ by showing that for two convergent sequences $a_{n}, b_{n}$ such that $\lim_{n \to \infty} a_{n} = \lim_{n \to \infty} b_{n} = p$, we have $\lim_{a_{n} \to p}f(a_{n}) \neq \lim_{b_{n} \to p}f(b_{n})$.  Similarly, if we find a sub-sequence $a_{n} \underset{n \to \infty}{\to}p$ such that $\lim_{n \to \infty}f(a_{n})$ does not exist, then $f$ does not have a limit at $p$.

**Proposition**: Let $f: M \to N$ be a constant function $\forall x \in M, f(x) = L$ for some $L \in N$, then the limit of $f$ at any point is $L$.

**Proof**: Let $\epsilon \gt 0$ and let $p \in \mathbb{M}$. $\forall \delta \gt 0$ and $\forall x \in M$ such that $x$ is in a deleted $\delta$-neighbourhood of $p$, we have $d(f(x), L) = d(L, L) = 0 \lt \epsilon$, so the limit of $f$ is $L$ everywhere.

**Definition (composition of functions)**: Let $A, B, C$ be metric spaces and let $f: A \to B$ and $g: B \to C$ be functions. The composition of $f$ and $g$ is an operator $\circ$ which takes $f$ and $g$ and o and produces a function $h= g \circ f: A \to C$ such that $h = g(f(x))$.

Given that:

$$
\lim_{x \to p_{0}}f(x) = p , \quad \lim_{x \to p}g(x) = L
$$

One may be tempted to consider $\lim x_{\to p_{0}}(g \circ f)(x)$ and declare that it must be $L$. This is *not* true in the general case. As a counter-example, consider

$$
g(x) = \begin{cases} a & x = p \\ L & x \neq p
\end{cases}
$$
For some $a, L \in C, a \neq L$. Then it is trivial to see that $g(x) \underset{x \to p}{\to}L$, however $g(p) = a \neq L$. Now consider $h=g \circ f$ as $x \to p_{0}$, we have $h(x) \underset{\lim x \to p_{0}}{\to} h(p) = a \neq L$.

Later on, when we present the notion of continuity and continuous functions, we will see that by also requiring that $g$ continuous at $p$, then this assumption is correct.

### Limit of a function on $\mathbb{R}$

Let us consider a couple of example to show how one may use the $\epsilon-\delta$ definition of a limit to find the limit of a function at a point.

**Exercise 1**: Let $f: \mathbb{R} \to \mathbb{R}$ be given by $f(x) = \sin\frac{1}{x}$. Show that $\lim_{x \to 0}f(x)$ does not exist.

**Solution**: One way to disprove the existence of a limit is to show two sequences $a_{n}, b_{n}$ in $\mathbb{R}$ that converge to $0$ such that $\lim_{a_{n}\to 0}f(a_{n}) \neq \lim_{b_{n} \to 0}f(b_{n})$. In particular, consider the sequences

$$
\begin{gathered}
a_{n} = \frac{1}{\pi n} \\
b_{n} = \frac{1}{\pi(2n + \frac{1}{2})}
\end{gathered}
$$
Both converge to $0$ as $n \to \infty$. Under $f$, each sequence gives

$$
\begin{gathered}
f(a_{n}) = \sin(\pi n) = 0 \\
f(b_{n}) = \sin(2 \pi n + \frac{\pi}{2}) =  sin(\frac{\pi}{2}) = 1
\end{gathered}
$$
By now we have two constant functions and by a previous result we know that $f(a_{n}) \underset{n \to \infty}{\to} 0$ and $f(b_{n}) \underset{n \to \infty}\to 1$, but $0 \neq 1$, so $f$ has no limit at $0$.

**Exercise 2**: Show that $f: \mathbb{R} \to \mathbb{R}$ given by $f(x) = x \sin \frac{1}{x}$ has a limit as $x \to 0$.

**Solution**:  Let $\epsilon \gt 0$. Observe that $|\sin\frac{1}{x}| \leq 1$, so

$$
d(f(x), 0) = |x \sin \frac{1}{x}| = |x| |\sin\frac{1}{x}| \leq |x|
$$
Let $\delta = \epsilon$, then $\forall x \in \mathbb{R}$ such that $0 \lt ||x - 0|| = |x| \lt \delta = \epsilon$, we have $d(f(x), 0) \lt \epsilon$, so $f(x)$ converges to $0$ as $x \to 0$.

**Definition (one-sided limit)**: Let $f: \mathbb{R} \to \mathbb{R}$ and let $x_{0} \in \mathbb{R}$ be some point, then $f$ has two **one-sided-limits** at $x_{0}$, defined as follows:
1. $\exists L^{+} \in \mathbb{R}$ such that $\forall \epsilon \gt 0 \exists \delta \gt 0, \forall x \in \mathbb{R}, (0 \lt x- x_{0} \lt \delta \implies d(f(x), L^{+})) \lt \epsilon)$.
2. $\exists L^{-} \in \mathbb{R}$ such that $\forall \epsilon \gt 0 \exists \delta \gt 0, \forall x \in \mathbb{R}, (0 \lt x_{0}- x \lt \delta \implies d(f(x), L^{-})) \lt \epsilon)$.

We denote $L^{+}=\lim_{x \to x_{0}^{+}}f(x)$ and $L^{-}=\lim_{x \to x_{0}^{-}}f(x)$.

Put less rigorously, consider that there are two directions in which we can approach a point on the real line, either in the positive direction (i.e. by a sequence of points whose norm is larger than that of the point) or in the negative direction. When defining the limit of a function at a point we require that the limit will be attained in approaches in both points (since we require that $d(f(x), L) \lt \epsilon$ for all $x \in B_{\delta}(x_{0}) \setminus \{x_{0}\}$). The one-sided limits are a more relaxed definition, where we only consider values to one side of the point.

**Theorem**: Let $x_{0} \in \mathbb{R}$ and $f: \mathbb{R} \to \mathbb{R}$. The limit of $f$ at $x_{0}$ is attained $\iff$ the one-sided limits of $f$ at $x_{0}$ are attained and agree. In that case, the one-sided limit at $x_{0}^{+}, x_{0}^{-}$ are equal to the limit of the function at $x_{0}$.

**Proof**: $\implies$ This direction is trivial since for the limit of a function to be attained at $x_{0}$ we require a stronger condition, and clearly if $0 \lt x - x_{0} \lt \delta$ then $x \in B_{\delta}(x_{0}) \setminus {x_{0}}$, and similarly for the limit approaching $x_{0}$ from the negative side. $\impliedby$ Let $\epsilon \gt 0$, then for the one-sided limit in the positive direction $\exists \delta_{1} \gt 0$ for which $d(f(x), L^{+}) \lt \epsilon$ for all $0 \lt x - x_{0} \lt \delta_{1}$, and similarly $\exists \delta_{2} \gt 0$ for the approach in the negative direction for which $0 \lt x_{0} - x \lt \delta_{2}$ implies $d(f(x), L^{-}) \lt \epsilon$.  By taking $\delta = \min\{\delta_{1}, \delta_{2}\}$ we have a $\delta$ for which both conditions are met, now suppose $L^{-}=L^{+}=L$, then we have that $\forall x \in \mathbb{R}, (0 \lt ||x_{0}-x|| \lt \delta \implies d(f(x), L) \lt \epsilon)$, which means that $f(x) \underset{x \to x_{0}} \to L$ by definition, which completes the proof.

### Limit of a function on $\mathbb{R}^2$

Let us proceed with a couple of examples from $\mathbb{R}^{2}$.

**Exercise 1**: Let $f: \mathbb{R}^{2} \to \mathbb{R}$ be defined as

$$
f(x, y) = \begin{cases} x \sin \frac{1}{y} + y \sin \frac{1}{x} & xy \neq 0 \\ 0 & xy = 0\end{cases}
$$
Show that $f(x, y) \underset{(x, y) \to (0, 0)}{\to} 0$.

**Solution**: We use $d$ for both the metric of $\mathbb{R}$ and the metric of $\mathbb{R}^{2}$ since the metric in $\mathbb{R}^{2}$ can also be used in $\mathbb{R}$ by taking $y=0$. 

Let $\epsilon \gt 0$.  Consider $d(f(x, y), 0)$. If $xy=0$ then $d(f(x, y), 0) = 0$ for all such $x, y$ so for any such sequence that maintains $xy=0$ the limit is trivially $0$, otherwise we have 

$$
d(f(x, y), 0) = \left|x\sin\frac{1}{y}+y\sin\frac{1}{x}\right| \underset{\triangle \text{ inequality}}{\leq} \left|x \sin \frac{1}{y} \right| + \left| y \sin \frac{1}{x}\right| \leq |x|\left| \sin \frac{1}{y} \right| + |y| \left| \sin \frac{1}{x} \right| \leq |x| + |y|
$$
Observe that the last term is just $d(x, 0) + d(y, 0)$. 

From a previous result on limits of sequences in $\mathbb{R}^{n}$ we know that $(x, y) \to (0, 0) \iff x \to 0 \land y \to 0$, so we can take some $\delta=\frac{\epsilon}{2}$ and have $d(x, 0) \lt \delta$, $d(y, 0) \lt \delta$, so we have

$$
d(f(x, y), 0) \lt d(x, 0) + d(y, 0) = \epsilon
$$
So $f(x, y) \underset{(x, y) \to (0, 0)}{\to} 0$.

**Exercise 2**: Let $f: \mathbb{R}^{2} \to \mathbb{R}$ be defined as $f(x, y) = \frac{xy}{x^2+y^{2}}$. Show that $f(x, y)$ has no limit at the origin.

**Solution**: Take the sequence $(x_{n}, x_{n})$ where $x_{n}=\frac{1}{n}$, since $x_{n} \to 0$ then $(x_{n}, x_{n}) \to (0, 0)$ so if $f$ has a limit at the origin then $f(x_{n}, x_{n})$ has the same limit as $n \to \infty$. Now we have

$$
f(x_{n}, x_{n}) = \frac{x_{n}^{2}}{2x_{n}^{2}} = \frac{1}{2}
$$
Which is a constant function, so $f(x_{n},x_{n}) \underset{n \to \infty}{\to}\frac{1}{2}$. Now consider the sequence $(x_{n}, 0)$ for the same $x_{n}$, again $(x_{n}, 0) \underset{n \to \infty}{\to} (0, 0)$, but now $f(x_{n}, 0) = 0$ so $f(x_{n}, 0) \underset{n \to \infty}{\to} 0$, but $0 \neq \frac{1}{2}$, so $f$ doesn't have a limit at the origin.

**Exercise 3**: Show that $f(x, y) = \frac{x+y}{x^2+y^{2}}$ has no limit at the origin.

**Solution**: Again take a sequence such that $x=y$, then we have $f(x, x) = \frac{1}{x}$ which diverges as $x \to 0$ so $f$ has no limit at the origin.

### Function limit arithemetics

In an analogous manner to sequences, we also have limit arithmetics theorems for limits of functions with very similar proofs.

**Theorem**: Let $M, N$ be normed vector spaces and let $f: M \to N$ and $g: M \to N$ be two functions which both have limits $L_{1}, L_{2} \in N$ at $p \in M$, then:

$$
\lim_{x \to p}(f(x) \pm g(x)) = \lim_{x \to p}f(x) \pm \lim_{x \to p}g(x) = L_{1} \pm L _{2}
$$
**Proof**: We present a proof only for the case of $f(x)+g(x)$. The proof for $f(x)-g(x)$ is analogous. Let $\epsilon \gt 0$. Observe that

$$
\begin{gathered}
d(f(x)+g(x), L_{1} + L_{2}) = ||f(x) + g(x) - (L_{1} + L_{2})|| = \\|| (f(x) - L_{1}) + (g(x) - L_{2}) || \leq ||f(x) - L_{1}|| + ||g(x) - L_{2}|| = \\
& d(f(x), L_{1}) + d(g(x), L_{2}) = (*)
\end{gathered}
$$
Since $f$ and $g$ converge to $L_{1}$ and $L_{2}$ respectively at $p$, then $\forall \epsilon' \gt 0$,  $\exists \delta \gt 0$ such that

$$
d(f(x), L_{1}) \lt \epsilon' \iff 0 \lt d(x, p) \lt \delta
$$

and  analogously for $g(x)$. In particular, take $\epsilon' = \frac{\epsilon}{2}$ and the smaller of the two $\delta$ values, and now e have $(*) = \epsilon$ for some $\delta \gt 0$, which completes the proof.

If $M, N \subseteq \mathbb{R}$, then we also have the following extension to the theorem:

**Theorem**: Let $M, N \subseteq \mathbb{R}$ and let $f, g$ be functions $M \to N$ with limits $L_{1}, L_{2} \in N$ at $p \in M$, then:

$$
\begin{gathered}
1. \quad \lim_{x \to p}(f(x)g(x))= (\lim_{x \to p}f(x))(\lim_{x \to p}g(x)) = L_{1}L_{2} \\\\
2. \quad g(x) \gt 0, L_{2} \neq 0 \implies \lim_{x \to p} \frac{f(x)}{g(x)} = \frac{L_{1}}{L_{2}}
\end{gathered}
$$
The proof for this theorem is analogous to the proof presented for the version of the theorem that deals with sequences, so it is left out for brevity.

**Theorem**: Let $f: \mathbb{R}^{k} \to \mathbb{R}$ and let $g: \mathbb{R}^{k} \to \mathbb{R}$ such that both have a limit at some point $p \in \mathbb{R}^{k}$. If the limit of $f$ at $p$ is $0$, and $g$ is bounded in a deleted neighbourhood of $g$, then $f(x) \cdot g(x) \underset{x \to p}{\to} 0$.

**Note**: this theorem is analogous to the one presented for sequences in $\mathbb{R}^{n}$ where $a_{n}b_{n} \underset{n \to \infty}{\to}0$ if $a_{n} \underset{n\to\infty}{\to}0$ and $b_{n}$ is bounded (i.e. $\exists M \gt 0 \in \mathbb{R}$  $\exists N(M) \in \mathbb{N}, \forall n \in \mathbb{N}, n \gt N(M) \implies ||b_{n}|| \leq M$).

**Proof**:  Let $\epsilon \gt 0$. Observe that $\mathbb{R}$ is a normed vector space and $g(x)$ is bounded in a deleted neighbourhood of $p$ so $\exists M \in \mathbb{R}$ such that $g(x) \leq M$ for all $x \in B_{\delta_{g}}(p) \setminus \{ p\}$ for some $\delta_{g} \gt 0$, and similarly $f(x)$ converges to $0$ so$\forall \epsilon_{f} \gt 0$ $\exists \delta_{f} \gt 0$ such that $\forall x \in B_{\delta_{f}} \setminus \{p\}$ , $||f(x)|| \lt \epsilon_{f}$. Take $\epsilon_{g}=\frac{\epsilon}{M}$ and $\delta = \min\{\delta_{f}, \delta_{g}\}$, then the following is true

$$
d(f(x)g(x), 0) = ||f(x)g(x)|| = ||f(x)|| ||g(x)|| \leq ||f(x)|| M \lt \epsilon_{g}M = \epsilon
$$

$\square$

**Theorem (squeeze theorem for functions)**: Let $X$ be a normed vector spaces and let $f: X \to \mathbb{R}, g: X \to \mathbb{R}, h: X \to \mathbb{R}$ be scalar functions and let $x_{0} \in X$ be some point, then if $f(x)$ and $h(x)$ converge to the same limit $L \in \mathbb{R}$ as $x \to x_{0}$, and if

$$
\exists \delta \gt 0\in \mathbb{R}, (\forall x \in X, x \in B_{\delta}(x_{0}) \setminus \{x_{0}\}, g(x) \leq f(x) \leq h(x))
$$
then $f(x) \underset{x \to x_{0}}\to L$.

The proof is analogous to the proof for sequences of real numbers.

# Continuity

**Definition (continuity at a point)**: Let $M, N$ be metric spaces and $f: M \to N$ be a function, we say that $f$ is **continuous at a point** $p \in M$ if $f(x)\underset{x \to p}{\to}f(p)$, i.e. the limit of $f(x)$ at $p$ is $f(p)$.

**Equivalent definition (continuity at a point)**: Let $f: M \to N$ be a function between two metric spaces, then it is continuous at some point $p \in M$ if $\forall x_{n} \in X_{n} \subseteq M$ where $X_{n}$ is the class of all convergent sequences in $M$ such that $x_{n} \underset{n \to \infty}{\to}p$, it follows that $f(x_{n}) \underset{n \to \infty}{\to} f(p)$.

The equivalence follows from the theorem showing the equivalence of the limit of a function at a point and the limit of the image of sequences that converge to the same point under a function.

**Definition (Sequential continuity)**: Let $f: M \to N$ be a function between two metric spaces, then $f$ is **sequentially continuous** if $\forall x \in M$ and $\forall x_{n} \in X_{n} \subseteq M$ where $X_{n}$ is the class of all convergent sequences in $M$ whose limit is $x$, the limit of $f(x_{n})$ as $n \to \infty$ is $f(x)$:

$$
\forall x \in A\forall x_{n} \in A, \lim_{n \to \infty} x_{n} = x \implies \lim_{n \to \infty} f(x_{n}) = f(x)
$$

More concisely, $f$ is sequentially continuous if it is continuous at all points in its domain $M$.

**Corollary**: From the definition it follows immediately that the limit of a continuous function at any point is the image of that point.

**Theorem**: Continuity preserves limits, i.e. if $f: M \to N$ is sequentially continuous, then

$$
\lim_{n \to \infty} f(x_{n}) = f(\lim_{n\to \infty}x_{n}) 
$$
**Proof**: Follows immediately from the definition of sequential continuity.

**Corollary**: If $f$ is continuous at a point $x \in M$, then limits are also preserved by the continuity at that point.

**Theorem**: Let $A, B, C$ be metric spaces and let $f: A \to B$ and $g: B \to C$ be functions such that $g$ is continuous at a point $p \in B$, and let the limit of $f$ at some point $x_{0}$ be $p$ and let the limit of $g$ at $p$ be $L \in B$, then the limit of the composition $g \circ f = g(f(x))$ at $x \to x_{0}$ is also $L$:

$$
\lim_{x \to x_{0}} g(f(x)) = g(\lim_{x \to x_{0}}f(x)) = g(p) = L
$$
**Proof**: Since $g$ is continuous at $p$ the limits are preserved and the transition from the first step to the second step is allowed.

**Theorem**: The composition of sequentially continuous functions is sequentially continuous.

**Proof**: Let $f: M \to N$ and $g: N \to K$ both be sequentially continuous and consider the composition $g \circ f = g(f(x))$, and let $x_{0} \in M$ then since continuity preserves limits we have

$$
\lim_{x \to x_{0}} g(f(x)) = g(\lim_{x \to x_{0}} f(x)) = g(f(\lim_{x \to x_{0}} x)) = g(f(x_{0}))
$$
Which means that $\forall x_{0} \in M$, $f \circ g$ is continuous at $x_{0}$, so $f \circ g$ is sequentially continuous.

**Corollary**: Consider sequentially continuous functions $f: \mathbb{R} \to \mathbb{R}^{n}$ and $g: \mathbb{R}^{n} \to \mathbb{R}^{m}$, then the composition $g \circ f = g(f(x))$ is sequentially continuous as well. This is a special case of the theorem, but it is useful in multi-variable real analysis.

**Theorem**: Let $M$ be a normed vector space and let $N$ be a metric space, then if $f: N \to M$ and $g: N \to M$ are sequentially continuous, then $f \pm g$ is sequentially continuous.

**Proof**: Follows from limit arithemetics theorems for the limit of a function and from the definition of sequential continuity.

So far we have used the definition of sequential continuity and of continuity at a point as defined for metric spaces. Let us introduce the notion of continuity in a topological space, and then show that continuity is equivalent to sequential continuity in metric spaces. Once we show that result, we will begin using continuity and sequential continuity interchangeably, as we do with compactness and sequential compactness.

**Definition (continuity)**: Let $M, N$ be metric spaces and let $f: M \to N$ be a function, then $f$ is **continuous** if $\forall B \subseteq \text{Im}(f)$ such that $B$ is open, then the pre-image of $B$, $f^{-1}(B)$ is also open.

**Theorem**: Let $M, N$ be metric spaces with a function $f: M \to N$. The following are equivalent:
1. $f$ is continuous.
2. $f$ is sequentially continuous.

**Proof**: $1 \implies 2: \quad$ Let $f$ be continuous. If $\text{Im}(f) = \emptyset$ then $f$ is constant so it is sequentially continuous by a previous result, and the same can be said if the image consists of a single point. Otherwise, let $x \in M$ and let $\epsilon \gt 0$, and consider the open ball $B_{\epsilon}(f(x)) \subseteq \text{Im}(f)$. Since it is open and $f$ is continuous then $f^{-1}(B_{\epsilon}(f(x)))$ is open and since it is open and $x \in f^{-1}(B_{\epsilon}(f(x)))$ we can find some open ball inside it, so $\exists \delta \gt 0$ such that $B_{\delta}(x) \subseteq f^{-1}(B_{\epsilon}(f(x)))$, so the image of $f$ restricted to $B_{\delta}(x)$ is a subset of $B_{\epsilon}f(x))$, so we that $\forall x \in M, f$ is continuous at $x$ by definition of continuity at a point, so by definition it is also sequentially continuous on $M$.

$2 \implies 1: \quad$ Let $f$ be sequentially continuous and consider an open set $K \subseteq \text{Im}(f)$. Take some $x \in M, f(x) \in K$, since $f$ is sequentially continuous then it is continuous at $x$ so $\forall \epsilon \gt 0 \exists \delta \gt 0$ such that $\forall y \in M, y \in B_{\delta}(x) \setminus x \implies f(y) \in B_{\epsilon}(f(x))$,  which means that $x \in B_{\delta}(x)$ which is an open set such that $B_{\delta}(x) \subseteq f^{-1}(K)$. By repeating this argument $\forall x \in M$ that satisfies $f(x) \in K$, we get $f^{-1}(K)$ as the union of open balls, and since $M$ is a topological space then the union of open balls is an open set, so $f^{-1}(K)$ is open $\forall K \subseteq\text{Im}(f)$ such that $K$ is open, so $f$ is continuous. 

**Example 1**: Let $(X, || \cdot ||)$ be a normed vector space, and let $f: X \to \mathbb{R}$ be the function $f(x) = ||x||$. Show that $f(x)$ is continuous.

**Solution**: We will show that $f(x)$ is sequentially continuous, and since $X$ is a normed vector space it is also a metric space so this property is equivalent to $f(x)$ being continuous. Let $x_{n}$ be a sequence in $X$ that converges to $x$, then

$$
f(x_{n}) = ||x_{n}|| = ||x_{n} - x + x|| \leq ||x_{n} - x|| + ||x|| = (*)
$$

Since $x_{n} \to x$ as $n \to \infty$, then $x_{n} - x \to 0$ as $n \to \infty$ by limit arithmetics, so

$$
\lim_{n \to \infty}(||x_{n}-x|| + ||x||) = ||x||
$$
Now, consider $f(x)$. By a similar argument we get

$$
f(x) = ||x|| = ||x - x_{n} + x_{n}|| \leq ||x - x_{n}|| + f(x_{n}) = (**)
$$
And similarly we get $\lim_{n \to \infty}(**) = \lim_{n \to \infty}f(x_{n})$, so

$$
f(x) \leq \lim_{n \to \infty}f(x_{n}) \land \lim_{n \to \infty}f(x_{n}) \leq f(x)
$$

Which means that $f(x_{n}) \underset{n \to \infty}{\to}f(x)$, so $f(x_{n})$ is sequentially continuous, which completes the proof.

**Theorem (continuity preserves compactness)**: Let $M$ be a compact set and let $f: M \to N$ be continuous, then $\text{Im}(f)$ is compact. In other words, continuity preserves compactness.

**Proof**: Let $M$ be compact and let $f: M \to N$ be continuous. Let $C$ be an open cover of $\text{Im}(f)$. Since $f$ is continuous the pre-image of every $c \in C$ is also open, so the pre-image of $C$ is also an open cover of $M$, but since $M$ is compact $\exists C' \subseteq f^{-1}(C)$ such that $C'$ is a finite sub-cover of $f^{-1}(C)$, but since $C'$ is also a cover of $M$ then pushing $C'$ through $f$ results in $\text{Im}(f)$ as well, so the union of all  elements in $\text{Im}_{f}(C')$ is a finite cover of $\text{Im}(f)$, but it is also a finite subset of $C$, so $C$ has a finite sub-cover, so $\text{Im}(f)$ is compact.

# Connectedness

Consider the real line $\mathbb{R}$. In our intuitive notion of connectedness, it "feels" connected, and if we were to consider $\mathbb{R} \setminus x$ for some $x \in \mathbb{R}$ then it becomes disconnected. When attempting to formalize this notion, one can intuitively say that $\mathbb{R}$ is connected because $\forall x, y \in \mathbb{R}$, the set of all numbers $tx + (1-t)y$ for $t \in [0, 1]$ is also a subset of $\mathbb{R}$, but this definition quickly runs into multiple issues: first, if we were to generalize this notion to non-Euclidean space we could not do so without presenting another definition and showing that they are equivalent in $\mathbb{R}$. Secondly, what about some suspace $A \subseteq \mathbb{R}$, where not necessarily all points $tx + (1-t)y$ are in $A$ for all $t \in [0, 1]$? We would need to revisit our definition for such cases as well. Indeed, our definition is aligned with our intuitive notion of connectedness only when considering $\mathbb{R}$. Instead, we use topological properties to define connectedness, and we later show that they align with our intuition in Euclidean space.

**Definition (connected set)**: Let $X$ be a topological space and let $A \subseteq X$. $A$ is **connected** if and only if it cannot be written as a union of two disjoint (i.e. their intersection is empty) non-empty open subsets of $A$. Otherwise, $A$ is **disconnected**.

**Theorem**: Let $X$ be a metric space and let $A$ be a set in $X$. The following are equivalent:
1. $A$ is connected.
2. The only clopen subsets of $A$ are $A$ itself and $\emptyset$.

**Proof**: $1 \implies 2$: We know that $A, \emptyset$ are clopen by a previous result. Suppose $\exists B \subset A$ such that $B \notin \{\emptyset, A \}$ and $B$ is clopen, and consider $B^{c} = A \setminus B$, since $B$ is closed then $B^{c}$ is open and also since $B \subset A$ then $B^{c} \neq \emptyset$. Consider $B^{c} \cup B = (A \setminus B) \cup B = A$, it is a union of two disjoint non-empty open sets, but then $A$ is not connected, which is a contradiction.

$2 \implies 1$: Suppose $X = M \cup N$ for some $M, N \subseteq X$ both of which are non-empty disjoint open sets in $X$. Note that $M = A$ necessitates $N = \emptyset$ and vice versa, so w.l.o.g. suppose $M \neq A$, then since $M, N$ are distinct and their union is $X$ we have $N = M^{c} = A \setminus M$. Now, since $(M^{c})^{c}=M$ is open, then $N = M^{c}$ is closed, but $N$ is also open by assumption so we have $N$ clopen and also $M \notin \{A, \emptyset \}$, which is a contradiction. 

**Theorem**: A union of non-dijsoint compact subsets in some metric space $M$ is compact.

**Proof**: Let $A$ be a union of non-disjoint compact sets $U_{\alpha} \subseteq M$ (i.e. $\forall U_{\alpha}, U_{\beta}, U_{\alpha} \cap U_{\beta} \neq \emptyset$). Let $B, C \subseteq M$ be two disjoint open subsets such that $A = B \cup C$ and consider $K = (U{\alpha} \cap B) \cup (U_{\alpha} \cap C)$. Each intersection $U\alpha \cap B, U_{\alpha} \cap C$ is the intersection of a finite number of open sets in $M$ so each set is also open in $M$ and each set is also contained in $U_{\alpha}$ so these are open subsets of $U_{\alpha}$, so the union of the two intersections is a union of two open subsets of $U_{\alpha}$, furthermore since $B, C$ are taken such that $B \cap C = \emptyset$ then $K$ is also a union of two disjoint open subsets of $U_{\alpha}$. Finally, since $C=B^{c}= A \setminus B$ and $U_{\alpha} \subseteq A$ we have $U_{\alpha} \cap C = U_{\alpha} \setminus (U_{\alpha} \cap B)$, so $K$ is in fact $U_{\alpha}$. Now, since $U_{\alpha}$ is connected, $U_{\alpha}$ must be equal to one of the two terms in the union, w.l.o.g suppose $U_{\alpha} = U_{\alpha} \cap B$, then we have $U_{\alpha} \subseteq B$. Now pick any other $U_{\alpha}$ and denote it $U_{\beta}$, a similar argument applies. Now we have that either $U_{\alpha}, U_{\beta} \subseteq B$, or $U_{\alpha} \subseteq B \land U_{\beta} \subseteq C$, but since $U_{\alpha} \cap U_{\beta} \neq \emptyset$ then only the former is possible, so we have that all such compact sets $U_{\alpha}$ which consist of $A$ are in $B$, so $B=A$ and $C = \emptyset$, so by the previous theorem $A$ is compact.

**Theorem (continuity preserves connectedness)**: Let $M, N$ be metric sub-spaces and let $f: M \to N$ be a continuous function. If $M$ is connected, then $\text{Im}_{f}(M)$ is connected.

**Proof**: Consider the contrapositive form of the theorem: If $f: M \to N$ is continuous and $\text{Im}_{f}(M)$ is disconnected, then $M$ is disconnected. We prove this form instead. Let $U = \text{Im}_{f}(M)$, then since $U$ is disconnected then $\exists A, B \subseteq U, A \cap B = \emptyset, A, B \neq \emptyset$ and both open, such that $U = A \cup B$. Now consider the pre-images $f^{-1}(A), f^{-1}(B)$, since $A \cup B = U$ then $f^{-1}(A) \cup f^{-1}(B) = M$, and since $f$ is continuous then both are open since they are pre-images of open sets, and also since they are pre-images of non-empty sets they are also non-empty. Finally, we show that they are disjoint: suppose not, then $\exists x \in f^{-1}(A) \cap f^{-1}(B)$, so the image of $x$ under $f$ must be in both $A$ and $B$, but that means $\exists f(x) \in A \cap B$ so $A \cap B \neq \emptyset$, which is a contradiction.

## Connected Sets in Euclidean Space

**Definition (interval)**: An **interval** $I$ in $(\mathbb{R}, d_{\text{euclid}})$ is a set $I \subseteq \mathbb{R}$ such that $\forall a, b \in I$, all points between $a$ and $b$ are also in $I$, where a point "between $a$ and $b$" is a point $x \in \mathbb{R}$ that satisfies $a \lt x \lt b$.

**Theorem**: The following are the only forms of intervals in $\mathbb{R}$:
1. **closed interval**: $\exists a, b \in \mathbb{R}$ such that $I$ is the set of all points $x \in \mathbb{R}$ that satisfy $a \leq x \leq b$. We denote this interval as $I=[a, b]$.
2. **open interval**: A compact interval $I$ for some $a, b \in \mathbb{R}$ but with $a, b$ removed, denoted $I = (a, b)$.
3. **half-close half-open interval**: A close interval $I_{a, b}$ with only one of $a, b$ removed, denoted $I = [a, b)$ (with $b$ removed) or $I = (a, b]$ (with $a$ removed).
4. **Infinite interval**: An interval such that either the upper bound ($b$) or the lower bound ($a$) is removed, or both, the removed bound is replaced with $\pm \infty$ (minus sign if it replaces the lower bound and plus sign if it replaces the upper bound), for example $I=(-\infty, b]$ is an infinite interval. If both are removed then the interval is $(-\infty, \infty)$ which is just the real line.

**Proof**: Let $I$ be an interval, so $\forall a, b \in I$ and $\forall x \in \mathbb{R}, a \lt x \lt b \implies x \in I$. Now let $a' = \inf I$ and $b' = \sup I$ and suppose both are attained. Since all $x$ of the form $a \lt x \lt b$ are in $I$, then $a' \leq a$ and $b \leq b'$ for all $a, b \in I$, and since $a'$ is the infimum then all $x \in \mathbb{R}$ of the form $a' \lt x$ must be in $I$ otherwise $I$ has a greater lower bound, contradicting that $a'$ is the infimum, and similarly for $b'$. Similarly, $I$ cannot contain any $x$ such that $x \lt a'$ or $x \gt b'$ because otherwise that $x$ is a better lower/upper bound, so all points in $x \in I$ are of the form $a' \leq x \leq b'$, and since $I$ is an interval then $I$ must include all points of the form $a' \lt x \lt b'$, and may also contain $a'$ or $b'$, so if both $\inf I, \sup I$ are attained then $I$ must have one of the first three forms described in the theorem statement. Now, suppose $\inf I$ is not attained, then $I$ has no lower bound so $\forall x \in I$ we must always find some $y \lt x$ also in $I$ and since $I$ is an interval it must consist of all points between $y$ and $x$, similarly if $\sup I$ is not attained then $I$ has no upper bound so it must include all points to the right of the lower bound (if such exists), and if both are not attained then $I$ must be $\mathbb{R}$. All these cases are captured by the infinite interval form.

**Corollary**: One can further show that all finite intervals are bounded (in the metric sense) and that a closed interval is compact (since it is closed and bounded in $\mathbb{R}$ so by Heine-Borel it is compact), and that an infinite interval is open (in $\mathbb{R}$).

**Theorem**: A subset $I \subseteq \mathbb{R}$ is connected in $\mathbb{R}$ $\iff$ $I$ is an interval.

**Proof**: $\implies$ Suppose $I$ is connected and has at least two distinct points $a, b \in I, a \neq b$, w.l.o.g suppose $b \gt a$. Suppose $I$ is not an interval, then $\exists x \in \mathbb{R}$ such that $a \lt x \lt b$ and $x \notin I$, then consider the intervals $A = (-\infty, x), B = (x, \infty)$. Both are open and $A \cap B = \emptyset$ so they are disjoint, and both are non-empty since $a \in A, b \in B$. Now consider $A \cap I, B \cap I$, both are again disjoint and both are open subsets of $I$ since they are the intersection of finitely many open subsets of $I$, furthermore it $(A \cap I) \cup (B \cap I) = I$, but then we have two non-empty disjoint open subsets of $I$ such that $I$ can be written as their union, so $I$ is disconnected, which is a contradiction, so $I$ must be an interval.

$\impliedby$ Suppose $I$ is an interval and suppose $I$ is disconnected, so $\exists A, B \subseteq I$ both disjoint, non-empty and open such that $I=A \cup B$, so $\exists a, b \in I$ such that $a \in A \setminus B, b \in B \setminus A$, which also means $a \neq b$. Since the space is $\mathbb{R}$ it is equipped with the usual total order so since $a \neq b$ then w.l.o.g suppose $b \gt a$. Now since $A, B$ are open sets then $\exists \epsilon_{1} \gt 0$ such that $B_{\epsilon_{1}}(a) \subseteq A$ and $\exists \epsilon_{2} \gt 0$ such that $B_{\epsilon_{2}}(b) \subseteq B$,  consider maximal $\epsilon_{1}, \epsilon_{2}$ such that the $\epsilon$-balls are in $A, B$ respectively. Now consider $b - \epsilon_{2}$ and $a + \epsilon_{1}$. If $b - \epsilon_{2}$ is not strictly larger than $a + \epsilon_{1}$, then $A$ and $B$ overlap, which is not allowed by assumption, so $b - \epsilon_{2} \lt a + \epsilon_{1}$. Let $\epsilon = \frac{1}{2}d(b-\epsilon_{2}, a+ \epsilon_{1})$, and take $x = a + \epsilon_{1} + \epsilon$. Clearly $a \lt x$ and also $x \lt b$, so since $a, b$ are points on $I$ and $I$ is an interval then $x \in I$, however $x \notin A \cup B$, so $A \cup B \neq I$, which is a contradiction.

**Theorem (Intermediate Value Theorem)**: Let $M$ be a connected metric space, $f: M \to \mathbb{R}$ be continuous and  $a, b \in M$, then $\forall y \in [f(a), f(b)]$, $\exists x \in M$ such that $f(x) = y$.

**Proof**: If $f(a)=f(b)$ then the statement is vacuously true. Suppose w.l.o.g $f(b) \gt f(a)$.Since $M$ is connected and $f$ is continuous then $f(M)$ is connected by a previous result, and since $f(M) \subseteq \mathbb{R}$ then $f(M)$ is an interval by a previous result, so $\forall y \in \mathbb{R}$ such that $f(a) \lt y \lt f(b)$, $y \in f(M)$ (by the interval property), so $\exists x \in M$ such that $x = f^{-1}(y)$ so $f(x) = y$.

**Alternative Proof**: In Calculus 1/Real Analysis, one usually sees a proof of the Intermediate Value Theorem (IVT) for a continuous $f: I \to \mathbb{R}$. such that $I \subseteq \mathbb{R}$ Suppose instead $f: I^{k} \to \mathbb{R}$ such that $I^{k} \subseteq \mathbb{R}^{k}$ is continuous and take some parametrized curve (we will present a proper definition soon) $\varphi: [0, 1] \to \mathbb{R}^{k}$ such that $\phi$ is continuous and $\text{Im}(\varphi) = \text{domain}(f)$, and consider the composition $\varphi \circ f$, clearly $\text{Im}(\varphi \circ f) = \text{Im}(f)$ and also it is continuous since the composition of continuous functions is continuous, we now have $\varphi \circ f: [0, 1] \to \mathbb{R}$ so we can apply the IVT for every two values in $\text{Im}(\varphi \circ f)$. Now, consider $f(\vec{a}), f(\vec{b})$ for some $\vec{a}, \vec{b} \in I^{k} \subseteq \mathbb{R}^{k}$. Since $\text{Im}(\varphi) = \text{domain}(f)=I^{k}$, there exists $a, b \in [0, 1]$ such that $\varphi(a)=\vec{a}, \varphi(b)=\vec{b}$, so we can apply the IVT for $\varphi \circ f$ in $[a, b]$ and get the pre-image of any value $y \in [f(\vec{a}), f(\vec{b})]$ and get some $x \in [0, 1]$, then we can take $\varphi(x)$ to get some $\vec{x} \in I^{k}$, which completes the proof.

We can generalize the notion of an interval to higher dimensions as well. Recall that the interval property is that given an interval $I \subseteq X$ and a total order, then $\forall a, b \in I, \forall x \in X, (a \lt x \lt b \implies x \in I)$. The next definition generalizes this property to subsets of $\mathbb{R}^{n}$.

**Definition (convex set)**: Let $P \subseteq X$ where $X$ is some Banach space. We say that $P$ is a **convex set** if $\forall a, b \in P$, the line segment $\mathcal{l}_{ab}$ between $a$ and $b$ (i.e. all points of the form $ta+(1-t)b$ for all $t \in [0, 1]$) satisfies $l_{ab} \subseteq P$.

**Corollary**: An interval is a convex set in $\mathbb{R}$ by definition, since all points $x \in \mathbb{R}$ that satisfy $a \lt x \lt b$ for some $a, b$ are incident with the line segment whose ends are $a, b$.

All previous results, i.e. the possible classifications of intervals and the equivalence of a connected subset in $\mathbb{R}$ to an interval in $\mathbb{R}$, can be generalized to convex sets.

## Path Connected Space

In this section,we discuss two stronger notions for connected sets/spaces.

**Definition (path)**: Let $A$ be some topological space with some $a, b \in A$. A **path** from $a$ to $b$, i.e. $(a, b)$-path is a *continuous* function $f: [0, 1] \to  A$ such that $f(0)=a$ and $f(1)=b$ (or vice versa).

**Example**: In $\mathbb{R}^{k}$, a path between two points $a, b \in \mathbb{R}^{k}$ is given by the affine combination $f: [0, 1] \to \mathbb{R^{k}}$ defined as $f(t) = ta + (1-t)b$.

**Definition (loop)**: A loop is a path that starts and ends at the same point, i.e. $f(0) = f(1)$. Sometimes it is useful to think of a loop as a continuous function $f: S^{1} \to X$, where $S^{1}$ is the unit circle.

**Definition (path-component)**: Let $X$ be a topological space. A **path-component** of $X$ is an equivalence class on $X$ such that $x, y \in X$ are equal if there exists an $x, y$ path in $X$.

**Example**: Consider $X=(0, 3) \cup (5, \infty)$. $\forall x, y \in (0, 3)$ there exists a path, similarly $\forall x, y \in (5, \infty)$, but $\nexists x \in (0, 3), y \in (5, \infty)$ such that $\exists(x, y)$-path in $X$, so $(0, 3)$ and $(5, \infty)$ are the two path-components of $X$. 

**Definition (path-connected)**: Let $X$ be a topological space, then $X$ is **path-connected** if it has exactly one path-component.

**Example**: $\mathbb{R}^{k}$ is path-connected because $\forall x, y \in \mathbb{R}^{k}$ we have an affine combination which is a path. Also, any convex set in $\mathbb{R}^{k}$ is path connected by definition.

**Theorem**: If $X$ is path-connected, then $X$ is connected.

**Proof**: Let $X$ be path-connected.Suppose $X$ is not connected, then $\exists A, B \subseteq X$ such that $A \cap B = \emptyset$, $A$ and $B$ are open and not empty and $A \cup B = X$, so $\exists a \in A \setminus B, b \in B \setminus A$. Since $X$ is connected, there is a path $f: [0, 1] \to X$ such that $f(0)=a$ and $f(1)=b$. Since $f$ is a continuous and $[0, 1]$ is connected then $U = \text{Im}_{f}([0, 1])$ is also connected. Also note $U \subseteq X$, so $(A \cap U) \cup (B \cap B)$ is an open cover of $U$, but since $U$ is connected either $A \cap U = \emptyset$ or $B \cap U = \emptyset$, but $a, b \in U$ and also $a \in A, b \in B$, so neither can be empty, contradicting that $U$ is connected, so $X$ itself must also be connected, which completes the proof. 

**Definition (simply-connected)**: Let $X$ be a topological space, then $X$ is **simply-connected** if $X$ is path-connected and $\forall x \in X$ and $\forall f: S^{1} \to X$ such that $f$ is a loop around $x$, $\exists F: D^{2} \to X$ such that $F$ restricted to the unit circle $F_{S^{1}}$ is $f$.  

Informally, a simply-connected space is space $X$ in which any loop can be contracted (shrunken) down to a point while still remaining in $X$ throughout, i.e. $X$ cannot have any holes (otherwise we can take a loop around that hole and shrink it down until eventually the loop is exactly aligned with the boundary of the hole so it cannot be shrunk any further).

**Corollary**: simply-connected implies connected (since simply-connected implies path connected).

We state without proof the the Euclidean space $\mathbb{R}^{n}$ is simply-connected.

# Operators

**Definition (operator)**: Let $U, V$ be normed vector spaces. An **operator** is a mapping $f: U \to V$ which preserves some structure.

**Definition (linear operator)**: An operator $T: U \to V$ is called a **linear operator** (or transf ormation, or map) if it satisfies:

1. $\forall u, v \in U$, $f(u + v) = f(u) + f(v)$ (additivity)
2. $\forall u \in U \forall \lambda \in \mathbb{F}$ where $\mathbb{F}$ is a scalar filed, $f(\lambda u) = \lambda f(u)$ (homogeneity)

A linear operator preserves the algebraic structure of the set.

**Proposition**: If $T: U \to V$ is a linear operator then $T(0_{U}) = T(0_{V})$.

**Proof**: Since $U$ is a vector space then $\forall u \in U, u + 0_{U}=u$, since $T$ is a linear operator $T(u) =T(u+0_{U})=T(u)+T(0_{U})$ so $T(0_{U})=0_{V}$.

**Definition (continuous operator)**: An operator $T: U \to V$ is called a **continuous operator** if $T$ is continuous, i.e. it preserves the topological structure of the set.

**Definition (operator norm)**: Let $(U, ||\cdot||_{U}), (V, ||\cdot||_{V})$ be normed vector spaces and let $T: U \to V$ be a linear operator, then the **operator norm of $T$**, denoted $||T||$ or $||T||_{U \to V}$ if $T$ is ambiguous, is defined as

$$
||T|| = \sup \left\{ \frac{||T(x)||_{V}}{||x||_{U}} \Big\vert \forall x \in U, x \neq 0_{U}\right\}
$$
i.e. it is the supremum of the rate of change in the length of a vector from the domain under the transformation. For brevity, we usually omit the subscript of the norm $||\cdot||$  when the space the operand is from is unambiguous.  

**Definition (bounded operator)**: An linear operator $T: U \to V$ is **bounded** if $||T||$ is attained, i.e. $||T|| \lt \infty$.

**Proposition**: If $T: U \to V$ is a bounded linear operator,  then $\forall x \in U$, $||T(x)|| \leq ||T|| \cdot ||x||$ .

**Proof**: For $x \neq 0$ this follows immediately from  the definition of a bounded operator. For $x=0$, by a previous proposition we have $T(0)=0_{V}$ so we have $0 \leq ||T|| 0 = 0$ we is also true.

**Theorem**: Let $(U, ||\cdot||_{U}), (V, ||\cdot||_{V})$ be normed vector spaces and let $T: U \to V$ be a linear operator. The following are equivalent:
1. $T$ is continuous.
2. $T$ is continuous at some point $x_{0} \in U$.
3. $T$ is bounded.
​
**Proof**: $1 \implies 2:\quad$ Trivially true since $T$ is continuous $\forall x \in U$ and in particular for $x=0_{V}$.

$2 \implies 3:\quad$ Let $T$ be continuous at $x_{0}$. Suppose $T$ is not bounded, then $\exists x \in U$ such that $\forall L \in \mathbb{R}, ||T(x)||_{V} \gt L$. Rewrite $x$ as a unit vector $||x||\hat{x}$, now we have

$$
||T(x)|| = ||T(||x||\hat{x})||=||x|| \cdot ||T(\hat{x})||
$$
Since $T$ is unbounded then $\forall L \in \mathbb{R}, ||T(x)|| \gt ||x|| L$ otherwise we can take $||x||L$ as an upper bound on $||T(x)||$, so suppose w.l.o.g $x$ is a unit vector. Now, consider a sequence of the form $x_{n}=\frac{1}{n}x + x_{0}$, clearly as $n \to \infty, x_{n} \to x_{0}$ and since $T$ is continuous at $x_{0}$ we get that $T(x_{n}) \to T(x_{0})$, i.e. $\forall \epsilon \exists N \in \mathbb{N}$ such that $\forall n \gt N, ||T(x_{n})-T(x_{0})||_{V} \lt \epsilon$, now by linearity we have

$$
T(x_{n})-T(x_{0})=T(x_{n}-x_{0}) = T(\frac{1}{n}x + x_{0}-x_{0})=\frac{1}{n}T(x)
$$
So for some $N \in \mathbb{N}$ and $\forall n \gt N$, we have $||T(x_{n})-T(x_{0})||_{V}=\frac{1}{n}||T(x)||_{V} \lt \epsilon$, but then we have that for $L=\epsilon n$, $||T(x)||_{V} \lt L$, which is a contradiction, so $T$ must be bounded.

$3 \implies 1:\quad$ We prove instead that $T$ is bounded $\implies$ $T$ is sequentially continuous and by a previous result this implies that $T$ is continuous. Let $x \in U$ and let $x_{n}$ be a sequence in $U$ such that $x_{n} \underset{n \to \infty}{\to}x$. We have

$$
||T(x_{n})-T(x)||_{U}\underset{\text{linearity}}{=}||T(x_{n}-x)||_{U} \underset{\text{bounded}}{\leq}||T|| \cdot||x_{n}-x|| = (*)
$$
IF $||T|| = 0$ then we are done, otherwise let $\epsilon \gt 0$ and pick $\delta = \frac{\epsilon}{||T||}$, since $x_{n} \to x, \exists N(\delta) \in \mathbb{N}$ such that $\forall n \gt N(\delta), ||x_{n}-x|| \lt \delta$, so $(*) \leq ||T|| \delta = \epsilon$, so $T$ is sequentially continuous.

## Differentiation

### Frechet Derivative

**Definition 1 (Frechet derivative)**: Let $f: X \to Y$ be an operator between two normed vector spaces $X, Y$, then $f$ is **Frechet differentiable** at $x \in X$ if there is a bounded linear operator $T: X \to \mathcal{L}(X; Y)$ (i.e. $T$ maps points in $x$ to linear maps $X \to Y$) such that

$$
\lim_{||h|| \to 0}\frac{||f(x + h) - f(x) -T(h)||_{Y}}{||h||_{X}} = 0
$$
If $f$ is Frechet differentiable at $x$, then we say that $T$ is the **differential** or the **Frechet (total) derivative** of $f$ at $x$ and denote it by $T = df(x)$.

**Definition 2 (Frechet derivative)**: Let $f: X \to Y$ be an operator and let $X, Y$ be normed vector spaces and let $x \in X$, then $f$ is Frechet differentiable at $x$ if $\exists T: X \to \mathcal{L}(X; Y)$ a bounded linear operator such that the remainder term in the first order expansion of $f(x + h)$ (where $x$ is fixed and $h$ is variable), i.e. (in Little-O notation)

$$
f(x+h) = f(x) + T(h) + o(||h||)
$$
Satisfies $\frac{||o(||h||)||_{Y}}{||h||_{X}} \underset{||h|| \to 0}{\to}0$. Intuitively this means that the remainder term diminishes "faster" then $h$ as $h$ decreases.

**Theorem**: Both definitions are equivalent.

**Proof**: $1 \implies 2$: Suppose $f: X \to Y$ is Frechet differentiable at $x \in X$ by definition $1$, and consider $o(||h||): X \to Y$ defined as $o(||h||) = f(x+h) - f(x) - T(h)$, observe that

$$
\frac{||o(||h||)||_{Y}}{||h||_{X}} = \frac{||f(x+h)-f(x)-T(h)||_{Y}}{||h||_{X}} \underset{||h|| \to 0}{\to}0
$$
So $f$ is also Frechet differentiable at $x$ by definition $2$.

$2 \implies 1$: Suppose $f: X \to Y$ is Frechet differentiable at $x \in X$ by definition $2$, and let $\epsilon \gt 0$. Consider the function $g: \mathbb{X} \to \mathbb{Y}$ defined as

$$
g(h) = \begin{cases} \frac{||o(||h||)||_{Y}}{||h||_{X}} & ||h|| \neq 0 \\ 0 & h = 0\end{cases}
$$
With $o(||h||)=f(x+h)-f(x)-T(h)$. It is triviail to show that the function is continuous at $h=0$, but $\lim_{||h|| \to 0} g(h)$ is simply the Frechet derivative at $x$ by definition $1$, so $f$ is also differentiable at $x$ by that definition.

**Theorem (differentiability implies continuity)**: If $f: X \to Y$ is Frechet differentiable at $x \in X$, then $f$ is continuous at $x$.

**Proof**: Since $f$ is differentiable at $x$ then $T$ exists and is linear and bounded so by a previous result it is continuous, so $T(h) \underset{h \to 0}{\to}T(0)=0$. Next, consider the function $g: X \to Y$ defined as
$$
g(h) = \begin{cases} \frac{||f(x+h)-f(x)+T(h)||}{||h|} & h \neq 0 \\ 0 & h = 0 \end{cases}
$$
Since $f$ is differentiable at $x$ then $g(h)$ is continuous at $0$ so we can write

$$
\lim_{||h|| \to 0}  (f(x+h)-f(x)+T(h)) = 0 \Rightarrow \lim_{||h|| \to 0} f(x+h)= \lim_{||h|| \to 0} (f(x) -\underbrace{T(h)}_{\to 0})= f(x)
$$
Which completes the proof.

**Corollary**: In the first-order expansion of $f$ around some point $x$, the remainder term $o(h)$ is continuous since it is a summation of continuous functions.

**Theorem**: If $f: X \to Y$ is Frechet differentiable at some point $x \in X$, then the linear operator $T$ is unique.

**Proof**: Suppose not, then $\exists T_{1}, T_{2}, T_{1} \neq T_{2}$ bounded linear operators $X \to Y$ such that $\forall i \in \{1,  2\}$,

$$
g(h) = \frac{||T_{1}(h)-T_{2}(h)||_{Y}}{||h||_{X}} = \frac{1}{||h||_{X|}}||f(x+h)-f(x) - T_{2}(h) - (f(x+h) - f(x) - T_{1}(h))||_{Y} = (*)
$$
Now by the triangle inequality we have
$$
\begin{gathered}
(*) \leq \frac{1}{||h||_{X}}||f(x+h)-f(x)-T_{2}(h)||_{Y} + ||f(x+h)-f(x)-T_{1}(h)||_{Y} = \\
\frac{||f(x+h)-f(x)-T_{1}(h)||_{Y}}{||h||_{X}} - \frac{||f(x+h)-f(x)-T_{2}(h)||_{Y}}{||h||_{Y}} \underset{h \to 0}{\to}0
\end{gathered}
$$
Similarly,  since $g(h)$ is a quotient of norms which are non-negative, we have $0 \leq g(h)$, so by the squeeze theorem we have $g(h) \underset{h \to 0}{\to}0$. Next we define a function $k(h)=\begin{cases}g(h) & h \neq 0 \\ 0 & h = 0 \end{cases}$, which is continuous at $h=0$ , so we can write

$$
\lim_{h \to 0} ||T_{1}(h)-T_{2}(h)||_{Y} = 0 \Rightarrow T_{1}(h)=T_{2}(h)
$$
Which contradicts that $T_{1} \neq T_{2}$.

**Theorem (chain rule)**: Let $U, V, W$ be normed vector spaces and let $f: U \to V$ and $g: V \to W$ and consider their composition $\phi = g \circ f = g(f(x)), h: U \to W$. If $f$ is Frechet differentiable at some $x \in U$ and $g$ is Frechet differentiable at $f(x) \in V$ with Frechet derivatives $df(x), dg(f(x))$ respectively, then $\phi$ is also differentiable at $U$ and

$$
d\phi(x) = dg(f(x))df(x)
$$

**Proof**: Consider the first-order expansion of $f$ around $x$ (using the second definition for Frechet differentiability):

$$
f(x+h) = f(x)+df(x)(h)+o(h)
$$

Since $g, f$ are continuous then their composition is also continuous so

$$
g(f(x+h)) = g(f(x)+df(x)(h)+o(h)) = (*)
$$
Denote $f(x)=y$ and $df(x)(h)+o(h)=h'$. Now, consider the first-order expansion of $f$ around $y$:

$$
g(y+h') = (*) =  g(y) + \underbrace{dg(y)(h')}_{(***)} + o(h') = (**)
$$
Expand $(***)$, since $dg(y), df(x)$ are linear operators, we have

$$
(***) = dg(y)(df(x)(h)+o(h)) = dg(y)df(x)(h)+dg(y)(o(h))
$$
Observe that, since $dg(y)$ is a bounded linear operator, the following holds

$$
0 \underset{\text{norm properties}}{\leq} \frac{||dg(y))(o(h))||}{||h||} \leq \frac{||dg(y)|| \cdot ||o(h)||}{||h||} = ||dg(y)||\frac{||o(h)||}{||h||}
$$
Now, as $||h|| \to 0$, we have $\frac{||o(h)||}{||h||} \to 0$ since $f$ is differentiable at $x$, but we also have $||dg(y)||$ bounded, and the whole term is strictly non-negative, so by the squeeze theorem the whole term converges to $0$ as $||h|| \to 0$.

Substituting the result for $(***)$ in $(**)$, we get

$$
o(h')=g(y+h')-g(y)-(***) = g(y+h')-g(y) - dg(y)df(x)(h) + dg(y)(o(h))
$$
Now since $g$ is differentiable at $y$, we have $\frac{||o(h')||}{||h'||} \underset{||h'|| \to 0}{\to}0$, all that remains is to show that $||h|| \to 0 \implies ||h'|| \to 0$, and indeed we have

$$
||h'||=||df(x)(h)+o(h)|| \Rightarrow ||df(x)(0)+o(0)||=0
$$
So now we can rewrite the remainder as $$o'(h')=o(h')-dg(y)(o(h))$$ which clearly also goes to $0$ as $h' \to 0$, so $g(y)=g(f(x))$ is Frechet differentiable at $y=f(x)$ by the second definition, with the derivative being $dg(y)df(x)$.

**Theorem (derivative arithemetics)**: Let $X, Y$ be Banach spaces and let $f: X  \to Y$ and $g: X \to Y$ be Frechet differentiable at a point $x_{0} \in \text{interior}(X)$, and let $\alpha \in \mathbb{F}$ be a scalar from the field of scalars of $X, Y$, then the following results hold:
1. $d(\alpha f(x_{0}))=\alpha d(f(x_{0}))$
2. $d(f(x_{0})+g(x_{0}))=df(x_{0})+dg(x_{0})$

**Proof**: Both follow immediately from the fact that we define the derivative to be a (bounded) linear operator.

## Differentiation in Euclidean Space

In this section we show that the Frechet derivative is indeed a generalization of the derivative defined for functions $f: \mathbb{R} \to \mathbb{R}$. Recall that for such functions, we define the derivative as follows:

**Definition (derivative of a single-variable scalar function in $\mathbb{R}$)**: Let $f: \mathbb{R} \to \mathbb{R}$, and consider the following limit at some point $x \in \mathbb{R}$:

$$
\lim_{h \to 0}\frac{f(x+h)-f(x)}{h}
$$
If the limit exists, we say that $f$ is **differentiable** at $x$, and we define the **derivative** of $f$ to be the limit, and denote it $f'(x)$.

**Proposition**: Let $f: \mathbb{R} \to \mathbb{R}$ and take some $x \in \mathbb{R}$,.$f$ is differentiable at $x$ $\iff$ $f$ is Frechet differentiable at $x$, and $df(x)=f'(x)h$.

**Proof**: Notice that for $f: \mathbb{R} \to \mathbb{R}$ with the usual norm, the Frechet derivative is of the form

$$
\frac{||f(x+h)-f(x)-T(h)||}{||h||} = \left| \frac{f(x+h)-f(x)-T(h)}{h} \right| = \left | \frac{f(x+h)-f(x)}{h}-\frac{T(h)}{h} \right | = (*)
$$
Since $T$ is a bounded linear operator $\mathbb{R} \to \mathbb{R}$ that $\frac{1}{h}T(h)$ is simply $T(\frac{h}{h})=T(1)$, so

$$
(*) = \left| \frac{f(x+h)-f(x)}{h} - T(1)\right|
$$

$\implies$ Suppose $f$ is differentiable at $x$, then let $T(h)=f'(x)h$ and then we get $T(1)=f'(x)$, substituting in $(*)$ we get that $(*) \underset{h \to 0}{\to}0$, so $f$ is Frechet differentiable at $x$ with $df(x)(h)=f'(x)h$.

$\impliedby$ suppose that $f$ is Frechet differentiable at $x$, then we get

$$
\begin{gathered}
\lim_{h \to 0}(*) = 0 \underset{(**)}{\Rightarrow} \lim_{h \to 0}\left(\frac{f(x)+h-f(x)}{h}-T(1)\right) = f'(x)-T(1) = 0 \Rightarrow f'(x)=T(1)
\\
(**)\quad |f(x)| \to 0 \iff f(x) \to 0
\end{gathered}
$$
So we have $T(1)=f'(x)$, so by linearity $df(x)(h)=T(h)=f'(x)h$.

**Corollary**: If $f: \mathbb{R} \to \mathbb{R}$ is differentiable at $x \in \mathbb{R}$, then it is continuous at $x$ (since Frechet differentiable implies continuous and we have seen that the properties are equivalent for $f: \mathbb{R} \to \mathbb{R}$).

**Definition (derivative of a single-variable vector function in $\mathbb{R}^{n}$)**: Let $f: \mathbb{R} \to \mathbb{R}^{n}$ and let $x \in \mathbb{R}$, such that $f(x)=(f_{1}(x), f_{2}(x), \dots, f_{n}(x))$ with $f_{i}: \mathbb{R} \to \mathbb{R}$ (such function is often called a parameterized curve), then $f$ is **differentiable** at $x$ if all the scalar functions $f_{i}$ are differentiable at $x$, i.e. if all the following limits exist:

$$
\lim_{h \to 0}\frac{f_{i}(x+h)-f_{i}(x)}{h} = f_{i}'(x)
$$
We then say that the vector $f'(x) = (f_{1}'(x), f'_{2}(x), \dots, f'_{n}(x))$  is the **derivative** of $f$ at $x$.

**Theorem**: Let $x \in \mathbb{R}$ and $f: \mathbb{R} \to \mathbb{R^{n}}$. $f$ is differentiable at $x$ $\iff$ $f$ is Frechet differentiable at $x$, and $f'(x)h=df(x)(h)$.

**Proof**: The result from the previous theorem holds $\forall f_{i} \in f$, so trivially by taking a vector of all $f'_{i}(x)$ and multiplying it with $h$ we get $f'(x)h = df(x)(h)$.

Now we turn our attention to real multi-variable scalar functions, i.e. functions $f: \mathbb{R}^{m} \to \mathbb{R}$ (or any open subsets of those spaces).

Let $f: \mathbb{R}^{m} \to \mathbb{R}$ and let $B = \{e_{1}, e_{2}, \dots, e_{m} \}$ be a basis of $\mathbb{R}^{m}$ such that $\forall e_{i} \in B, ||e_{i}||=1$ (i.e. they are unit vectors) and let $x \in \mathbb{R}^{m}$ be expressed in terms of that basis, i.e. $\exists \lambda = \{\lambda_{1}, \lambda_{2}, \dots, \lambda_{m} \} \in \mathbb{F}^{m}$ where $\mathbb{F}$ is a scalar field such that $x$ is a linear combination of the basis vectors, i.e. $x=\sum_{i=1}^{m}\lambda_{i}e_{i}$, then we define the following:

**Definition (partial derivative)**: Take some $e_{i} \in B$ and some point $x \in \mathbb{R}^{m}$ and some scalar $h \in \mathbb{F}$. If the following limit:

$$
\lim_{h \to 0}\frac{f(x+he_{i})-f(x)}{h}
$$
is attained, then we denote it $\frac{\partial f}{\partial x_{i}}(x)$ or $f_{x_{i}}(x)$ and call it the **partial derivative** of $f$ at a point $x$ by the basis vector $e_{i}$ (which corresponds to the $i$-th component of $x$, $x_{i}$, up to scalar multiplication).

Essentially, the partial derivative fixes all components of $x$ aside from $x_{i}$, which corresponds to $e_{i}$, and then calculates the derivative of a single-variable scalar function $h: \mathbb{R} \to \mathbb{R}$. In that sense, it can be thought of as the derivative of $f$ with respect to the direction of the basis vector $e_{i}$, and indeed later on when we discuss directional derivatives we will define a partial derivative as a directional derivative restricted to the directions of the basis vectors.

For $f: \mathbb{R}^{m} \to \mathbb{R}$, there are $m$ such partial derivatives. Note that the choice of basis effects the partial derivatives but they are isomorphic (since there exists a permutation matrix between every two bases of a finite-dimension vector space).

**Exercise**: Calculate the partial derivatives of $f: \mathbb{R}^{2} \to \mathbb{R}$ defined as

$$
f(x,y) = \begin{cases}\frac{xy}{x^2+y^2} & (x, y) \neq (0, 0) \\ 0 & (x, y) = (0, 0) \end{cases}
$$
At the origin $(0, 0)$, and determine if $f$ is continuous at that point

**Solution**: Note that the choice of basis is implied by how the function is defined, i.e. we already have an expression for any point $v \in \mathbb{R}^{2}$ as $v=x\hat{x}+y\hat{y}$, so we use these variables as a basis. Now, going by definition we have

$$
\frac{\partial f}{\partial y}(0, 0) = \lim_{h \to 0}\left(\frac{f(0, 0 + h) - f(0, 0)}{h} = \frac{f(0, h)-0}{h}=\frac{0}{h}\right) = 0
$$
Similarly for $f_{x}(0, 0)$ we have

$$
\frac{\partial f}{\partial y}(0, 0) = \lim_{h \to 0}\frac{f(0 + h, 0) - f(0, 0)}{h}=0
$$
To check whether $f$ is continuous at that point or not, we check if it has a limit at the origin and if that limit is $f(0, 0)=0$. Consider the sequence $x=y$, then we have $f(x, x)=\frac{x^{2}}{2x^{2}}=\frac{1}{2}$, so if $f$ has a limit as $v \to (0, 0)$ it must be $\frac{1}{2}$ from the uniqueness of the limit, but $\frac{1}{2} \neq 0$, so $f$ is not continuous at the origin.

**Corollary**: The existence of all partial derivatives of $f$ at a point $x$ does not imply continuity at $f$.

**Definition (differentiability of a real multi-variable scalar function)**: Let $f: \mathbb{R}^{m} \to \mathbb{R}$ and let $x, h \in \mathbb{R}^{m}$. We say that $f$ is **differentiable** at $x$ if and only if there exists a bounded linear operator $T: \mathbb{R}^{m} \to \mathbb{R}$ such that the first-order expansion of $f$ around $x$,

$$
f(x+h) = f(x) + T(h) +  o(||h||)
$$
Satisfies that $\frac{o(||h||)}{||h||}\underset{||h|| \to 0}{\to} 0$.

**Theorem**: If $f: \mathbb{R}^{m} \to \mathbb{R}$ is differentiable at $x \in \mathbb{R}^{m}$ with some linear transformation $T: \mathbb{R}^{m} \to \mathbb{R}^{m}$, then all partial derivatives of $f$ at $x$ exist, and $\forall h \in \mathbb{R}^{m}$, $T(h)=\sum_{i=1}^{m}f_{x_{i}}(x)h_{i}$, where $f_{x_{i}}(x)$ is the partial derivative of $f$ at $x$ with respect to the $i$-th component, and $h_{i}$ is the $i$-th component of $h$.

**Proof**: Since $T$ is a linear transformation from $\mathbb{R}^{m}$ to $\mathbb{R}$ then clearly $T(h)=\sum_{i=1}^{m}\alpha_{i}h_{i}$ for some $\alpha = \{\alpha_{1}, \alpha_{2}, \dots, \alpha_{m}\} \in \mathbb{F}^{m}$, so it suffices to show that $\forall \alpha_{i} \in \alpha, \alpha_{i}=f_{x_{i}}(x)$. Consider one such $\alpha_{i}$, and take $h=\lambda e_{i}$ for some $\lambda \in \mathbb{F}$, so $h_{i}=||\lambda e_{i}||=\lambda$, so $T(h)=\alpha_{i} \lambda$. Now, consider the first-order expansion of $f$ at $x$ and plug $h$:

$$
f(x+h)=f(x)+T(h)+o(||h||) = f(x) +\alpha_{i}\lambda + o(||h||)
$$
Solving for $\alpha_{i}$, we get

$$
\alpha_{i} = \frac{f(x+h)-f(x)}{\lambda}-\frac{o(||h||)}{\lambda} = \frac{f(x+\lambda e_{i})-f(x)}\lambda{}-\frac{o(||h||)}{||h||}
$$
Now, since $f$ is differentiable at $x$, then $\frac{o(||h||)}{||h||} \underset{||h|| \to 0}{\to}0$, and taking the limit as $||h|| \to 0$ on the equation is well-defined since $T$ exists so $\alpha_{i}$ exists, so

$$
\underbrace{\lim_{||h|| \to 0} \alpha_{i}}_{\alpha_{i}} = \underbrace{\lim_{||h|| \to 0}}_{||h|| \to 0 \iff \lambda \to 0} \left(\frac{f(x+\lambda e_{i})-f(x)}{\lambda} - \underbrace{\frac{o(||h||)}{||h||}}_{\to 0}\right) = \lim_{\lambda \to 0} \frac{f(x+\lambda e_{i})-f(x)}{\lambda} = \frac{\partial f}{\partial x_{i}}(x)
$$
Which completes the proof.

**Exercise**: Determine if $f: \mathbb{R}^{2} \to \mathbb{R}$ defined as 

$$
f(x, y) = \begin{cases} \frac{x^{2}y}{(x^2+y^2)^{\frac{1}{2}}} & (x, y) \neq (0, 0) \\ 0 & \text{otherwise} \end{cases}
$$
is:
1. Differentiable at the origin.
2. Continuous at the origin.

**Solution**: Consider the first-order expansion of $f(x, y)$ at the origin:

$$
f(x +h, y+k)=f(y, k) = f(0, 0) + T(0)+o(||(h, k)||)
$$
Since $T$ is linear then $T(0)=0$ and we have $f(0, 0)=0$ so

$$
o(||(h, k)||)=f(y, k)
$$
So it remains to show that $\frac{f(y, k)}{||h||} \to 0$ as $||(h, k)|| \to 0$. Introduce a change of variable by using polar coordinates, i.e. the parametrization $\phi(r, \theta) = (r \cos \theta, r \sin \theta)$ and denote $k=r\sin \theta, h=r\cos \theta$, now we have

$$
f(r, \theta) = \frac{r^{2}\cos^{2}\theta r \sin \theta}{r}=r^{2} cos^{2}\theta \sin \theta
$$
Observe that $||h|| \to 0$ implies $r \to 0$, and $cos^{2}\theta \sin\theta$ is bounded, so by a previous result $f(r, \theta) \to 0$ as $r \to 0$, but $f(0, 0)=0$, so $f$ is continuous at $0$. To answer the question regarding differentiability, observe that

$$
\frac{f(r, \theta)}{||(h, k)||}=\frac{r^{2} \cos^{2}\theta \sin \theta}{r}=r \cos^{2}\theta \sin \theta
$$
And again we have an expression which can be written as a bounded scalar function multiplied by a scalar function which goes to $0$ ar $r \to 0$, so $f$ is also differentiable at the origin.

**Definition (gradient)**: Let $f: \mathbb{R}^{m} \to \mathbb{R}$ have all partial derivatives at some $x \in \mathbb{R}^{m}$. The **gradient** of $f$ at $x$ is an ordered tuple of all partial derivatives of $f$ at $x$ with respect to some basis, and it is denoted $\nabla f(x) = (f_{x_{1}}(x), f_{x_{2}}(x), \dots, f_{x_{m}}(x))$, where $\nabla f$ is an operator $\mathbb{R}^{m} \to \mathbb{R}^{m}$.

**Corollary**: If $f$ is differentiable at $x$ then we can use the previous result and rewrite the definition for differentiability as follows

$$
f(x+h)=f(x)+\nabla f(x) \cdot h + o(||h||), \quad \lim_{||h|| \to 0}\frac{o(||h||)}{||h||} = 0
$$

**Theorem**: If $f: \mathbb{R}^{m} \to \mathbb{R}$ has all partial derivatives at some point $x \in \mathbb{R}^{m}$ and the partial derivatives are continuous at $x$, then $f$ is differentiable at $x$.

**Proof**: Left as an exercise for the reader.

**Theorem**: Let $f: \mathbb{R}^{m} \to \mathbb{R}$ and let $x$ be some point in $\mathbb{R}^{m}$, the following are equivalent:
1. $f$ is Frechet differentiable at $x$.
2. $f$ is differentiable at $x$.

Additionally,  $df(x)(h)=\nabla f(x) \cdot h$ and in particular $df(x)(e_{i})=\frac{\partial f}{\partial x_{i}}(x)$.

**Proof**: First, recall the definitions of each property. We use the second definition for the Frechet derivative. $f$ is *Frechet differentiable* at $x$ if $\exists T: X \to Y$ linear bounded operator, such that when considering the remainder term $o(||h||)$ of the first-order expansion of $f$ around $x$:

$$
f(x+h)=f(x)+df(x)(h)+o(||h||)
$$
We have that $o(||h||)$ satisfies that $\frac{||o(||h||)||}{||h||} \underset{||h|| \to 0}{\to}0$.
$f$ is *differentiable* at $x$ is $\exists T: \mathbb{R}^{m} \to \mathbb{R}$ linear transformation such that, when considering the remainder term $o(||h||)$, it satisfies $\frac{o(|h||)}{||h||} \underset{||h|| \to 0}{\to 0}$.
Clearly for $X \to Y$, these definitions are equivalent with $df(x)=T$, but from a previous result we know that $T(h)=\nabla f(x) \cdot h$,  so $df(x)(h)=\nabla f(x) \cdot h$, and with the restriction $h=e_{i}$ we get $df(x)(e_{i})=\nabla f(x) \cdot e_{i}$, which is just the $i$-th component of $\nabla f(x)$, which is $\frac{\partial f}{\partial x_{i}}(x)$.

**Corollary**: By a previous result we know that Frechet differentiability implies continuity, so by the theorem if $f: \mathbb{R}^{m} \to \mathbb{R}$ is differentiable at $x$ by the definition provided earlier, then it is continuous at $x$.

**Definition (differentiability of a multi-variable vector function)**: Let $f: \mathbb{R}^{m} \to \mathbb{R}^{n}$ be defined as an $n$-tuple composed of function $f_{i}: \mathbb{R}^{m} \to \mathbb{R}$, i.e. $f = (f_{1}, f_{2}, \dots, f_{n})$, then $f$ is **differentiable** at $x \in \mathbb{R}^{m}$ if and only if $\forall f_{i} \in f$, $f_{i}$ is differentiable at $x$, which implies that all of $f_{i}$ have their gradient defined at $x$, i.e.

$$
\forall f_{i} \in i, f_{i}(x+h)=f_{i}(x)+\nabla f_{i}(x) \cdot h + o(||h||), \quad \lim_{||h|| \to 0}\frac{||o(||h||)||}{||h||}=0
$$
**Definition (Jacobian matrix)**: Let $f: \mathbb{R}^{m} \to \mathbb{R}^{n}$ be a differentiable at $x \in \mathbb{R}^{m}$. Consider the $\mathbb{R}^{m \times n}$ matrix $J(x)$ whose rows are $\nabla f_{i}^{T}(x)$ (where $\nabla^{T} f_{i} = (\nabla f_{i})^{T}$, i.e. the transpose of the gradient of $f_{i}$), i.e.

$$
J(x) = \begin{pmatrix}
\nabla^{T} f_{1}(x) \\ \nabla^{T} f_{2}(x) \\ \dots \\ \nabla^{T} f_{n}(x)
\end{pmatrix}
$$
Or, used $ij$ indexing,

$$
J(x)_{ij} = \frac{\partial f_{i}}{\partial e_{j}}(x)
$$

We call $J$ the **Jacobian matrix**, or simply the **Jacobian**, of $f$ at $x$.

**Theorem**: Let $f: \mathbb{R}^{m} \to \mathbb{R}^{n}$ and let $x \in \mathbb{R}^{m}$, the following are equivalent:
1. $f$ is differentiable at $x$.
2. $f$ is Frechet differentiable at $x$.

Additionally, we have $J(x)=df(x)$.

**Proof**: Observe that for each $f_{i}$, $f_{i}$ is differentiable at $x$ $\iff$ $f_{i}$ is Frechet differentiable at $x$, with $df_{i}(x)=\nabla f_{i}(x)$. Now, consider the first-order expansion of $f$ around $x$:

$$
f(x+h)=f(x)+df(x)(h)+o(||h||)
$$
Since this equation is an equivalence of vectors (since $f: \mathbb{R}^{m} \to \mathbb{R}^{m}$ and so is $o(||h||)$), then it holds if and only if it holds for each $f_{i}$, i.e.

$$
f(x+h)=f(x)+df(x)(h)+o(||h||) \iff \forall f_{i} \in f, f_{i}(x+h)=f_{i}(x)+df_{i}(x)(h)+o_{i}(||h||)
$$
But the latter implies that each $f_{i}$ must be both differentiable and Frechet differentiable at $x$. Now, if all $f_{i}$ are differentiable at $x$, then $f$ is differentiable at $x$ by definition, and similarly since all $o_{i}(||h||)$ satisfy that $\frac{||o_{i}(||h||)||}{||h||} \underset{||h|| \to 0}{\to}0$, then it must be that $o(||h||) \to 0$ as well (in fact, these two properties are equivalent by a prior result), so the raminder term $o(||h||)$ satisfies the condition for $f$ to be Frechet differentiable at $x$, so $f$ is also Frechet differentiable at $f$. Finally, since each $df_{i}(x)=\nabla f(x)$, we have that $df(x)=J(x)$.

## Differentiation Rules

In this section, we present a few practical examples that show how one may use the Frechet derivative to find the derivative of a function, and to prove differentiation rules which are harder to prove using the definitions used in real analyis.

**Example 1**: Consider a scalar function $f: A \subseteq \mathbb{R} \to \mathbb{R}$ defined as $f(x)=x^{2}$. We wish to find an expression for the derivative of $f$. We know by a prior result that $df(x)=f'(x)$ where $df(x)$ is the Frechet derivative, so if $f$ is differentiable at some point $x \in A$, then we can write $f$ at some point $x+h$ for $h \in A$ as

$$
f(x+h)=f(x)+df(x)(h)+o(h)
$$
By using the explicit form of $f$, we get

$$
f(x+h)=(x+h)^{2}=x^2+2xh+h^2=f(x)+(2x)(h)+h^2
$$
Observe that $\frac{h^{2}}{h}=h$ so $\lim_{h \to 0}\frac{h^{2}}{h}=0$ so $h^{2}=o(h)$, so we have expression $f(x+h)$ in terms of the Frechet derivative

$$
f(x+h)=f(x)+(2x)(h)+o(h)
$$
Which means that $df(x)=f'(x)=2x$, which is a known result from analysis.

**Example 2**: Let $f(x)=c$ with some constant $c$, then $f(x+h)=c=f(x)=f(x)+0+0$, clearly $0=o(h)$ and $0=df(x)(h)$, which implies $df(x)=0$.

**Example 3**: Let $f(x)=ax$ with some constant $a$, then $f(x+h)=a(x+h)=ax+ah=f(x)+(a)(h)+0$, so $df(x)=a$.

**Theorem**: Let $f: A \to B$ where $A, B$ are Banach space, let $x \in A$ and let $Q$ be a matrix representing a linear transformation $Q: A \to B$, then the derivative of the quadratic form of $Q$ given by $x^{T}Qx$ is given by $df(x)=(Q +Q^{T})x$.

**Proof**: Again we write $f(x+h)$ and attempt to arrive at the Frechet derivative form:

$$
\begin{aligned}
f(x+h)=\\
&(x+h)^{T}Q(x+h)=\\
&x^{T}Qx+x^{T}Qh+h^{T}Qx+h^{T}Qh=\\
&f(x)+x^{T}Qh+h^{T}Qx+o(h)=\\
&f(x)+x^{T}(Q+Q^{T})h+o(h)=\\
&&f(x)+\underbrace{(Q+Q^{T})x \cdot h}_{\text{dot product}} + o(h)
\end{aligned}
$$
So $df(x)=(Q+Q^{T})x$.

## Gateaux Derivative

**Definition (Gateaux derivative)**: Let $(X, ||\cdot||_{X}), (Y, ||\cdot||_{Y})$ be Banach spaces, and let $f: \mathbb{X} \to Y$ be an operator. Fix a point $x \in X$ and a direction $h \in X$. The **Gateaux derivative** of $f$ at $x$ in direction $h$ is defined as the limit

$$
D_{x}f(h) = \lim_{t \to 0}\frac{f(x+th)-f(x)}{t}
$$
Where $t$ is some scalar from $\mathbb{F}$ the scalar field of $X$. If the limit $D_{x}f(h)$ exists for all $h \in X$, then we say that $f$ is **Gateaux differentiable** at $x$.

**Theorem**: The Gateaux derivative is homogenous, i.e. $\forall \alpha \neq 0 \in \mathbb{F}$, $D_{x}f(\alpha h) = \alpha D_{x}f(h)$.

**Proof**: By definition, observe that

$$
D_{x}f(\alpha h) = \lim_{t \to 0}\frac{f(x+t \alpha h)-f(x)}{t} = \lim_{t \to 0}\alpha\frac{f(x+\alpha th)-f(t)}{\alpha x} = \alpha \lim_{t \to 0}\frac{f(x+ \alpha t h)-f(x)}{\alpha t}=(*)
$$
Now, observe that $t \to 0 \iff \alpha t \to 0$ so we can change the limit, and now we get

$$
(*) = \alpha \lim_{\alpha t \to 0}\frac{f(x+\alpha th)-f(x)}{\alpha t}=\alpha D_{x}f(h)
$$
Which completes the proof.

**Theorem**: If $f$ is Frechet differentiable at $x$, then it is also Gateaux differentiable at $x$, and the Frechet derivative and Gateaux derivative at $x$ agree.

**Proof**: Let $f: X \to Y$ be Frechet differentiable at $x \in X$ and pick a direction $h \in X$. For $f$ to be Gateaux differentiable at $f$ and for the derivatives to agree, their distance must be exactly 0, and since the Frechet derivative exists and is bounded, this is equivalent to claiming that

$$
||D_{x}f(h) - df(x)(h)|| = 0
$$
Let us prove that this is true. First, recall the definition for the Gateaux derivative at $x$ and replace it with $D_{x}f(h)$:

$$
\left|\left| \lim_{t \to 0} \frac{f(x+th)-f(x)}{t} - df(x)(h)\right|\right| = (*)
$$
Now, since $df(x)(h)$ is independent of $t$, we can add it to the limit, and we have

$$
(*) = \left| \left| \lim_{t \to 0} \left( \frac{f(x+th)-f(x)}{t} - df(x)(h) \right ) \right| \right| = (**)
$$
Since $df(x)$ is a linear operator, we have $df(x)(h)=\frac{1}{t}df(x)(th)$, and by multiplying the expression by $\frac{||h||_{X}}{||h||_{X}}$ we maintain the value and we ca move $||h||_{X}$ outside the limit since it is independent of $t$, and now we have an expression with $t||h||_{X}$ in the denominator, which is just $||th||_{X}$ since $t$ is a scalar, so we get

$$
(**) = ||h||_{X} \lim_{t \to 0}\frac{||d(x+th)-f(x)-df(x)(th)||_{Y}}{||th||_{X}}
$$
But since $f$ is Frechet differentiable at $x$ for all sequences that vanish to $0$, we can consider the sequence $||th|| \to 0$ which, for $h \neq 0_{X}$ is true if and only if $t \to 0$, so now we have a bounded real value $||h||_{X}$ multiplied by a function which vanishes to $0$ as $||th|| \to 0$, so overall we get $(**) \underset{t \to 0}{\to} 0$, which means that the Gateaux derivative exists at $f$, and is equivalent to the Frechet derivative at $f$.

### Directional Derivatives in Euclidean Spaces

**Definition (directional derivative)**: Let $f: \mathbb{R}^{m} \to \mathbb{R}$ and let $h \in \mathbb{R}^{m}$ be some direction vector and let $x \in \mathbb{R}^{m}$ be a point. The **directional derivative** of $f$ at $x$ in direction $h$ is given by

$$
\lim_{t \to 0}\frac{f(x+th)-f(x)}{t}
$$
and denoted $\frac{\partial f}{\partial h}(x)$ or $f_{h}(x)$.

**Corollary 1**: From the definition it is easy to see that given a basis $B=\{e_{1}, e_{2}, \dots, e_{m}\}$, the directional derivative with respect to some basis vector $e_{i}$ is in fact the partial derivative $f_{e_{i}}$.

**Corollary 2**: If $f$ is Gateaux differentiable at $x$ with Gateaux derivative $D_{x}f$, then $D_{x}f(h)$ is the directional derivative in direction $h$.

**Theorem**: Let $f: \mathbb{R}^{m} \to \mathbb{R}$ be differentiable at $x \in \mathbb{R}^{m}$ and let $h$ be some direction, then the directional derivative of $f$ at $x$ in direction $h$ is given by $f_{h}(x)=\nabla f(x) \cdot h$

**Proof**: Suppose $f: \mathbb{R}^{m} \to \mathbb{R}$ is differentiable at $x \in \mathbb{R}^{m}$, then by a previous result we know that $f$ is Frechet differentiable at $x$ with $df(x)=\nabla f(x)$, and we also know that $f$ is Gateaux differentiable at $x$ with $D_{x}f = df(x)$. Furthermore, we know that $D_{x}f(h)=f_{h}(x)$, so 

$$
f_{h}(x)=D_{x}f(h)=df(x)=\nabla f(x) \cdot h
$$
which completes the proof.

**Corollary**: Let $B=\{e_{1}, e_{2}, \dots, e_{m} \}$ be a basis for $\mathbb{R}^{m}$ and let $x \in \mathbb{R}^{m}$, then the set of partial derivatives of $\{f_{e_{1}}(x), f_{e_{2}}(x), \dots , f_{e_{m}}(x) \}$ forms a basis for $\text{Im}(D_{x}f)$. 

**Proof**: Since $B$ is a basis we can write every vector $h \in \mathbb{R}^{m}$ as a linear combination of basis vectors, i.e. $\forall h \in \mathbb{R}^{m}\exists \alpha = \{\alpha_{1}, \alpha_{2}, \dots, \alpha_{m} \} \in \mathbb{F}^{m}$ such that $h=\sum_{i=1}^{n}\alpha_{i}e_{i}$. Now since $df(x)$ is a linear operator, we have

$$
df(x)(h)=df(x)(\sum_{i=1}^{n}\alpha_{i}e_{i})=\sum_{i=1}^{n}\alpha_{i} df(x)(e_{i}) =\sum_{i=1}^{n} \alpha_{i} f_{x_{i}}
$$
So the set of partial derivatives of $f$ at $x$ with respect to some basis $B$ forms a basis for the vector space of its directional derivatives.

## Multilinear Maps

**Definition (bilinear map)**: Let $E_{1}, E_{2}$ and $F$ be vector spaces and let $f: E_{1} \times E_{2} \to F$ be a mapping $(e_{1}, e_{2}) \mapsto f(e_{1}, e_{2})$, then $f$ is a **bilinear map** if it is linear in each component(we will expand upon what this means when defining a multilinear map which generalizes this definition).

**Definition (algebra over a field)**: Let $V$ be a vector space over a scalar field $\mathbb{F}$ and let $f: V \times V \to V, (v_{1}, v_{2}) \mapsto f(v_{1}, v_{2})$ be a bilinear operator (map), then $(V, f)$ is called an **algebra** over $\mathbb{F}$. The operator $f$ is called a **product operator** and $f(v_{1}, v_{2})$ is called the **product** of $v_{1}$ and $v_{2}$ and is often denoted $v_{1}v_{2}$.

**Example**: The usual multiplication operation $\times: \mathbb{R} \times \mathbb{R} \to \mathbb{R}, (a, b) \mapsto ab$ is a bilinear operator, since $\forall a, b, c, \lambda \in \mathbb{R}$ we have

$$
\begin{align*}
&1. \text{ homogeneity}:\quad a(\lambda b)= \lambda(ab),\quad (\lambda a)b=\lambda(ab)\\
&2. \text{ additivity}:\quad a(b+c)=(ab)+(ac), \quad (a+c)b = (ab)+(cb)
\end{align*}
$$
So $\times$ is linear in both components, so $(\mathbb{R}, \times)$ is an algebra over $\mathbb{R}$.

**Corollary**: Now that we have properly defined an algebra over a field, we can generalize limit arithemetics theorems which dealt with the product in $\mathbb{R}$, by rephrasing these as theorems which apply for Banach algebras (i.e. Banach spaces equipped with a bilinear operator). As an example, consider the following generalization of the limit of a product:

**Theorem**: Let $(A, \times)$ be a Banach algebra, and let $a_{n}$ and $b_{n}$ be convergent sequences in $A$ which converge to $\tilde{a}, \tilde{b}$ respectively, then the sequence of products $c_{n}=a_{n}b_{n}$ converges to $\tilde{a}\tilde{b}$ (where $\tilde{a}\tilde{b}=\times(\tilde{a}, \tilde{b})$).

**Proof**: The proof is left as an exercise, but it is essentially a restatement of the proof provided for the case where $A=\mathbb{R}$ and $\times$ was intuitively taken to be the usual Euclidean multiplication.

**Definition (multilinear map)**: Let $E_{1}, E_{2}, \dots, E_{n}$ and $F$ be vector spaces and let $f: E_{1} \times E_{2} \times \dots \times E_{n} \to F$ be a mapping from the $n$-tuple whose $i$-th element is from $E_{i}$ to the vector space $F$, then $f$ is **mutlilinear**, by choosing any $i \in \mathbb{N}, i \in [1, n]$  and fixing the rest of the elements of the $n$-tuple, $f$ becomes a linear map on $E_{i}$, i.e.  $f(e_{1}, e_{2}, \dots, e_{n})=h(e_{i})$ such that $h$ is a linear map $E_{i} \to F$. Such linear map is called a **partial mapping** of $f$ to $e_{i}$ and denoted $f_{e_{i}}$}.

**Proposition 1**: Let $f$ be a multilinear map $f: E_{1} \times \dots \times E_{n} \to F, (e_{1}, \dots, e_{n}) \mapsto f$, then if $e_{i}=0$ for some $i$, $f(e_{1}, \dots, e_{n})=0$.

**Proof**: Let $e \in E_{1} \times \dots \times E_{n}$, then $f(e)=f((e_{1}, \dots, e_{n}))$. Pick some index $i \in [1, n]$ and let it vary, now since $f$ is multilinear we have $f(e)=h(e_{i})$ where $h$ is a linear map. Choose $e_{i}=0$, now $h(e_{i})=0$ from linearity, so $f(e)=0$, which completes the proof.

**Proposition 2**: Let $f$ be a multilinear map and let $\lambda_{1}, \lambda_{2}, \dots \lambda_{n}$ be scalars, then

$$
f(\lambda_{1}e_{1}, \lambda_{2}e_{2}, \dots, \lambda_{n}e_{n})=(\prod_{i=1}^{n}\lambda_{i})f(e_{1}, e_{2}, \dots, e_{n})
$$
**Proof**: Pick some $i$ and fix the rest, again we can write

$$
f(\lambda_{1}e_{1}, \dots, \lambda_{n}e_{n})=h(\lambda_{i}e_{i})=\lambda_{i}h(e_{i})=f(\lambda e_{1}, \dots, e_{i}, \dots, \lambda_{n}e_{n})
$$
This process can be repeated for all $i$, and when we are done we arrive at the proposition.

**Corollary**: Let $v \in E_{1} \times \dots \times E_{n}$, then each $v_{i} \in E_{i}$, so each $v_{i}$ can be written as $v_{i}=||v_{i}|| e_{i}$ where $e_{i}$ is a unit vector in the direction of $v_{i}$, so by proposition $(2)$ we have

$$
f(v_{1}, v_{2}, \dots)= \prod_{i=1}^{n}||v_{i}|| f(e_{1}, e_{2}, \dots)
$$
**Proposition 3**: Let $f$ be a multilinear map and let $a \in E^{n}=E_{1} \times \dots \times E_{n}, \space b_{i} \in E_{i}$, then

$$
f(a_{1}, \dots, a_{i}+b_{i}, \dots, a_{n})=f(a_{1}, \dots, a_{i}, \dots, a_{n})+f(a_{1}, \dots, b_{i}, \dots, a_{n})
$$
i.e. $f$ is additive in each component.

**Proof**: Consider the partial map of $f$ restricted to the $i$-th component, it is linear so we have

$$
\begin{gathered}
f(a_{1}, \dots, a_{i}+b_{i}, \dots, a_{n})= g(a_{i}+b_{i})=g(a_{i})+g(b_{i})=\\
f(a_{1}, \dots, a_{i}, \dots, a_{n})+f(a_{1}, \dots, b_{i}, \dots, a_{n})
\end{gathered}
$$
Which completes the proof.

**Exercise**: Let $a, b \in \mathbb{R}^{2}$ and let $f: \mathbb{R}^{2} \to Y$, show that:

$$
f(a_{1}, a_{2})-f(b_{1}, b_{2}) = f(a_{1}-b_{1}, a_{2})+f(b_{1}, a_{2}-b_{2})
$$
**Solution**:

$$
\begin{gathered}
f(a_{1}, a_{2})-f(b_{1}, b_{2})=\\
f(a_{1}-b_{1}+b_{1}, a_{2})-f(b_{1}, b_{2}-a_{2}+a_{2})\underset{\text{proposition }(3)}{=}\\
f(a_{1}-b_{1},a_{2}) + f(b_{1}, a_{2}) -( f(b_{1}, b_{2}-a_{2}) + f(b_{1}, a_{2})) \underset{\text{proposition }(2)}{=}\\
f(a_{1}-b_{1}, a_{2})+f(b_{1}, a_{2}) + f(b_{1}, a_{2}-b_{2})-f(b_{1}, a_{2}) = \\
f(a_{1}-b_{2}, a_{2}) + f(b_{1}, a_{2}-b_{2})
\end{gathered}
$$

**Theorem**: $\forall a, b \in E^{m}$ where $E^{m}$ is the product space $E_{1} \times \dots \times E_{m}$ and for $f: E^{m} \to F$ a multilinear map, the following is true:

$$
\begin{gathered}
f(a_{1}, a_{2}, \dots, a_{m}) - f(b_{1}, b_{2}, \dots, b_{m}) =\\
f(a_{1}-b_{1}, a_{2}, \dots) + f(b_{1}, a_{2}-b_{2}, a_{3}, \dots) + \dots + f(b_{1}, b_{2}, \dots, a_{m}-b_{m})
\end{gathered}
$$
Equivalently, define $x_{k}$ to be a finite sequence of $m$-tuples from $E^{m}$ such that $k$ ranges from $1$ to $m$, and the $i$-th element of $x_{k}$ is given by

$$
(x_{k})_{i}=\begin{cases}
b_{i} & i \lt k \\
a_{i}-b_{i} & i = k \\
a_{i} & i \gt k
\end{cases}
$$
Then

$$
f(a_{1}, \dots, a_{m})-f(b_{1}, \dots, b_{m})=\sum_{k=1}^{m}f((x_{k})_{1}, \dots , (x_{k})_{m})
$$

**Proof**: We follow a similar argument to the one presented in the previous exercise solution.

$$
\begin{gathered}
f(a_{1}, \dots)-f(b_{1},\dots)=\\
f(a_{1}-b_{1}, a_{2}, \dots)+f(b_{1}, a_{2}, \dots) - f(b_{1}, \dots) = \\
f(a_{1}-b_{1}, a_{2}, \dots) + f(b_{1}, a_{2}-b_{2}, a_{3}, \dots) + f(b_{1}, b_{2}, a_{3}, \dots) - f(b_{1}, \dots) \underset{\text{repeat }m \text{ times}}{=}\\
\sum_{k=1}^{m}f((x_{k})_{1}, \dots, (x_{k})_{m}) + f(b_{1}, \dots) - f(b_{1}, \dots) = \\ 
\sum_{k=1}^{m}f((x_{k})_{1}, \dots, (x_{k})_{m}) + f(b_{1}, \dots)
\end{gathered}
$$
Which completes the proof.

**Definition (bounded multilinear operator)**: Let $f: X^{n} = X_{1} \times \dots \times X_{n} \to F$ where $X_{1}, \dots, X_{n}, F$ are normed vector spaces, then $f$ is said to be **bounded** if $\exists M \gt 0$ such that

$$
$\forall x \in X^{n}, ||f(x_{1}, x_{2}, \dots)||_{F} \leq M \prod_{i=1}^{n}||x_{i}||
$$
For $n=1$, we retrieve the definition for a bounded linear operator.

**Definition (multilinear operator norm)**: Using the previous result, take the supremum on $M$ which satisfies the inequality, then the **operator norm** of $f$ denoted by $||f||$ is $M$:

$$
||f(x_{1}, \dots)|| \leq ||f|| \prod_{i=1}^{n}||x_{i}||
$$

One can further verify that this is indeed a norm and that for $n=1$ we retrieve the definition for a linear operator norm.

**Theorem**: Let $f: X^{m}=X_{1} \times X_{2} \times \dots \times X_{m} \to F$ be a multilinear map and let $X^{m}, F$ be Banach vector spaces, then the following are equivalent:
1. $f$ is continuous in its domain.
2. $f$ is continuous at the origin $x_{0}=(0, 0, \dots, 0)$.
3. $f$ is a bounded operator (i.e. the operator norm for $f$ is defined).

Note that this is a generalization of the thoerem for linear operators.

**Proof**: $1 \implies 2$ is trivial.

$2 \implies 3:\quad$First note that if any of the components of $x \in X^{m}$ is zero then by proposition $(1)$ we have $f(x_{1}, \dots)=0$ so clearly $f$ is bounded in these cases, so suppose none of the component of $x$ are zero.

Suppose $f$ is continuous at the origin, then the pre-image of a unit ball in the image of $f$ around the origin is open and $\exists r \gt 0$ for which the product of $r$-open balls in each $X_{i}$ around the respective origin (we denote these via $B_{X_{1}}(0, r)$, meaning a ball in $X_{1}$ centred at the point $0 \in X_{1}$ with radius $r$) is in the pre-image of the unit ball of $f$ around the origin, i.e.

$$
x \in B_{X_{1}}(0, r) \times \dots \times B_{X_{m}}(0, r) \implies f(x_{1}, \dots) \in B_{F}(0, 1)
$$
Now, consider an arbitrary point $x \in X^{m}$ (not necessarily in the product of open balls). Take some component $x_{i} \in x$, then $g: X_{i} \to X_{i}$ defined as $g(x_{i})=\frac{r}{||x_{i}||}x_{i}$ transforms $x_{i}$ to a member of $B_{X_{i}}(0, r)$, and also $x_{i}=\frac{||x_{i}||}{r}\frac{r}{||x_{i}||}x_{i}=\frac{||x_{i}||}{r}g(x_{i})$. By a corollary to proposition $(2)$, we have

$$
f(x_{1}, x_{2}, \dots) = f(\frac{||x_{1}||}{r}g(x_{1}), \frac{||x_{2}||}{r}g(x_{2}), \dots)= \prod_{i=1}^{m}\frac{||x_{i}||}{r}f(g(x_{1}), g(x_{2}), \dots) \in B_{F}(0, 1)
$$
So

$$
||f(x_{1}, x_{2}, \dots)|| = \frac{1}{r}\prod_{i=1}^{m}||x_{i}||f(g(x_{1}), \dots) \leq \frac{1}{r}\prod_{i=1}^{m}||x_{i}||
$$
Denote $M=\frac{1}{r}$,  then $f$ is bounded at any point $x \in X^{m}$ be definition.

$3 \implies 1:\quad$Let $\epsilon \gt 0$. Suppose $f$ is bounded and let $x \in X^{m}$ and $x_{n}$ be a sequences such that $x_{n} \underset{n \to \infty}{\to}x$. We wish to show that $f(x_{n}) \underset{n \to \infty}{\to}f(x)$, which will prove that $f$ is sequentially continuous, which implies continuity in a metric space. 

Define a finite sequence $c_{k} \subseteq X^{m}$ as follows

$$
(c_{k})_{i}=\begin{cases} x_{n_{i}} & i \lt k \\ x_{n_{i}} - x_{i} & i= k \\ x_{i} & i \gt k\end{cases}
$$

Consider $||f(x_{n_{1}}, x_{n_{2}}, \dots)-f(x_{1}, x_{2}, \dots)||$:

$$
\begin{gathered}
||f(x_{n_{1}}, \dots) - f(x_{1}, \dots)|| \underset{\text{by theorem}}{=}\\
||\sum_{k=1}^{m}f(c_{k_{1}}, \dots)|| \underset{\triangle \text{ inequality}}{\leq}\\
\sum_{k=1}^{m}||f(c_{k_{1}}, \dots)|| \underset{\text{bounded}}{\leq}\\
\sum_{k=1}^{m}\left(M\prod_{i=1}^{m}||c_{k_{i}}||\right)=M \sum_{k=1}^{m}\left(\prod_{i=1}^{m}||c_{k_{i}}||\right) = (*)
\end{gathered}
$$
For some $M \gt 0$. Since $x_{n}$ converges to $x$ then each component $x_{n_{i}}$ also converges to $x_{i}$ by a previous result (which was proven for Euclidean spaces but holds for any product space), then $\forall \delta \gt 0, \exists N \in \mathbb{N}$ such that $\forall n \in \mathbb{N}, (n \gt N \implies d(x_{n_{i}}, x_{i}) \lt \delta$), so by the second triangle inequality

$$
| \space ||x_{n_{i}}|| - ||x_{i}|| \space | \leq ||x_{n}-x_{i}|| \lt \delta \Rightarrow ||x_{n_{i}}|| \lt \delta + ||x_{i}||
$$
Now take $A=\max\{\delta + ||x_{1}||, \delta + ||x_{2}||, \dots, \delta + ||x_{m}||\}$,  then $\forall x_{i} \in x, ||x_{n_{i}}|| \lt A$, and also $||x_{i}|| \lt A$. Now consider one summand $\prod_{i=1}^{m}||c_{k_{i}}||$, then

$$
\begin{gathered}
\prod_{i=1}^{m}||c_{k_{i}}||=||c_{k_{1}}|| \space ||c_{k_{2}}|| \space \dots  \space ||c_{k_{m}}|| = \\
||x_{n_{1}}|| \space ||x_{n_{2}}|| \space \dots ||x_{n_{k}}-x_{k}|| \space ||x_{k+1}|| \dots ||x_{m}|| \lt \\
A^{m-1} \delta
\end{gathered}
$$
So we have

$$
(*) \lt M\sum_{k=1}^{m}A^{m-1}\delta = mMA^{m-1}\delta = (**)
$$
Now, pick $\delta=\frac{\epsilon}{mMA^{m-1}}$, and we get $(**) \lt \epsilon$, so by definition $f(x_{n}) \underset{n \to \infty}{\to}f(x)$, so $f$ is continuous.

### Isometry of $\mathcal{L}(X1, X2;F)$ and $\mathcal{L}(X1,\mathcal{L}(X2; F))$

In this section we present the natural isometry of $\mathcal{L}(X_{1}, X_{2}; F)$ and $\mathcal{L}(X_{1}, \mathcal{L}(X_{2}; F))$

First let's limit out discussion to billinear maps, i.e. maps of the form $f \in \mathcal{L}(X_{1}, X_{2}; F)$, $(x_{1}, x_{2}) \mapsto f(x_{1}, x_{2})$. Suppose $f$ is bounded, i.e. $\forall (x_{1}, x_{2}) \in X_{1} \times X_{2}, ||f(x_{1}, x_{2})|| \leq ||f|| \space ||x_{1}|| \space ||x_{2}||$, where each norm is the appropriate norm for the object (i.e. $||x_{1}||$ is $||x_{1}||_{X_{1}}$ etc), and consider the partial mapping $f_{x_{1}}$ (i.e. $f$ such that $x_{2}$ is fixed), which is a linear mapping $X_{2} \to F, x_{2} \mapsto f(x_{1}, x_{2})$, then we have

$$
\forall x_{2} \in X_{2}, \quad||f_{x_{1}}(x_{2})||=||f(x_{1}, x_{2})|| \leq ||f|| \space ||x_{1}|| \space ||x_{2}||
$$
And so we have $\frac{||f_{x_{1}}(x_{2}))||}{||x_{2}||} \leq ||f|| \space ||x_{1}||$, so by definition $f_{x_{1}}$ is a bounded (linear) operator and so by a previous result it is a continuous (linear) operator. Note that $f_{x}$ is nothing more than a mapping $X_{1} \to \mathcal{L}(X_{2}; F), x_{1} \mapsto f_{x_{1}}$. Denote this mapping $g$, then $f_{x_{1}}=g(x_{1})$. Also, observe that $g$ is also linear, since it is additive

$$
g(x_{1})(x_{2}+x_{3})=f_{x_{1}}(x_{2}+x_{3})\underset{\text{linearity of } f_{x_{1}}}{=}f_{x_{1}}(x_{2})+f_{x_{2}}(x_{3})=g(x_{1})(x_{2})+g(x_{1})(x_{3})
$$
And homogeneous

$$
g(x_{1})(\alpha x_{2})=f_{x_{1}}(\alpha x_{2})= \alpha f_{x_{1}}(x_{2})=\alpha g(x_{1})(x_{2})
$$
Finally, since $g(x_{1})=f_{x_{1}}$ and since $f_{x_{1}}$ is a bounded operator such that $||f_{x_{1}}|| \leq ||f|| \space ||x_{1}||$ as shown above, then $g(x_{1}) \leq ||f|| \space ||x_{1}||$ and so $g$ is also a bounded (linear) operator and thus continuous, with $||g|| \leq ||f||$.

Observe that the mapping $g: X_{1} \to \mathcal{L}(X_{1}; \mathcal{L}(X_{2} ;F))$ is defined by $f$, so we have naturally defined a mapping

$$
\phi: \mathcal{L}(X_{1}, X_{2}; F) \to \mathcal{L}(X_{1}, \mathcal{L}(X_{2};F)), f \mapsto g
$$
Since $f, g$ are linear then $\phi$ must also be linear, finally since we have $||g|| \leq ||f||$then it follows that $\phi$ is also bounded with $||\phi|| \leq \frac{||g||}{||f||} \leq 1$.

Similarly, consider a mapping

$$
\tau: \mathcal{L}(X_{1}, \mathcal{L}(X_{2}; F)) \to \mathcal{L}(X_{1}, X_{2}; F), g \mapsto f
$$
Clearly $\tau$ and $\phi$ are inverse maps, and clearly $\tau$ is linear, and again since $||g|| \leq ||f||$ we have $||\tau|| \leq 1$, but since $\tau$ and $\phi$ are inverses of each other we have that $\tau \circ \phi$ is the identity on $\mathcal{L}(X_{1}, \mathcal{L}(X_{2}; F))$, so $||\tau \circ \phi||=1$, but also $||\tau \circ \phi|| \leq ||\tau|| \space ||\phi|| \leq 1$, so we have $||\tau||=||\phi||=1$, so $\tau, \phi$ are global isometries, so they define an isomorphism $\mathcal{L}(X_{1}, \mathcal{L}(X_{2};F)) \cong \mathcal{L}(X_{1}, X_{2}; F)$,

This argument can be repeated by induction for arbitrary finite dimensional Cartesian product spaces. We call this the natural isomorphism between multilinear maps and linear maps.

**Corollary**: Let $f: X^{n}=X_{1} \times \dots \times X_{n} \to F, (x_{1}, \dots) \mapsto f(x_{1}, \dots)$, then $\exists g \in \mathcal{L}(X_{1}; \mathcal{L}(X_{2}; \mathcal{L}(X_{3}; \dots, \mathcal{L}(X_{n}; F))))$ such that

$$
f(x_{1}, x_{2}, \dots, x_{n})=g(x_{1})(x_{2})\dots(x_{n}) \underset{\text{linearity}}{=}(\dots(g(x_{1})\cdot x_{2}) \cdot \dots \cdot x_{n})
$$
This is a direct result of the isomorphism extended for finite dimensional Cartesian product spaces. Note that the inverse is also true.

## Derivative of Multilinear Maps

Let $f \in \mathcal{L}(X, Y; F), (x, y) \mapsto f(x, y)$ be a bounded billinear map over Banach spaces $X, Y, F$. We wish to find its Frechet derivative at some $(x, y)$. Let $(h, k) \in X \times Y$. We know that

$$
f(x+h, y+k)=f(x, y+k)+f(h, y+k)=f(x, y)+f(x, k)+f(h, y)+f(h, k)
$$
Now consider $f(h, k)$. Since $f$ is bounded, we know that

$$
0 \leq ||f(h, k)|| \leq ||f|| \space ||h|| \space ||k|| \leq ||f||\space (\max\{||h||, ||k||\})^{2}\underset{\text{product metric}}{=} ||f|| \space ||(h, k)||^{2}
$$
Dividing both sides by $||(h, k)||$, we get

$$
0 \leq \frac{||f(h, k)||}{||(h, k)||} \leq ||f|| \space ||(h, k)|| 
$$
As $(h, k) \to (0, 0)$, the RHS becomes a constant $||f||$ times $0$, so the RHS limit is also zero, so by the squeeze theorem we have $\frac{||f(h, k)||}{||(h, k)||} \underset{(h, k) \to (0, 0)}{\to}0$, so $f(h, k)=o(||(h, k)||)$, and we can write

$$
f(x+h, y+k)=f(x, y)+(f(x, k)+f(h, y))+o(||(h, k)||)
$$
Clearly $f(x, k)+f(h, y)$ is linear in each component since it is a summation of multilinear maps, so by the second definition of the Frechet derivative we have

$$
df(x, y)(h, k)=f(x, k) + f(h, y)
$$
This result can be generalized for any finite dimensional product space by the following theorem:

**Theorem**: Let $f: X^{n}=X_{1} \times X_{2} \times \dots \times X_{n} \to F$ be a bounded multilinear function, then $f$ is **differentiable** in $X^{n}$ and the derivative of $f$ at a point $(x_{1}, x_{2}, \dots) \in X^{n}$ evaluated at a point $(h_{1}, h_{2}, \dots) \in X^{n}$ is given by

$$
df(x_{1}, x_{2}, \dots) = df(h_{1}, x_{2}, \dots)+ df(x_{1}, h_{2}, x_{3}, \dots) + df(x_{1}, \dots, x_{n-1}, h_{n})
$$
Equivalently, define a finite sequence $c_{k} \subseteq X^{n}$ as follows:

$$
(c_{k})_{i}=\begin{cases} x_{i} & i \neq k \\ h_{i} & i = k\end{cases}
$$
Then

$$
df(x_{1}, x_{2}, \dots)=\sum_{k=1}^{n}df(c_{k_{1}}, c_{k_{2}}, \dots)
$$

**Corollary (Product rule)**: Let $(Y, X)$  be Banach (sub-)spaces and $f: X \to \mathbb{R}, g: X \to Y$ be differentiable on $X$, then $\forall x \in X$,

$$
d(fg)(x)=f(x)dg(x) + df(x)g(x)
$$
Where $fg(x)=f(x)g(x)$.

**Proof**: Consider a mapping $h: X \to \mathbb{R} \times Y, x \mapsto (f(x), g(x))$ and a mapping $\varphi: \mathbb{R} \times Y \to Y, (\alpha, y) \to \alpha y$. It is apparent that

$$
f(x)g(x)= (\varphi \circ h)(x)
$$
Now by the chain rule we know that

$$
d(\varphi \circ h)(x)=(d\varphi)(h(x))(dh(x)
$$
Clearly $dh(x)=(df(x), dg(x))$, and since $\varphi \in \mathcal{L}(\mathbb{R}, X;Y)$ then

$$
d\varphi(f(x), g(x))(df(x), dg(x))=\varphi(f(x), dg(x))+\varphi(df(x), g(x))=f(x)dg(x)+df(x)g(x)
$$
Which completes the proof.

**Example**: We shall calculate $x \sin x$. Define $f(x)=x$ and $g(x)=\sin x$, then $(fg)(x)=x \sin x$, now by the product rule

$$
(x \sin x)'=x (\sin x)'+ (x)' \sin x = x \cos x + \sin x
$$
Here we used the derivative of $\sin x$ as a classical result without proof.

## Higher Order Derivatives

Let $X, Y$ be Banach spaces and let $f: X \to Y$ be Frechet differentiable in an open subset $U \subseteq X$, then exists $Df: U \to \mathcal{L}(X;Y), x \mapsto Df(x)$ a bounded linear operator such that $Df(x)$ is the Frechet derivative of $f$ at some $x \in U$. Now suppose $Df(x) \in \mathcal{L}(X;Y)$ is also Frechet differentiable by definition at $x$, i.e. $\exists T: X \to \mathcal{L}(X; \mathcal{L}(X;Y))$ such that

$$
\lim_{h \to 0}\frac{||Df(x+h)-Df(x)-T(h)||}{||h||}=0
$$
with $T=d(df(x))(x)$.

Which motivates the following definition:

**Definition (second-order derivative)**: Let $X, Y$ be Banach spaces and let $f: X \to Y$ be Frechet differentiable at an open subset $U \subseteq X$ and let $x \in U$. If the linear operator $df(x)$ (i.e. the **first-order derivative**) is Frechet differentiable at $x$, then we say that $f$ is **twice differentiable** at $x$, and denote the Frechet derivative of $df(x)$, i.e. $d(df(x))(x)$, and call it the **second-order derivative** of $f$ at $x$. By the natural isomorphism $\mathcal{L}(X;\mathcal{L}(X; Y)) \to \mathcal{L}(X, X;Y)$ defined in our discussion of mutlilinear maps, the second derivative also uniquely defines a billinear map, so by abuse of notation we usually denote $d^{2}f(x, x)$ as the second Frechet derivative of $f$ at $x$.

**Definition (higher-order derivative)**: We recursively extend the definition of the second-order derivative to define the $n$-th order derivative of $f: X \to Y$ at some point $x \in X$. The $n$-th order derivative is identified with a map

$$
d^{n}f: \mathcal{L}(\underbrace{X, X, \dots}_{n \text{ times}};Y), \quad (\underbrace{x, x, \dots}_{n \text{ times}}) \mapsto d^{n}f(\underbrace{x, x, \dots}_{n \text{ times}})
$$
For brevity, we often denote the $n$-tuple $(x, x, \dots)$ as $(x)^{n}$, and the we simply write $d^{n}f(x)^{n}$. This is **not** to be confused with "taking $x$ to the $n$-th power".

**Definition (higher-order Frechet differentiability)**: Let $f: X \to Y$ and let $U \subseteq X$ be an open subset, then $f$ is said to be $n$ times differentiable at $U$ if $d^{n}f(x)^{n}$ exits for all $x \in U$.

**Theorem**: Let $f: X \to Y$ be $n$ times differentiable at $X$, then $\forall a \in X$ and $\forall x \in X^{n}$ (where $X^{n}$ is the product space of $X \times \dots \times X$), then $d^{n}f(a): X^{m} \to Y$ is a **symmetric** mutlilinear map, i.e. for every possible permutation $p$ of the components of $x$ ($x_{1}, x_{2}, \dots, x_{n}$), we have

$$
d^{n}f(a)(x_{1})(x_{2})\dots(x_{n})=d^{n}f(a)(p_{1})(p_{2})\dots (p_{n})
$$
**Proof sketch**: The proof for this theorem is rather involved, so instead we limit ourselves to the case where $n=2$ and then $f$ becomes $f: X_{1}, X_{2} \to Y$, and we only present a sketch for that proof.

Let $\epsilon \gt 0$ and let $(h, k) \in X_{1} \times X_{2}$ and fix $a \in X_{1}$. Define the function $A: X_{1} \times X_{2} \to Y$ as

$$
A(h, k) = f(a+h+k)-f(a+h)-f(a+k)+f(a)
$$
Clearly $A$ is a symmetric map ($A(h, k)=A(k, h)$). Let $df^{2}(a)$ be the second derivative of $f$ at $a$.

**Lemma**: $||A(h, k) - df^{2}(a)(k)(h)|| = o(||(h, k)||^{2})$

**Proof**: First, rewrite the expression as

$$
\begin{gathered}
||A(h, k) - df^{2}(a)(k)(h)||=\\
||A(h, k)-df(a+k)(h)+ df(a)(h)+df(a+k)(h)-df(a)(h)-df^{2}(a)(k)(h)|| \underset{\triangle \text{ ineq.}}{\leq}\\
\underbrace{||A(h, k)-df(a+k)(h)+df(a)(h)||}_{(1)}+\underbrace{||df(a+k)(h)-df(a)(h)+df^{2}(a)(k)(h)||}_{(2)}
\end{gathered}
$$
Consider $(2)$, Since $df, d^{2}f$ are bounded linear operators, we have

$$
\begin{gathered}
(2) \leq ||h|| \space ||df(a+k)-df(a)+df^{2}(a)(k)||\underset{\text{differentiability at $a$}}{=} ||h|| \space o(||k||) \leq\\ &||h|| \space o(||(h, k)||)
\end{gathered}
$$
As for $(1)$, it can be shown by using the mean-value inequality which will be proven later (but will not be shown here due to complexity and since we still haven
t introduced the mean-value inequality) that $(1) \leq ||h|| \space o(||(h, k)||)$ as well, so in total we have $(1)+(2) \leq ||h|| \space o(||(h, k)||)\leq o(||(h, k)||^{2})$, which completes the proof.

**Corollary**: Since $A(h, k)=A(k, h)$, we can follow the same proof for $||A(k, h)-df^{2}(a)(k)(h)||$ and arrive at the same result and then finally replace $A(k, h)$ with $A(h, k)$ and get $||A(k, h)-df^{2}(a)(k)(h)||=o(||(h, k)||^{2})$.

**Proof sketch (cont'd)**: Continuing from where we left off, we have

$$
\begin{gathered}
(*)=||df^{2}(a)(k)(h)-df^{2}(a)(h)(k)|| = \\||A(h, k)-df^{2}(a)(h)(k)-(A(k, h)-df^{2}(a)(k)(h))|| \underset{\triangle \text{ ineq.}}{\leq} \\
||A(h, k)-df^{2}(a)(h)(k)||+||A(k, h)-df^{2}(a)(h)(k)=\\o(||(h, k)||^{2})
\end{gathered}
$$
So for small enough $||h||, ||k||$, we have $(*) = \epsilon(||(h,k)||^{2})$, and $\forall h, k \in X$ we can find $\lambda \in \mathbb{R}$ such that $||\lambda h||+||\lambda k||$ is small enough to satisfy this relation, so by homogeneity of $f$ we have 

$$
||df^{2}(a)(\lambda k)(\lambda h)-df^{2}(a)(\lambda k)(\lambda h)||=\lambda^{2} ||df^{2}(a)(k)(h)-df^{2}(a)(k)(h)||=\lambda^{2} \epsilon (||(h,k)||^{2})
$$

From here it is easy to carry out the rest of the proof.

**Definition (higher-order partial derivative)**: Let $X, Y$ be Banach spaces and let $f: X \to Y$ have some partial derivative $f_{x_{i}}$ at a point $x$, then we say $f_{x_{i}}$ is a **first-order partial derivative** of $f$ at $x$ with respect to $e_{i}$. If $f_{x_{i}}$ itself has a partial derivative with respect to some $x_{j}$ (not necessarily $x_{i}$), then we say that its partial derivative with respect to $x_{j}$ is a **second-order partial derivative** of $f$ at $x$ with respect to $e_{i}, e_{j}$, and denote it either via $f_{e_{i}e_{j}}(x)$ or $\frac{\partial ^{2} f}{\partial e_{i} \partial e_{j}}(x)$. This notion generalizes to higher-order partial derivatives as well. Note that when the higher-order derivatives are taken with respect to different basis vectors, we also call it a **mixed partial derivative**.

**Theorem (Schwart'z theorem)**: Let $f: \mathbb{R}^{m} \to  \mathbb{R}$ and suppose $f$ has continuous second-order partial derivative at $x \in \mathbb{R}^{m}$ with respect to $x_{i}, x_{j}$, i.e. both $f_{x_{i}x_{j}}$ and $f_{x_{j}x_{i}}$ exist, then

$$
f_{x_{i}x_{j}}= \frac{\partial ^{2} f}{\partial x_{i} \partial x_{j}} = \frac{\partial^{2} f}{\partial x_{j} \partial x_{i}} = f_{x_{j}x_{i}}
$$

The theorem can be trivially proven by Green's theorem which deals with line integrals which we have not discussed so we will not present a proof.

**Definition (Hessian matrix)**: Let  $f: \mathbb{R}^{m} \to \mathbb{R}$, and suppose all second-order partial-derivatives of $f$ exist at some point $x \in \mathbb{R}^{m}$, then the **Hessian matrix** of $f$ at $x$ is a square $m \times m$ matrix defined as

$$
H_{f}(x) = \begin{pmatrix}
\frac{\partial^{2} f}{\partial^{2} x_{1}} & \frac{\partial^{2} f}{\partial x_{1}x_{2}} & \dots & \frac{\partial^{2} f}{\partial^{2} x_{1} \partial x_{m}} \\
\frac{\partial^{2} f}{\partial x_{2} \partial x_{1}} & \frac{\partial^{2} f}{\partial^{2} x_{2}} & \dots & \frac{\partial^{2} f}{\partial x_{2} \partial x_{m}} \\
\vdots & \ddots & 0 & \vdots \\
\frac{\partial^{2} f}{\partial x_{m} \partial x_{1}} & \frac{\partial^{2} f}{\partial x_{m} \partial x_{2}} & \dots & \frac{\partial^{2} f}{\partial^{2} x_{m}} 
\end{pmatrix}
$$
i.e., the $ij$-th entry of $H$ is

$$
(H_{f}(x))_{ij} = \frac{\partial^{2} f}{\partial x_{i} \partial x_{j}}
$$
**Corollary 1**: $H_{f}(x) = J(\nabla f(x))^{T}$.

**Proof**: Suppose $f: \mathbb{R}^{m} \to \mathbb{R}$. First observe that this relation is well-defined, i.e. $J(\nabla f(x))^{T}$ is an $m \times m$ matrix. By previous results $\nabla f(x)$ is a function $\mathbb{R}^{m} \to \mathbb{R}^{m}$, so its Jacobian can be taken and results in an $m \times m$ matrix whose transpose is also $m \times m$. Now, recall the definition of the Jacobian by $ij$ indexing (we intentionally replace $i$ and $j$ to get the $ij$-th entry of the transpose of the Jacobian):

$$
J(\nabla f(x))_{ji} = \frac{(\nabla f)_{j}}{\partial x_{i}}(x) = \frac{\frac{\partial f}{\partial x_{j}}}{\partial x_{i}}(x) = \frac{\partial f}{\partial x_{i} \partial x_{j}}(x) = (H_{f}(x))_{ij}
$$
Which completes the proof.

**Corollary 2**: If the second-order partial derivatives of $f$ are continuous, then $H_{f}(x)$ is symmetric.

**Proof**: By Schwart'z theorem we get $(H_{f}(x))_{ij} = f_{x_{i}x_{j}}=f_{x_{i}x_{j}}=(H_{f}(x))_{ji}$.

**Exercise for the reader**: Find the second Frechet derivative of some $f: \mathbb{R}^{m} \to \mathbb{R}$ and show how it relates to the Hessian matrix.

# Extremum

For the purpose of the following discussion, suppose that $f$ is a real-valued function whose domain is a Banach space $(X, ||\cdot||_{X})$, i.e. $f: X \to \mathbb{R}$. In $\mathbb{R}$ we have the usual notion of a total order so we can define the notion of an extremum, which describes points in the domain of $f$ for which the image is larger or smaller than the image of the entire domain, or an open $\epsilon$-neighbourhood of that point.

**Definition (Local extremum)**: Let $x_{0} \in X$. We say that $f$ attains a **local extremum** at $x_{0}$ if $\exists \epsilon \gt 0$ such that $\forall x \in X, (x \in B_{\epsilon}(x_{0}) \implies (f(x) \leq f(x_{0}) \lor f(x) \geq f(x_{0}))$. In the former ($f(x) \leq f(x_{0})$ for all $x$ in an $\epsilon$-ball centered at $x_{0}$,), we say that $f$ has a **local maximum** at $x_{0}$, in the latter we say that $f$ has a **local minimum** at $x_{0}$.

**Theorem (Extreme Value theorem)**: Let $f: X \to \mathbb{R}$ such that $X$ is compact and $f$ is continuous, then $f$ attains a minimum and maximum at $X$. Note that these are the **global minimum and maximum** (i.e. global extremum) at $X$, i.e. if $f(x_{0})$ is a global minimum then $\forall x \in X, f(x_{0}) \leq f(x)$, which is a stronger condition than that of a local minimum.

**Proof**: Recall that continuity preserves compactness, so $\text{Im}(f)=f(X)$ is compact. Now, since $\text{Im}(f)$ is compact in $\mathbb{R}$, then by the Heine-Borel property we know that $\text{Im}(f)$ is closed and bounded. Now, consider $\sup_{x \in X} f(x)$ and $\inf_{x \in X}f(x)$. Since $f(X) \subseteq \mathbb{R}$, then by the completeness axiom both are attained. Denote these values $M=\sup_{x \in X}f(x), m=\inf x_{\in X}f(x)$. It suffices to show that $\exists x_{1}, x_{2} \in X$ such that $f(x_{1})=M, f(x_{2})=m$, then by the definition of the supremum and infimum we know that $f$ attains a maximum at $x_{1} \in X$ and a minimum at $x_{2} \in X$.

Let $n \in \mathbb{N}$, and consider the sequence $a_{n}=M-\frac{1}{n}$. Since $M$ is the supremum of $f$ in $X$, then $\forall M' \lt M \in \mathbb{R}, \exists x\in X$ such that $f(x) \gt M'$, otherwise that $f(x)$ would be a tighter upper bound and $M$ would not be the supremum, so $\forall a_{n} \exists b_{n} \in X$ such that $a_{n} \lt f(b_{n}) \leq M$. Now observe that $a_{n} \underset{n \to \infty}{\to}M$, similarly since $X$ is compact then it is also sequentially compact so by the BW theorem it has a convergent sub-seqeunce $b_{n_{k}}$ which converges to some $b \in X$, and since $f$ is continuous we have $\lim_{n \to \infty} f(b_{n})=f(b)$, but now by the squeeze theorem we have $f(b) = \lim_{n \to \infty} f(b_{n}) = M$, so the supremum is attained in $X$. An identical argument is used to show that the infimum is also attained in $X$.

**Theorem (Fermat's Interior extremum theorem)**: Let $f: X \to \mathbb{R}$ where $X$ is an open set be Frechet differentiable at $X$, and suppose $f$ attains an extremum at some point $x_{0} \in X$, then $df(x_{0})=0$.

**Proof**: Let $x_{0}$ be an (interior) extremum and suppose it is a maximum, then $\exists \delta \gt 0$ such that $\forall x \in B_{\delta}(x_{0})$, $f(x) \leq f(x_{0})$, i.e. $f(x) - f(x_{0}) \leq 0$. Now since $f$ is Frechet differentiable at $X$ then it is also Frechet differentiable at $x_{0}$, and by a previous result it is also Gateaux differentiable at $x_{0}$ and both derivatives agree. Fix a direction $h \in X$. Recall the definition for the Gateaux derivative at $x_{0}$:

$$
D_{x_{0}}f(h) = \lim_{t \to 0}\frac{f(x_{0}+th)-f(x_{0})}{t}
$$
Notice that $d(x_{0}+th, x_{0})=||x_{0}+th-x_{0}||=||th||=|t| \space ||h||$, starting from a small enough $t$ such that $|t| \lt \frac{\delta}{||h||}$, we have $d(x_{0}+th, x_{0}) \lt \delta$, so $x_{0}+th \in B_{\delta}(x_{0})$, so $f(x_{0}+th)-f(x_{0}) \leq 0$. The limit on the RHS is a function of $t$ so we can discuss the one-sided limits of the RHS.

In the case $t \to 0^{+}$ we have $t \gt 0$, so $\frac{f(x_{0}+th)-f(x_{0})}{t} \leq 0$, in the case $t \to 0^{-}$ we have $t \lt 0$ so $\frac{f(x_{0}+th)-f(x_{0})}{t} \leq 0$. Since $D_{x_{0}}f(h)$ exists then the two limits $t \to 0^{+}$ and $t \to 0^{-}$ must agree, which is only possible if $D_{x_{0}}f(h)=0$, but since we picked an arbitrary direction $h$ we have that $D_{x_{0}}f = 0$, which is also the Frechet derivative at $x_{0}$, so $df(x_{0})=0$.

In the case of a minimum, the same argument is repeated with inverse signs in each direction, leading to the same conclusion.

**Corollary 1** : If $f: \mathbb{R} \to \mathbb{R}$ differentiable at $x_{0}$ and $x_{0}$ is an interior extremum, then $f'(x_{0})=0$.

**Corollary 2**: If $f: \mathbb{R}^{m} \to \mathbb{R}$ is differentiable at $x_{0}$ and $x_{0}$ is an interior extremum, then $\nabla f(x_{0})=0$ the zero vector.

**Proof**: Both corollaries follow from earlier result showing the equivalence of the notion of differentiability in $f: \mathbb{R}^{m} \to \mathbb{R}$ with $\dim m \geq 1$ and Frechet differentiability and showing that $df(x_{0})=f'(x_{0})$ in the case of $m = 1$ and $df(x_{0})=\nabla f(x_{0})$ in the case of $m \gt 1$ .

**Corollary 3**: Let $X$ be a compact Banach sub-space and let $f: X \to \mathbb{R}$ be continuously differential in $X$, then $f$ attains an extremum either in points of the form $x \in \partial X$ or in points of the form $x \in X, df(x)=0$.

**Proof**: If $x$ is an interior extremum then by the interior extremum theorem we know $df(x)=0$. If $x$ is not an interior extremum then it must be that $x \in \partial X$.

**Corollary 4**: Let $f: X \to \mathbb{R}$ with $X$ compact Banach sub-space, and suppose $f$ is differentiable on $\text{Interior}(X)$. If $f$ is constant on $X$, i.e. $\exists K \in \mathbb{R}$ such that $\forall x \in X, f(x)=K$, then  $\forall X \in X \setminus \partial X, df(x)=0$, i.e. the derivative of $f$ is always zero.

**Proof**: Since $f$ is constant then all points interior points of $X$ are interior extremum so by the interior extremum theorem $df(x)=0$. 

**Definition (critical point)**: Let $f: X \to \mathbb{R}$ and $x_{0} \in X$. We say that $x_{0}$ is a **critical point** of $f$ is either of the following is true:
1. $df(x_{0})$ is defined and $df(x_{0})=0$.
2. $df(x_{0})$ is undefined (doesn't exist).

**Corollary**: An extremum of a function can only occur at a critical point.

**Proof**: By the contra-positive of the interior extremum theorem, if $df(x_{0})$ exits and not $0$ then $x_{0}$ cannot be an extremum. If $df(x_{0})$ doesn't exist, then we cannot say whether $x_{0}$ is an extremum or not, even if we impose stronger restrictions on the function (for example, impose continuity).

**Theorem (Rolle's theorem)**: Let $f: X \to \mathbb{R}$ such that $X$ is a compact Banach sub-space and $f$ is differentiable on $X$. If $\exists K \in \mathbb{R}$ such that $\forall a \in \partial X, f(a)=K$ (i.e. $\text{Im}_{f}(\partial X) = \{ K \}$), then  $\exists c \in X$ such that $df(c)=0$.

**Proof**: $f$ is differentiable hence continuous on $X$, so by the extreme value theorem (EVT) $f$ attains an extrema in $X$. Let $M=\sup_{x \in X}f(x)$ and $m = \sup_{x \in X}f(x)$. If $m=M$ then $f$ must be a constant function and so for all interior points $x$ of $X$, $f(x)=f(a)=m=M$, but then $x$ is an interior extremum so by the interior extremum theorem $df(x)=0$, Otherwise $m \lt M$, so the extremum cannot be along the boundary, so it must be attained at an interior point, so by the interior extremum theorem, $df(x)=0$ at the interi/or extremum.

## Mean-Value Inequality

We begin with the familiar Mean-Value theorem for $f: A \subseteq \mathbb{R} \to \mathbb{R}$ and then generalize to Banach spaces. The Mean-Value theorem (MVT) and its generalized form the mean-value inequality will serve us later when discussing the Taylor expansion theorem in Banach spaces.

**Theorem (Mean-Value theorem)**: Let $f: I \subseteq \mathbb{R} \to \mathbb{R}$ be differentiable in the interior of $I$ and let $I$ be an interval whose boundary points are $a, b \in \mathbb{R}$, then $\exists c \in \mathbb{R}$ such that

$$
f'(c)=\frac{f(b)-f(a)}{b-a}
$$
**Proof**: Consider the function $h(x) = f(x)-\frac{f(b)-f(a)}{b-a}x$. Identify that since $f$ is differentiable and the other term is linear thus differentiable then $h$ is also differentiable in the interior of $I$ and $h'(x)=f'(x)-\frac{f(b)-f(a)}{b-a}$. Suppose $h$ has a interior extremum in $I$ at some point $c$, then $h'(c)=0$, but $h'(c)=f'(c)-\frac{f(b)-f(a)}{b-a}$, so $f'(c)=\frac{f(b)-f(a)}{b-a}$. Observe that

$$
h(a) = f(a) - \frac{f(b)-f(a)}{b-a}a = \frac{f(a)b-f(a)a - f(b)a+f(a) a}{b-a} = \frac{f(a)b - f(b)a}{b-a}
$$

$$
h(b)=f(b)-\frac{f(b)-f(a)}{b-a}b = \frac{f(b)b - f(b)a - f(b)b + f(a)b}{b-a}=\frac{f(a)b-f(b)a}{b-a}
$$
So $h(a)=h(b)$ and since $\partial I = \{a, b\}$ we apply Rolle's theorem and find that $\exists c \in I$ such that $h'(c)=0$, which completes the proof.

**Corollary**: Let $y: \mathbb{R} \to \mathbb{R}$ be the tangent of $f$ at $c$, then the slope of the tangent is equal to the slope of the secant connecting $f(b)$ and $f(a)$.

**Proof**: The tangent of $f$ at $c$ is given by $y(x) = f'(c)(x-c)+f(c)$, and its slope is $f'(c)$. The slope of the secant connection $f(b)$ and $f(a)$ is $\frac{f(b)-f(a)}{b-a}$, and by the MVT they are equivalent.

**Corollary**: Let $f: I \subseteq \mathbb{R} \to \mathbb{R}$ where $I$ is an interval and let $f$ be differentiable in the open interval of $I$, and denote the boundary of the interval $\partial I = \{a, b\} \subseteq I$. The following is true:
1. If $\forall c \in I, f'(c)=0$, then $f$ is constant in $I$.
2. If $\forall c \in I, f'(c) \gt 0$, then $f$ is monotonically increasing in $I$ (i.e. $\forall x, y \in I, (y \gt x \implies f(y) \gt f(x)$).
3. If $\forall c \in I, f'(c) \lt 0$, then $f$ is monotonically decreasing in $I$.

**Proof**: 
1. By the MVT, $\exists c \in (a, b)$ such that $f'(c)=\frac{f(b)-f(a)}{b-a}$, but if $f'(c)=0$ for all $c$ then we have $f(b)=f(a)$,  and the same holds for every sub-interval, so $f$ is constant in $I$.
2. Again by the MVT we have that $\forall x, y \in I$ that $\frac{f(y)-f(x)}{y-x} \gt 0$, which is true only if either both the nominator and the denominator are positive, which is true if and only if $y \gt x$ and $f(y) \gt f(x)$,  or if both are negative, which is true if and only if $y \lt x$ and $f(y) \lt f(x)$, so if $y \gt x$ then $f(y) \gt f(x)$.
3. The proof is identical to the one described in the previous section.

To prove the theorem, we need to consider $f$ in some closed interval. The reason for this is that to use Rolle's theorem, we need the domain of $f$ to be compact and for the only boundary points of the domain to be $a, b$ i.e. the ends of the interval, i.e. we needed every point in $x \in \mathbb{R}$ such that $a \lt x \lt b$ to be in the domain of the function, i.e. we needed the domain to be **connected**.

Recall that a generalization of an interval in $\mathbb{R}$ is a convex set. Indeed, we can generalize the mean-value theorem to higher dimensions as follows:

**Theorem (extended mean-value theorem)**: Let $X$ be a Banach space and let $f: P \subseteq X \to \mathbb{R}$ be Frechet differentiable in $P \setminus \partial P$, and let $P$ be a convex set, then $\forall a, b \in P$, $\exists c \in P$ such that

$$
df(c)(b - a)= f(b) - f(a)
$$
**Proof**: Let $\varphi: [0, 1] \to X, \varphi(t)=(1-t)a + tb$ be a parameterization of $l_{ab}$. Clearly $\varphi(t)$ is continuous and also since $P$ is convex, $\text{Im}(\varphi) \subseteq P$. Now, consider $h = f \circ \varphi$. $f$ is differentiable hence continuous in $P$ and in particular in $\text{Im}(\varphi)$, so $h$ is continuous as the composition of continuous functions. Observer that $h$ is a mapping $[0, 1] \to \mathbb{R}$, so we can employ the MVT, so $\exists c \in [0, 1]$ such that

$$
h'(c)=\frac{h(1)-h(0)}{1 - 0} = h(1) - h(0)
$$

But since $h(0)=f(\varphi(0)) = f(a)$, and also $h(1)=f(b)$, we have $h'(c)=f(b)-f(a)$. Now by the chain rule, we have $h'(c)=df(h)(c)=d(f \circ \varphi)(c) = df(\varphi (c)) \space d \varphi(c)$. Now observe that $d \varphi (x) = -a + b = b - a$, so $d \varphi (c) = b-a$, and also $df(\varphi (c))=\nabla f(\varphi (c))$ by a previous result. Finally, denote $c' = \varphi(c) \in l_{ab} \subseteq P$, and we get:

$$
df(c')(b-a) = f(b)-f(a)
$$
Which completes the proof.

We cannot generalize the MVT further, however we can provide a related albeit weaker result for generalized Banach spaces.

**Theorem (mean-value inequality)**: Let $X, Y$ be Banach spaces, and let $U \subseteq X$ be an open set, and let $f: U \to Y$ be differentiable. If $\exists C \subseteq U$ such that $C$ is a convex subset of $U$, and $\exists k \gt 0$ such that $\forall x \in C, ||df(x)||_{Y} \leq k$, then $\forall a, b \in C$ we have

$$
||f(b)-f(a)||_{Y} \leq k ||b-a||_{X}
$$
**Proof**: Let $C$ be a convex subset of $U$ and suppose $\forall x \in C, ||df(c)(x)||_{Y}$ is bounded with some constant $k \gt 0$, and let $b \in C$. We first prove a simpler version of the theorem, where $X=\mathbb{R}$. In this case, $f$ becomes $f: \overline{I} \to Y$ where $I$ is an open interval and $\overline{I}$ is its closure (i.e. the closed interval corresponding to $I$), and $||b-a||_{X}$ becomes the more familiar $b-a$, so the inequality now becomes

$$
||f(b)-f(a)||_{Y} \leq k(b-a)
$$
Fix $a$ and take some $x \in I$. The claim above is equivalent to

$$
\forall x \in I, ||f(x)-f(a)||_{Y} \leq k(x-a)
$$
The outlay of the proof is as follows:
1. We make a stronger claim, which will involve $\epsilon$ notation. This will come in handy later on when we introduce the derivative of $f$ (which we assume to be bounded).
2. We assume by contradiction that the stronger claim does not hold, and then classify the points for which it does not hold by introducing a new function (similar to what we did in the proof for the original MVT) and showing that the set of points for which the claim does not hold is open.
3. We involve the infimum of that set of points and show that it must be in $(a, b)$,
4. We show that there must exist a point in the set which is in a $\delta$-neighbourhood of the infimum of that set for which we can write down the $\epsilon-\delta$ definition of a limit (i.e. the derivative).
5. We show that this leads to a property which contradicts the membership test of the set of points introduced in $(2)$, thus arriving at a contradiction.
6. By contradiction the stronger claim is proven true, so the weaker claim is also true, which comletes the proof.

Let $\epsilon \gt 0$. Observe that $k(x-a) \leq (k+\epsilon)(x-a)$, so if the claim holds then $\forall x \in I, ||f(x)-f(a)||_{Y} \leq (k+\epsilon)(x-a)$. Let $U_{\epsilon}$ be the set of points for which the latter inequality does not hold, i.e.

$$
U_{\epsilon} = \{ x \in I | \quad ||f(x)-f(a)||_{Y} \gt (k+\epsilon)(x-a) \}
$$
Consider the function $\varphi: I \to \mathbb{R}$,  $\varphi(x) = ||f(x)-f(a)||_{Y} - (k+\epsilon)(x-a)$, we can rewrite $U_{\epsilon}$ as $U_{\epsilon}=\{ x \in I | \varphi(x) \gt 0\}$.  By a previous result we know that the norm of a vector space is a continuous function, so $\varphi$ is an addition of continuous functions and thus the pre-image of open sets of $\text{Im}(\varphi)$ is an open set. Observe that the subset of $\text{Im}(\varphi)$ which satisfies $\varphi(x) \gt 0$ must be an open set since its boundary $0$ is not in the set, so $U_{\epsilon}$ must also be open by continuity. 

Suppose by contradiction that $U_{\epsilon}$ is not empty. Since $U_{\epsilon}$ is bounded (by $a$ below and $b$ above) it has an $c \in \mathbb{R}$ such that $c \geq a$ (since $a$ is a lower bound), and by the completeness axiom the infimum exists. Observe that if $c=a$, and since $U$ is a subspace of a Banach space, then $\exists b_{n} \in U_{\epsilon}$ some sequence such that it converges to its infimum, but since $\varphi$ is continuous, we have $\lim_{n \to \infty} \varphi(b_{n})=\varphi(a)$, but clearly $\varphi(a)=-\epsilon \lt 0$ even though $\varphi(b_{n}) \gt 0$ for all $n$ so $\varphi(a)$ should be $\geq 0$, so $c \neq a$. Furthermore, $c \notin U_{\epsilon}$ since $c$ is the infimum of a subset of $\mathbb{R}$ so it must belong to $\partial U_{\epsilon}$ however $U_{\epsilon}$ is open so $U_{\epsilon} \cap \partial U_{\epsilon} = \emptyset$ so $c \notin U_{\epsilon}$. Finally, $c$ must be strictly less than $b$, otherwise we can take any $x \in I$ such that $c \lt x \lt b$ and it would be a better upper lower bound.

Recall that by assumption $||df(x)|| \leq k$ for all $x \in I$, it follows that $||df(c)|| \leq k$. Since $f$ is Frechet differentiable in $I$, so we have

$$
\frac{||f(c)-f(x)-df(c)(c-x)||_{Y}}{c-x} \underset{c \to x}{\to} 0
$$
And since $df(c)$ is a linear operator and since $\frac{1}{c-x}$ is a scalar, we have that the LHS is equivalent to

$$
\left| \left | \frac{f(c)-f(x)}{c-x}-df(c) \right | \right |_{Y} \underset{c \to x}{\to}0
$$
Which is equivalent to $\lim_{c \to x}\frac{f(c)-f(x)}{c-x}=df(c)$,.

Now by definition of the limit of a function, take $\epsilon$ (the same $\epsilon$), then $\exists \delta \gt 0$ such that $\forall x \in B_{\delta}(c) \setminus \{c\}$, we have

$$
\left|\left|\frac{f(c)-f(x)}{c-x} - df(c)\right|\right|_{Y} \lt \epsilon
$$
Which implies by the second triangle-inequality

$$
\left | \left| \frac{f(c)-f(x)}{c-x} \right | \right|_{Y} - ||df(c)||_{Y} \lt \epsilon
$$
So we have

$$
\tag{*}
\frac{||f(c)-f(x)||_{Y}}{||c-x||} \lt ||df(c)||_{Y} + \epsilon \leq k + \epsilon
$$

Finally, since $c$ is the infimum of $U_{\epsilon}$, then $\exists x \in U_{\epsilon}$ such that $c+\delta \geq x$ (otherwise $c+\delta$ is a better upper lower bound than the infimum, which is a contradiction) and also $x \gt c$ (since $c$ is the infimum and $c \notin U_{\epsilon}$ as we have shown) which means that $||x-c||=x-c$, so by multiplying $(*)$ with $||c-x||=x-c$ we get

$$
\tag{**}
||f(c)-f(x)||_{Y} \lt (k+\epsilon)(x-c)
$$
Since $c \gt a$ (as we have shown earlier), this also holds for $x=a$, and yields

$$
\tag{***}
||f(c)-f(a)||_{Y} \lt (k+\epsilon)(c-a)
$$
By summation of $(**)$ and $(***)$ and by the triangle inequality we get

$$
||f(x)-f(a)||_{Y} \leq ||f(c)-f(x)||_{Y}+ ||f(c)-f(a)||_{Y} \lt (k+\epsilon)(c-x + c- a) = (k+\epsilon)(c-a)
$$
But $x \in U_{\epsilon}$ so we expect $||f(x)-f(a)||_{Y} \gt (k+\epsilon)(c-a)$, which is a contradiction $\square$

 To generalize this result for the case where $X \neq \mathbb{R}$, consider $\varphi: [0, 1] \to X$ such that $\varphi(0)=a, \varphi(1)=b$ and again consider $h = f \circ \varphi: [0, 1] \to Y$, for which the theorem as we have proved it above holds, which completes the proof for the general case.

Practically speaking, the MVT, and the MVI (mean-value inequality), can be used to prove inequalities by the following steps: suppose you wish to prove that $f(x) \geq g(x)$ for all $x \in D$ where $x$ is some domain, then define a newYfunction $h(x)=g(x)-f(x)$, and show that $h(x)$ is differentiable in the interior of $D$. Next, show that $\forall x \in \partial D, h(x) \leq 0$. Then, show that $\forall c \in \text{Interior}(D), ||dh(c)(x)|| \leq 0$, then by the MVT we know that $h(x)$ is monotonically decreasing in $\text{interior}(D)$, which proves the inequality holds.

**Corollary 1**: The lowest upper bound on $||df(x)||$ is the supremem $\sup ||df(x)||$, so the MVI is equivalent to

$$
||f(b)-f(a)||_{Y} \leq \sup||df(x)|| \space ||b-a||_{X}
$$
**Corollary 2**: Let $f: [0, 1] \to Y$ and let $g: [0, 1] \to \mathbb{R}$ where $Y, X$ are Banach spaces and $f, g$ are differentiable on the open interval $(0, 1)$, and $||df(x)|| \leq dg(x)||$ on $(0, 1)$, then

$$
||f(1)-f(0)|| \leq g(1) - g(0)
$$
**Proof**: By the MVI applied on $f$, we have

$$
\tag{*}
||f(1)-f(0)|| \leq \sup ||df(x)|| \space ||1-0|| = \sup ||df(x)|| \underset{\text{supposition}}{\leq} \inf dg(x)
$$

Now by the MVT applied on $g$, we have that $\exists c \in (0, 1)$ such that

$$
g(1)-g(0)=dg(c)(1-0)=dg(c)
$$
Clearly $dg(c) \geq \inf dg(x)$, so by $(*)$ and the above we get

$$
||f(1)-f(0)|| \leq dg(c)=g(1)-g(0)
$$
Which completes the proof.

# Taylor Series Expansion

**Definition (Taylor Polynomial)**: Let $X, Y$ be Banach spaces and let $U\subseteq X$ be an open subset, and fix a point $x_{0} \in U$. Let $f: U \to Y$ be (Frechet) differentiable $n$ times at $x_{0}$, i.e. all high-order Frechet derivatives of $f$ up to (and including) the $n$-th degree exist. Then the **Taylor polynomial of degree $n$** of $f$ at $x_{0}$ is denoted $T_{n, f(x_{0})}: X \to Y$ and given by

$$
T_{n, f(x_{0})}(h)=f(x_{0}) + \sum_{i=1}^{n}\frac{D^{i}f(x_{0})(h)^{i}}{i!}
$$
Where $D^{n}f(x_{0})$ is the $n$-th derivative of $f$ at $x_{0}$, which is a multilinear map map $D^{n}f(x_{0}): U^{n}=U \times U \times \dots \times U \to Y$.

**Note**: In this context, $(h)^{i}$ is not "the point $h$ multiplied with itself $i$ times", as such operation is generally not defined, but instead it is the $n$-tuple $(\underbrace{h, h, \dots, h}_{n \text{ times}})$, and $D^{i}f(x_{0})(h)^{i}$ is shorthand for $D^{i}f(x_{0})(\underbrace{h, h, \dots}_{h \text{ times}})$.

**Proposition**: $T_{n, f(x_{0})}(0)=f(x_{0})$ (supposing the Taylor polynomial is defined as above and exists).

**Proof**: Since $D^{n}f(x_{0})$ is a linear operator we have $D^{n}f(x_{0})(0)^{n}=D^{n}f(x_{0})(0)=0$ so the only term in the polynomial that doesn't vanish when $h=0$ is $f(x_{0})$.

**Proposition**: Suppose that $f: U \subseteq X \to Y$ is $n$ times differentiable at a point $x_{0}$, then $T_{n, f}$ is also differentiable at $x_{0}$ and

$$
d(T_{n, f(x_{0})})(h) = T_{n-1, df(x_{0})}(h)
$$

i.e., the derivative at $x_{0}$ of the $n$-th degree Taylor polynomial  of $f$ at $x_{0}$ is the $n-1$ degree Taylor polynomial of the derivative $df$ at $x_{0}$ evaluated at $h$.

**Proof**: Recall that $d^{i}f(x_{0})$ is identified with a multilinear mapping $d^f(x_{0}): \mathcal{L}(\underbrace{X, X, \dots}_{n \text{ times}}; Y)$, thus by a previous result its derivative at some $(h)^{i} \in X^{i}$ evaluated at some $k \in X$ is given by

$$
d(d^{i}f(x_{0})(h)^{i})(k)=d^{i}f(x_{0})(k, \underbrace{h, h, \dots}_{i-1 \text{ itmes}})+d^{i}f(x_{0})(h, k, h, \dots) + \dots + d^{i}f(x_{0})(h, h, \dots, k)
$$
By a previous result. Also by a previous result, $d^{i}f$ is a symmetric multilinear mapping, so all summands are the same, and we can write

$$
\tag{*}
d(d^{i}f(x_{0})(h)^{i})(k)=id^{i}f(x_{0})(\underbrace{h, \dots, h}_{i-1 \text{ times}}, k) = i(d^{i}f(x_{0})(h)^{i-1})(k)
$$

Consider $T_{n, f(x_{0})}(h)$, since every term is differentiable at $x$ (since each derivative is a bounded (mutli)linear map thus differentiable by a previous result),  then $dT_{n, f(x_{0})}(h)$ is given by the sum of the derivatives of its summands. $f(x_{0})$ is constant so 0by a previous result its derivative is constantly $0$, and for the rest we have by the last relation:

$$
\begin{gathered}
d\left(\frac{D^{i}f(x_{0})(h)^{i}}{i!}\right)(h)\underset{\text{homogeneity}}{=}\\\\
\frac{1}{i!}d(D^{i}f(x_{0})(h)^{i})(h) \underset{\text{by }(*)}{=}\\\\
\frac{1}{(i-1!)i}i(D^{i}f(x_{0})(h)^{i-1})(h)=\\\\
\frac{(D^{i}f(x_{0})(h)^{i-1})(h)}{(i-1)!} \underset{\text{definition}}{=}\\\\
\frac{D(D^{i-1}f(x_{0}(h)^{i-1})(h)}{(i-1)!}
\end{gathered}
$$

By summation of all such derivatives, we retrieve

$$
d(T_{n}, f(x_{0}))(h)=T_{n-1, df(x_{0})}(h)
$$
$\square$

**Theorem (Taylor expansion theorem)**: Let $f: U \subseteq X \to Y$ where $X, Y$ are Banach spaces and $U$ is an open set and fix $x_{0} \in U$, and suppose $f$ is $n$ times differentiable. Let $x \in U$ and consider the remainder term $R_{n, f(x_{0})}(h) = f(x_{0} + h) - T_{n, f(x_{0})}(h)$, then $R_{n, f(x_{0})}(h)$  satisfies

$$
\lim_{h \to 0} \frac{||R_{n, f(x_{0})}(h)||_{Y}}{||h||_{X}^{n}} \to 0
$$
i.e. in Little-o notation, $R_{n, f(x_{0})}(h)=o(||h||^{n})$.

**Note**: The requirement on the differentiability of $f$ can be relaxed so that $f$ need only be $n-1$ differentiable in $U$, and $n$ times differentiable at $x_{0}$.

**Proof**: We prove by induction on $n$. For $n=1$, $T_{1, f(x_{0})}(h)=f(x_{0})+df(x_{0})(h)$ and $R_{1, f(x_{0})}(h)=f(x_{0}+h)-f(x_{0})-df(x_{0})(h)$, so $R_{1, f(x_{0})}(h)=O(||h||)$ is exactly the second definition provided for $f$ to be Frechet differentiable at $x_{0}$, which covers the base case. 

Assume by induction that this holds for some $n$, and consider $n+1$, i.e. we suppose that $f$ is $n+1$ times differentiable at $x_{0}$. Observe that $R_{n+1, f(x_{0})}$ is differentiable at $h$ as the sum of differentiable functions, so by derivative arithmetics

$$
dR_{n+1, f(x_{0})}(h) = df(x_{0}+h)-dT_{n, f(x_{0})}(h) \underset{\text{by proposition}}{=}df(x_{0}+h)-T_{n-1, df(x_{0})}(h)
$$

Observe that $df(x_{0}+h)$ is $n$ times differentiable at $x_{0}$ (since $f$ is assumed to be $n+1$ times differentiable there) and that $dR_{n+1, f(x_{0})}(h)$ is the remainder of the $n$-th degree of the Taylor polynomial of $df(x_{0}+h)$, so we can use the induction hypothesis and get that $dR_{n+1, f(x_{0})}h = o(||h||^{n})$. Let $\epsilon \gt 0$, and let $\epsilon'=2\epsilon$. Since $dR_{n+1, f(x_{0})}(h)=o(||h||^{n})$, $\exists \delta \gt 0$ such that $\forall h \in B_{\delta}(0) \setminus \{0\}$, we have

$$
\frac{||dR_{n+1, f(x_{0})}(h)||_{Y}}{||h||^{n}_{X}} \lt 2 \epsilon \Rightarrow ||dR_{n+1, f(x_{0})}(h)||_{Y} \lt 2 \epsilon ||h||_{X}^{n}
$$
So $\forall h \in B_{\delta}(0) \setminus \{0\}$, the derivative of $R_{n+1, f(x_{0})}$  is bounded then we can apply the mean-value inequality to the ball $B_{\delta}(0)$ with $c=h, a=0$ and $k=\epsilon ||h||_{X}^{n}$, and get:

$$
||R_{n+1, f(x_{0})}(h) - R_{n+1, f(x_{0})}(0)||_{Y} \leq (2 \epsilon ||h||_{X}^{n})||h||_{X}=2 \epsilon ||h||^{n+1}_{X}
$$
Now observe that $R_{n+1, f(x_{0})}(0)=f(x_{0})-T_{n, f(x_{0})}(0)=f(x_{0})-f(x_{0})$ by a previous result, so $R_{n+1, f(x_{0})}(0)=0$, so by dividing the last inequality by $||h||_{X}^{n+1}$ we get

$$
\frac{||R_{n+1, f(x_{0})}(h)||_{Y}}{||h||_{X}^{n+1}} \leq 2 \epsilon \lt \epsilon
$$
For the same $\delta$, so by definition we have $\lim_{h \to 0} \frac{||R_{n+1, f(x_{0})}(h)||_{Y}}{||h||_{X}^{n+1}} \to 0$, which completes the proof by induction.

**Theorem (Taylor's remainder theorem)**: Let $f: U \subseteq X \to Y$ be differentiable $n+1$ times on a convex set $U$ and suppose the $(n+1)^{\text{th}}$ derivative of $f$ is bounded on $U$, i.e. $\exists M = \sup|| d^{n+1}f(x)||$, and let the remainder term in Lagrange form of the Taylor polynomial of $f$ around some point $x_{0} \in U$ be given by

$$
R_{n, f(x_{0})}(h) =f(x_{0}+h) - T_{n, f(x_{0})}(h)
$$
Then the remainder term satisfies:
$$
R_{n, f(x_{0})}(h) \leq \frac{1}{(n+1)!}M ||h||^{n+1}
$$
To prove the theorem, we will use the following lemmas:

**Lemma 1**: Let $v: I \subseteq \mathbb{R} \to Y$ where $I$ is an interval and $Y$ is a Banach space (which scalars from $\mathbb{R}$) be  differentiable $n+1$ times on the open interval $I$, then

$$
d\left(v(t) + \sum_{i=1}^{n}(1-t)^{i}\frac{d^{i}v(t)}{i!}\right)=\frac{(1-t)^{n}}{n!}d^{n+1}h(t)
$$
Where $(1-t)^{i}$ is $(1-t)$ to the $i$-th power.

**Proof (lemma)**: Define $h_{n}(t)=v(t)+\sum_{i=1}^{n}\frac{d^{i}v(t)}{i!}(1-t)^{i}$.

We prove by induction on $n$. For $n=1$, we have

$$
d(v(t)+dv(t)(1-t))\underset{\text{product rule}}{=}dv(t) + d^{2}v(t)(1-t) - dv(t)=d^{2}v(t)(1-t)
$$
Which covers the basis case. Assume this is true for some $n-1 \geq 1$, and consider $n$. Note that due to the additive property of the derivative of differential functions and by homogeneity, we have

$$
d(h_{n}(t))=d\left(h_{n-1}(t)+(1-t)^{n}\frac{d^{n}v(t)}{n!}\right)=d(h_{n-1}(t))+\frac{1}{n!}d\left((1-t)^{n}d^{n}v(t) \right)=(*)
$$
By the induction hypothesis and by the product rule, we have

$$
(*)=\frac{(1-t)^{n}}{(n-1)!}d^{n}v(t)+\frac{1}{n!}((1-t)^{n}d^{n+1}v(t)-n(1-t)^{n}d^{n}v(t))=\frac{(1-t)^{n}}{n!}d^{n+1}v(t)
$$
Which completes the proof by induction.

**Lemma 2**: Let $Y$ be a Banach space over $\mathbb{R}$ and let  $v: [0, 1] \to Y$ be differentiable $n+1$ times on $(0, 1)$ and suppose $d^{n+1}v$ is bounded, i.e. $\exists M \gt 0$ such that $\forall t \in [0, 1], ||d^{n+1}v(t)|| \leq M$, then

$$
||v(1)-v(0) - \left(\sum_{i=1}^{n}\frac{1}{i!}df^{i}v(0)\right)|| \leq \frac{M}{(n+1)!}
$$

**Proof**: Suppose $v$ is differentiable $n+1$ times on $(0, 1)$ and that $d^{n+1}v$ is bound by some scalar $M \gt 0$. Define $h(t)$ such that

$$
h(t)=v(t)+\sum_{i=1}^{n}\frac{(1-t)^{i}}{i!}d^{i}v(t)
$$
Then by Lemma $1$,

$$
\tag{1}
dh(t) = \frac{(1-t)^{n}}{n!}d^{n+1}v(t) \underset{\text{supposition}}{\leq} \frac{(1-t)^{n}}{n!}M \Rightarrow ||dh(t)|| \leq \frac{(1-t)^{n}}{n!}M
$$
Clearly $h(1)=v(1)$ (since all terms scaled by $(1-t)^{i}$ vanish), and that

$$
h(0)=v(0) + \sum_{i=1}^{n}\frac{1}{i!}df^{i}v(0)
$$
Now consider $h(1)-h(0)$, we have

$$
\tag{2}
h(1)-h(0)=v(1)-v(0)-\left(\sum_{i=1}\frac{1}{i!}df^{i}v(0)\right)
$$
Define a new function $g(t) = -\frac{M}{(n+1)!}(1-t)^{n+1}$, by the product rule we get

$$
dg(t)=\frac{M}{(n+1)!}(1-t)^{n}
$$
So by $(1)$, we have $||dh(t)|| \leq dg(t)$ for $t \in (0, 1)$. By a corollary to the mean-value inequality, this implies that

$$
||h(1)-h(0)|| \leq g(1) - g(0) =0 + \frac{M}{(n+1)!}
$$
By $(2)$, the LHS is exactly the term we wish to bound by $\frac{M}{(n+1)!}$, which completes the proof.

**Proof (Taylor's remainder theorem)**: Suppose $f: X \to Y$ is differentiable $n+1$ times on a convex subset $U \subseteq X$ and that $d^{n+1}f$ is bounded, and fix some point $x_{0} \in U$. Let $h \in X$. We define a new function $g: [0, 1] \to Y, t \mapsto f(x_{0}+th)$. Clearly, one has $g(0)=f(x_{0})$ and $g(1)=f(x_{0}+h)$, and also $g$ is differentiable $n+1$ times in $[0, 1]$ due to continuity and differentiability of $f$, so by the chain rule we have

$$
dg(t)=d(f(x_{0}+th))=hdf(x_{0}+t)
$$
Applied recursively, one gets

$$
d^{n+1}g(t)=h^{n+1}\space d^{n+1}f(x_{0}+th)
$$
Since $d^{n+1}f$ is bounded on $U$, $\exists M \gt 0$ such that $||d^{n+1}f(x_{0}+t)|| \leq M$, so by taking the norm of both sides we get

$$
\tag{1}
||d^{n+1}g(t)|| = ||h||^{n+1} \space || d^{n+1}f(x_{0}+th)|| \leq M \space ||h||^{n+1}
$$
Finally, consider the expression

$$
\tag{2}
||g(1)-g(0) - \sum_{i=1}^{n}\frac{1}{i!}d^{i}g(0)||
$$
By lemma $2$, we know that given some value $K$ which bounds $d^{n+1}g(t)$, the previous expression has an upper bound $\frac{K}{(n+1)!}$. By $(1)$, we know that $M \space ||h||^{n+1}$, where $M$ is an upper bound on $d^{n+1}f(x)$ in $U$ is such a bound. Also, we know that $g(0)=f(x_{0})$ and $g(1)=f(x_{0}+h)$, so $(2)$ is the same as

$$
(2) = \left|\left|f(x_{0}+h)-\left( f(x_{0} + \sum_{i=1}^{n}\frac{d^{i}f(x_{0})(h)^{i}}{i!}\right)\right|\right| = ||f(x_{0}+h)-T_{n, f(x_{0})}(h)||=||R_{n, f(x_{0})}(h)||
$$

So

$$
||R_{n, f(x_{0})}(h)|| = (2) \leq \frac{1}{(n+1)!}M||h||^{n+1}
$$
Where $M= \sup d^{n+1}f(x_{0})$ on $U$ $\square$

**Corollary**: For $f: \mathbb{R}^{m} \to \mathbb{R}$, one can use the mean value theorem instead of the mean value inequality and follow the same proof layout to obtain a stronger result, that is that for some $c \in U$, one can take $M=||d^{n+1}f(c)||$  and obtain an equality instead of an inequality.

## Taylor Polynomial In Euclidean Space

We limit our discussion to functions of the form $f: U \subseteq \mathbb{R} \to \mathbb{R}$, then since multiplication commutes and from previous results on the relation between the Frechet derivative and the classical derivative we have that the $n$-th degree Taylor polynomial of $f$ differentiable $n$ times at some point $x_{0} \in \mathbb{R}$ takes the form

$$
T_{n, f(x_{0})}(h) = f(x_{0})+\sum_{i=1}^{n}\frac{f^{i}(x_{0})h^{i}}{i!}
$$
Here $h^{i}$ is taken as "$h$ raised to the $i$-th power" and $f^{n}(x_{0})$ is the $n$-th order "classical" derivative of $f$.

Note that for $f: U \subseteq \mathbb{R}^{n} \to \mathbb{R}$, we have

$$
T_{n, f(x_{0})}(h)=f(x_{0})+\sum_{i=1}^{n}\frac{(\nabla^{i}f(x_{0}))(h)^{i}}{i!}
$$
Which is the familiar definition of the Taylor polynomial for a scalar field.

The Taylor series expansion is extremely valuable both in proving inequalities, and in numerical analysis.

Taylor's remainder theorem has strong implications for numerical analysis - since we know that the remainder vanishes in $O(||h||^{n})$ as $h \to 0$, and since we have an upper bound on the error as a function of $h$, we have two immediate results:

1. The Taylor polynomial always provides a good approximation when $h$ vanishes, and the degree of accuracy is exponential in the degree of the polynomial.
2. The upper bound on the error is linear in the supremum of the $(n+1)^{\text{th}}$ derivative of $f$ at $B_{h}(x)$, i.e. if the norm of the derivative is very large compared to $||h||^{n+1}$, then the Taylor polynomial will not provide a good approximation.

**Example 1**: Let us calculate the Taylor polynomial of $e^{x}$. It is a classical result in calculus that $(e^{x})'=e^{x}$, which implies that $e^{x}$ is infinitely differentiable at any point in its domain, and that all derivatives are equal and equal to the function itself, so:

$$
T_{n, f(x_{0})}(h)=e^{x_{0}}+\sum_{i=1}^{n}\frac{e^{x_{0}}}{i!}h^{i}
$$
For $x_{0}=0$ it is is known that $e^{x_{0}}=1$ even without knowing the value of $e$, so

$$
T_{n, f(0)}(h)=1+\sum_{i=1}^{n}\frac{h^{i}}{i!}
$$
**Example 2**: Let's calculate $e$ to the $5^{\text{th}}$ digit, i.e. with a level of accuracy of $10^{-5}$, using Taylor polynomials. This is equivalent to saying that we wish to find an $n$ and an $x_{0}$ such that $R_{n, f(x_{0})}(h) \leq 10^{5}$. Let's use $x_{0}=0$ since it is easy to calculate the Taylor polynomial without knowing the value of $e$, as demonstrated in the previous example. Since we wish to calculate $e^{1}$, we have to take $h=1$. By the Taylor remainder theorem we have

$$
R_{n, e^{0}}(1) \leq \sup||f^{n+1}(x)||\space\frac{1^{n}}{(n+1)!}\underbrace{=}_{\text{monotonicity}}\frac{e^{1}}{(n+1)!}
$$

It is a classical result in real analysis that $2 \lt e \lt 3$, which we will not prove here. Using this result, we have

$$
R_{n, e^{0}}(1) \leq \frac{3}{(n+1)!}
$$

The desired error is $10^{-5}$, so we can find the lowest value of $n$ for which the Taylor polynomial provides the desired degree of accuracy by solving the following optimization problem

$$
\min n \in \mathbb{N} \quad\text{s.t.}\quad \frac{3}{(n+1)!}\leq10^{-5}
$$
And indeed we can find that for $n=7$, $\frac{3}{8!} \leq 10^{-5}$  and this is the smallest $n$ which satisfies this condition, so we need to evalute $T_{7, f(0)}(1)$ to get the answer, which is

$$
T_{7, f(0)}(1)=1+\sum_{i=1}^{7}\frac{1}{i!}=1+1+\frac{1}{2}+\frac{1}{6}+\frac{1}{24}+\frac{1}{120}+\frac{1}{720}+\frac{1}{5040}=\frac{685}{252} \approx2.71825
$$
This method can also be used for evaluating trigonometric functions to a desired precision, by using known values which can be derived geometrically and then expanding a taylor Polynomial around those points.

Taylor polynomials are also useful in calculating the limit of a function at a point. Consider the following example:

**Example 3**: Consider the function $f(x) = \sin x - x$. We wish to consider the limit of $h(x)=\frac{\sin x - x}{x^{3}}$ as $x \to 0$. To do this, we can express $\sin x$ in terms of a Taylor polynomial with a remainder term. First, recall the classical results $(\sin x)'=\cos x$ and $(\cos x)' = -\sin x$. Furthermore it is known that $\sin 0 = 0$ and $\cos 0 = 1$, so all even degree derivatives vanish, and  the Taylor polynomial of $\sin x$ expanded at $x=0$ is given by

$$
T_{n, \sin(0)}(h) = x - \frac{x^{3}}{3!}+\frac{x^{5}}{5!}-\frac{x^{7}}{7!}+\dots
$$
So by the Taylor expansion theorem, we have

$$
\sin x = x - \frac{x^3}{3!} + \frac{x^{5}}{5!} - \frac{x^{7}}{7!} + \dots + o(||x||^{n}) \Rightarrow \sin x - x =-\frac{x^3}{3!}+o(||x||^{3})
$$
Where $x-\frac{x^{3}}{3!}$ is the $3^{\text{rd}}$ degree taylor polynomial of $\sin x$ at $0$, so $h$ can be written as

$$
h(x)=\frac{-\frac{x^3}{3!}+o(||x||^{3})}{x^{3}}=-\frac{1}{6}+\frac{o(||x||^{3})}{x^3}
$$
And as $x \to 0$, the second summand vanishes and we have $h(x) \underset{x \to 0}{\to}-\frac{1}{6}$.
